---
title: "Pyseter: A Python package for processing images before photo-identification"
author:
  - name: Philip T. Patton
    orcid: 0000-0003-2059-4355
    corresponding: true
    email: PattonP@si.edu
    affiliations:
      - Migratory Bird Center, Smithsonian's National Zoo and Conservation Biology Institute
      - Marine Mammal Research Program, University of Hawaiʻi at Mānoa
  - name: Claire Lacey
    orcid: 0000-0003-0541-8193
    corresponding: false
    affiliations:
      - Marine Mammal Research Program, University of Hawaiʻi at Mānoa
  - name: Lars Bejder
    orcid: 0000-0001-8138-8606
    corresponding: false
    affiliations:
      - Marine Mammal Research Program, University of Hawaiʻi at Mānoa
      - Zoophysiology, Department of Bioscience, Aarhus University
keywords: computer vision, deep learning, clustering, whale, dolphin
abstract: Photographic identification (photo-ID) is an effective and non-invasive method for studying many aspects of animal ecology. Photo-ID, however, is a labor-intensive process that involves many steps including, but not limited to, grading individuals by the distinctiveness of their markings (useful for partially marked populations), and curating the best image of each individual from each encounter. This latter step is helpful for reducing false negative errors and reducing the labor effort involved in manual or semi-automated identification. We introduce Pyseter, a Python package for automating several of these steps. Pyseter’s extract module allows users to extract feature vectors from images with AnyDorsal, an individual identification algorithm that has been trained and tested on 24 species of cetacean. The grade module has experimental features for evaluating distinctiveness with information in the feature vectors. The sort module includes two clustering algorithms for grouping images into proposed IDs, as well as methods for investigating cluster performance. Finally, sort also allows users to sort images into subdirectories by this proposed ID then encounter. We demonstrate a typical Pyseter workflow with an example of 1200 spinner dolphin images from Hawaiʻi. Both the grade and sort modules are taxon agnostic, in that they will work with similarity scores or feature vectors from any identification algorithm. Feature vectors are useful for several types of analyses, including estimating individual similarity, comparing a query set with a reference set, and evaluating algorithmic performance in terms of population assessments. As such, the extract module facilitates several useful analyses for users with cetacean datasets. Finally, Pyseter can readily accommodate additional algorithms, e.g., animal detection and quality grading, as the technology becomes available. 
date: last-modified
bibliography: references.bib
csl: ecology.csl
number-sections: true
jupyter: "python3"
  
execute:
  echo: true
  include: true
engine: jupyter
---

## Introduction

Photographic identification (photo-ID) is an effective and non-invasive method for studying many aspects of ecology, including movement [@gardiner-2014-dragon], social behavior [@bejder-1998-social], abundance [@mcpherson-2024-dolphin], survival [@morrison-2011-survival], distribution [@mcguire-2020-belu0], recruitment [@setyawan-2022-population], migration [@hill-2020-humpback], and resource-selection [@patton-2025-dissertation]. Photo-ID is a multi-step process that culminates in comparing a query set—images of animals whose identity we wish to know—against a reference set—images of known individuals. This last step can be accomplished by hand [@karanth-1995-camera], with a database [@adams-2006-finbase], or with individual identification software that comes with database management, such as Happywhale [@cheeseman-2021-happywhale].

Curating a query set from a batch of field images involves several steps that are often manual and labor-intensive. These include selecting images with animals [@beery-2019-efficient], selecting images of sufficient quality [@urian-2015-cmr], and selecting images of animals with distinctive markings [@rosel-2011-grading], which is necessary in partially marked populations. Additionally, users may want to limit the query set to only the highest quality images of each individual from a single encounter (e.g., a burst of images from a camera trap). This especially helpful for manual photo-ID because it can dramatically reduce the number of comparisons between the query and the reference set. Further, limiting the query set to only the best images from each encounter may reduce the chance of a missed match, i.e., a false negative error [@urian-2015-cmr]. One way to curate the query set this way is to “sort” the images by apparent individual and encounter (Figure 1).Sorting, however, is a labor-intensive process in itself, in that it requires $\binom{m}{2}=m(m-1)/2$ comparisons, where $m$ is the number of images in the query set. As such, a query set of 500 images requires 124,750 comparisons to complete the sort. 

![Example of sorted images. Before sorting, the entire query set is in one directory. After sorting, the query set is divided into many directories. In this example, the second level of directories are the temporary IDs and the third level of directories are the encounters](images/sort-figure.png){#fig-sort}

We developed a Python package, Pyseter, to help automate several of these steps. For example, Pyseter’s `extract` module extracts feature vectors from images, which are useful for estimating similarity scores and identifying individuals [@miele-2021-metric]. The `grade` module includes functions for evaluating the distinctiveness of an animal’s markings. Finally, the `sort` module contains two clustering algorithms for classifying individuals into proposed identities. Additionally, `sort` includes functions for sorting images into proposed identities and encounters [@fig-sort]. Pyseter currently lacks functions for detecting animals in images [@beery-2019-efficient] or grading the quality of images [@rosel-2011-grading]. We plan on adding these functions as the technology advances.

## Installation

Here, we assume some familiarity with Python, conda, and pip. Users coming from R, who may be less familiar with these concepts, should reference the “General Overview” Notebook that’s included in this manuscript’s attendant Zenodo repository. [Reviewers will find it in the Anonymous GitHub repository.]

If using conda, we recommend creating a fresh conda environment. Additionally, before installing Pyseter, users must install Pytorch. Follow the directions on the Pytorch website, which will vary based on your operating system and how you plan to use GPU acceleration. Users who plan on extracting features should have an NVIDIA GPU that is CUDA compatible, or a Mac with at least 16 GB of RAM. Below, we demonstrate the bash commands necessary to install Pytorch. These can be executed in a Jupyter Notebook (as below) or a command line interface (e.g., the miniforge prompt in Windows or the Terminal application in a Unix-like OS). After installing Pytorch, users can install Pyseter from PyPI.

```{python}
#| eval: false
%%bash
conda create -n pyseter_env  
conda activate pyseter_env
conda install pip3
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128
pip3 install pyseter
```

Users can verify the installation by running the following Python commands in, say, a Jupyter Notebook,

```{python}
import pyseter
pyseter.verify_pytorch()
```

which will tell the user which form of GPU acceleration is available, if any.
 
 
## Spinner dolphin example 

We demonstrate the core modules and functions of Pyseter with a example using spinner dolphins (*Stenella longirostris*). Theoretically, Pyseter is taxon agnostic. Functions in the grade and sort modules work with similarity scores, which can be generated from any individual identification algorithm [@miele-2021-metric]. The extract module, however, only includes one individual identification algorithm, namely, AnyDorsal, which is only suitable for cetacean dorsal images (see below) [@patton-2023-deep]. As such, we chose a cetacean example to demonstrate the package’s full capabilities. The images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi [@lacey-2025-spinner]. Every image collected during the study was graded for quality and distinctiveness [@urian-2015-cmr, @lacey-2025-spinner]. This example dataset only includes images of sufficient quality that have been cropped to the identifying mark—the dorsal fin [@rosel-2011-grading] (@fig-demo). This example includes 208 images of animals without distinctive markings, and 1043 images of animals with distinctive markings.  

```{python}
#| code-fold: true
#| label: fig-demo
#| fig-cap: Nine images from the spinner dolphin example [@lacey-2025-spinner]
import os
import matplotlib.pyplot as plt
from PIL import Image

# grab the first nine images in the dataset
nrow = ncol = 3
demo_dir = '/Users/philtpatton/datasets/pyseter-data/working_dir/all_images'
demo_images = os.listdir(demo_dir)[:(nrow * ncol)]

# plot a grid of images 
fig, axes = plt.subplots(nrow, ncol, figsize=(6, 6), tight_layout=True)
for i, filename in enumerate(demo_images):
    path = os.path.join(demo_dir, filename)
    image = Image.open(path)
    axes.flat[i].imshow(image)
    axes.flat[i].axis('off')

fig.savefig('images/fig-demo.png', transparent=False, bbox_inches='tight', dpi=600)
```

### Folder management

To do keep things clean and tidy, we recommend establishing a `working_directory` with a subfolder, e.g., called, `all_images`, that contains every image that needs to be sorted (see below for a different case). The working directory should also contain a .csv with encounter information. This .csv would contain two columns: one for the image name, i.e., every image in `all_images`, and another for the encounter. As such, the working directory would look like this.

```
working_dir
├── all_images
│   ├── 0a49385ef8f1e74a.jpg
│   ├── 0aca671c4afbd9b9.jpg
        ...
│   └── ffa8759a92174857.jpg
└── encounter_info.csv
```

Alternatively, you might have your images organized into subfolders by encounter. 

```
working_dir
└── original_images
    ├── enc0
    │   ├── 0a49385ef8f1e74a.jpg
        ├── 1e105f9659c12a66.jpg
            ...
    │   └── f5093b3089b44e67.jpg
    └── enc12
        ├── 0b5c44f167d89d6c.jpg
        ├── 1e0c186da31a53c4.jpg
            ...
        └── f9bb41e7ce0d672d.jpg
```
In this case, you might want to accomplish two tasks: move all these images to one folder, e.g., `all_images`, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The `prep_images()` function does just that.

```{python}
from pyseter.sort import prep_images

working_dir = '/Users/philtpatton/datasets/pyseter-data/working_dir'
original_image_dir = working_dir + '/original_images'

# new, flattened directory containing every image
image_dir = working_dir + '/all_images'
prep_images(original_image_dir, all_image_dir=image_dir)
```

### Extracting features

Pyseter’s `extract` module includes the AnyDorsal algorithm, which was trained to identify cetaceans of 24 species [@patton-2023-deep]. AnyDorsal’s identifying performance varies by species. Species primarily identified by nicks and notches along the dorsal fin will perform best [@patton-2023-deep].

Pyseter extracts features with the `FeatureExtractor` class, which needs to be initialized by setting the `batch_size`. The `batch_size` dictates how many images will be processed by the GPU at once. Larger batch sizes might run faster yet might also produce an `OutOfMemoryError`. For lower memory GPUs, we recommend smaller batch sizes. If you encounter an `OutOfMemoryError` with `batch_size=1`, then you will have to reduce the size of your images. See the Supplement for how to do so with the Pillow library.

```{python}
from pyseter.extract import FeatureExtractor

# feature extraction can take time so it's useful to save the result 
feature_dir = working_dir + '/features'
os.makedirs(feature_dir, exist_ok=True)

fe = FeatureExtractor(batch_size=4)
```

The `extract()` method of the `FeatureExtractor` class only takes one argument, `image_dir`, the flattened directory containing every image to be processed. Feature extraction can take several minutes, depending on the number of files and the GPU, so we recommend saving the results afterwards. The `extract()` function returns a Python dictionary where the filenames are the keys, and the features are the values. It can be useful to convert these to NumPy arrays 

```{python}
#| eval: false
import numpy as np

features = fe.extract(image_dir=image_dir)

# this saves the dictionary as an numpy file
out_path = feature_dir + '/features.npy'
np.save(out_path, features)

# convert keys and values to numpy arrays
filenames = np.array(list(features.keys()))
feature_array = np.array(list(features.values()))
```

Alternatively, we can load previously saved results with the `load_features()` function. In either case, `feature_array` will be a two-dimensional matrix of shape `(n, 5504)`, where `n` is the number of images and `5504` is the number of features returned by AnyDorsal.

```{python}
# alternatively, load in the feature dictionary from file 
import numpy as np 
from pyseter.sort import load_features
out_path = feature_dir + '/features.npy'
filenames, feature_array = load_features(out_path)
```

### Grading individuals by distinctiveness

Here, we introduce one of Pyseter’s clustering algorithms, `NetworkCluster`, because doing so helps understand the distinctiveness grading algorithm (see Section 3.4 for a more thorough description of `NetworkCluster`). Network clustering works with similarity scores, which represent the similarity between two individuals in a pair of images. We can define a threshold score, the `match_threshold`, above which we consider two individuals to be the same. That is, if the similarity score between two images is above a certain threshold, we cluster them into a proposed ID. As such, network clustering works by treating the query set as a network, where the nodes are images and the edges are similarity scores above a threshold. Each set of connected components, i.e., images whose similarity scores are above the match threshold, represents a proposed ID.

We might expect the indistinct individuals to cluster together. In the context of facial recognition, @deng-2023-ui observed that “unrecognizable identities”, e.g., extremely blurry or masked faces, tend to cluster together. As such, for partially marked populations, the largest cluster in the query set may represent every indistinct individual. Following @deng-2023-ui, we can compute the average feature vector for this cluster. The distance between this average feature vector and the feature vector for each image is the distinctiveness score for that image. As such, the score applies to the image, not the animal. To get a score for an animal, users could average the distinctiveness scores across images for that animal.

```{python}
from pyseter.grade import rate_distinctiveness
distinctiveness = rate_distinctiveness(feature_array, match_threshold=0.5)
```

We can evaluate the effectiveness of these scores with AUC (i.e., the area under the receiver operating curve). AUC is a metric for evaluating a classifier's ability to balance true positive and false positive rates. In this example, a classifier built from the distinctiveness grades achieves an AUC of 0.925 [@fig-roc].

```{python}
#| code-fold: true
#| label: fig-roc
#| fig-cap: Receiving operator characteristic (ROC) curve for a classifier based on the ERS. 
#|   The area under the curve (AUC), a measure of overall performance, is listed in the middle.
from sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score
import pandas as pd

# merge the ers scores with the true values to compare for each image
mapping = pd.read_csv('/Users/philtpatton/datasets/pyseter-data/file_mapping.csv').iloc[:, 1:]
ers_df = pd.DataFrame({'image_new': filenames, 'ers': 1 - distinctiveness})
ers_df = ers_df.merge(mapping)

# compute the curve first, which will get displayed
y_score = ers_df['ers']
y_test, _ = ers_df.distinctiveness.factorize()
fpr, tpr, _ = roc_curve(y_test, y_score)

fig, ax = plt.subplots(figsize=(4, 3))

roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)

roc_auc = roc_auc_score(y_test, y_score)
ax.text(0.95, 0.6, f'AUC={roc_auc:0.3f}', ha='right', va='top')

ax.set_title('ROC Curve for \nDistinctiveness Classifier')

fig.savefig('images/fig-auc.png', transparent=False, bbox_inches='tight', dpi=300)
```

These distinctiveness scores are experimental in that have not been robustly tested across a variety of scenarios. Nevertheless, users might use them as a guide, potentially making distinctiveness grading somewhat easier.

### Clustering images into proposed IDs

To use `NetworkCluster`, users must first compute the similarity scores between each pair of images. After computing the scores, we cluster the images, where each cluster represents a proposed identity.

```{python}
from sklearn.metrics.pairwise import cosine_similarity
from pyseter.sort import NetworkCluster, report_cluster_results

similarity_scores = cosine_similarity(feature_array)
nc = NetworkCluster(match_threshold=0.55)
results = nc.cluster_images(similarity_scores)
```

As mentioned above, network clustering has one major hyperparameter, `match_threshold`, which indicates whether two images should be grouped within a cluster. High thresholds mean that few images will be clustered together, creating many clusters. Very low thresholds mean that many images will be clustered together, creating few clusters. The `report_cluster_results()` function produces a quick and dirty summary of the number of clusters created, and the size of the largest cluster (i.e., the number of images associated to the most photographed individual). This is a quick sanity check. The `results` object has several useful attributes and methods (see below). For example, `results.cluster_idx` contains the proposed ID for each image.

```{python}
network_idx = results.cluster_idx
report_cluster_results(network_idx)
```

In this example, the `nc.cluster_images()` function warned that some clusters may contain “false positives.” False positive matches occur when two separate individuals fall under the same proposed ID. We can diagnose possible false positives by evaluating the network. Recall that, in the network, a blob of connected nodes (i.e., connected components) represents a proposed ID. These connected components, however, can sometimes look less like a blob and more like a barbell, where two sets of images have many connections amongst each other, yet the two blobs are only connected by a single link. We suspect that such clusters represent false positives, i.e., two sets of images for two individuals connected by one spurious link. We can plot the networks of the suspicious clusters with the `results.plot_suspicious()` function.

```{python}
#| label: fig-false
#| fig-cap: Proposed IDs that may contain false positive errors. Each image (colored)
#|     circle is linked to another image (gray line) if similarity between them exceeds
#|     a threshold. The colors of each circle represent potential IDs within the proposed
#|     ID. 
results.plot_suspicious()
```

Both `ID_0011` and `ID_0150` appear to have spurious links combining two separate IDs. To deal with this, we could increase manually separate these clusters, or increase the match threshold. Cluster `ID_0002` represents the "unrecognizable individual" cluster (see Section 3.3).

As the number of images being clustered grows, the overall false positive rate also grows (this is analogous the multiple comparison problem in statistics). At some point, network matching becomes untenable; all but the highest match thresholds would produce too many false positives to be useful. For these cases, there is `HierarchicalCluster`, which relies on the Hierarchical Agglomerative Clustering algorithm provided by the popular machine learning package, scikit-learn [@scikit-learn]. Note that `HierarchicalCluster` will run noticeably slower than `NetworkCluster`. See the supplement for an example with `HierarchicalCluster`.

### Sorting images by proposed ID then encounter

With these cluster results, we can sort images by proposed ID then encounter. To do so, we need to create a Pandas `DataFrame` that indicates the proposed ID and encounter for each filename [@pandas]. Recall that we created the `encounter_info.csv` with the `prep_images()` function above.

```{python}
import pandas as pd

id_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})

# join with the encounter information using "encounter" as a key 
encounter_info = pd.read_csv(working_dir + '/encounter_info.csv')
id_df = id_df.merge(encounter_info)
id_df.head()
```

Finally, to sort the images, we need to specify an output directory, then run the `sort_images()` function. Note that the ID `DataFrame` must have columns named `image`, `proposed_id`, and `encounter`. Otherwise `sort_images()` will not work.

```{python}
from pyseter.sort import sort_images

# make an output directory 
sorted_dir = working_dir + '/sorted_images'
os.makedirs(sorted_dir, exist_ok=True)

sort_images(id_df, all_image_dir=image_dir, output_dir=sorted_dir)
```

Now the flat directory, `all_images` has been copied to a new folder, `sorted_images`, that is organized by proposed ID, then encounter.

```
sorted_images
├── ID-0000
│   ├── enc0
│   │   ├── 0a49385ef8f1e74a.jpg
│   │   ├── 4d69031e07ef3393.jpg
│   │   ├── b4f1ca6229180f18.jpg
│   │   └── e0016bed0be5bf9f.jpg
│   ├── enc10
│   │   ├── 0cdbe7c151420b0c.jpg
│   │   ├── 19dd8e9db3a26066.jpg
            ...
│   └── enc3
│       ├── 1ced4b1e3bd63781.jpg
            ...
│       └── bc17d3aa572320cc.jpg
├── ID-0001
│   ├── enc0
│   │   ├── e7dbbe01ac71d6e8.jpg
│   │   └── f86cdadd6ecb59aa.jpg
│   ├── enc2
│   │   ├── 2b66eacdced6c165.jpg
│   │   ├── 7ce2b670757443de.jpg
            ... 
├── ID-0206
│   └── enc4
│       └── 767bdedefe51aabb.jpg
└── ID-0207
    └── enc4
        └── e345d4ddaae7db02.jpg
```

## Conclusion

We have demonstrated one possible workflow for using Pyseter. A potential user would manually grade images for quality and crop images of sufficient quality to the identifying mark. Then, they would use Pyseter to grade these images for distinctiveness and to sort the images above a distinctiveness threshold into folders, selecting the highest quality image of each proposed individual. Finally, the user would compare these images against the reference set, either manually or with an interface such as Happywhale.

Nevertheless, Pyseter offers users with cetacean datasets flexibility to conduct several kinds of analyses because it allows them to extract feature vectors from images with AnyDorsal. For example, users could compute the false negative rate for their dataset, which is useful for population assessments [@patton-2025-optimizing]. Similarly, one could use Pyseter to evaluate the predictive performance of AnyDorsal on a species that was outside the training dataset. Further, after the initial sort, users could clean up the results by doing a second round of clustering and sorting. As Pyseter matures, we plan to add documentation on how to conduct such analyses with Pyseter. Finally, as photo-ID technology improves, e.g., automated quality grading, we plan on adding these algorithms to Pyseter. 

## Supplements

### Hierarchical Agglomerative Clustering

Below is a demonstration of how to use the HAC algorithm to cluster images, then sort them into folders.

```{python}
from pyseter.sort import HierarchicalCluster, format_ids

hc = HierarchicalCluster(match_threshold=0.5)
hac_idx = hc.cluster_images(feature_array)

# format_ids converts the integer labels to something like 'ID-0001'
hac_labels = format_ids(hac_idx)
report_cluster_results(hac_labels)

hac_df = pd.DataFrame({'image': filenames, 'proposed_id': hac_labels})
encounter_info = pd.read_csv(working_dir + '/encounter_info.csv')
hac_df = hac_df.merge(encounter_info)

# separate directory for the hac images
hac_dir = working_dir + '/sorted_images_hac'
os.makedirs(hac_dir, exist_ok=True)
sort_images(hac_df, all_image_dir=image_dir, output_dir=hac_dir)
```

`HierarchicalCluster` is useful for large datasets, yet will be more prone to false negative errors. In this example, it found 60 more clusters (proposed IDs) than the network matching, which may be dubious. Users will have to decide how to balance false positive versus false negative matches. For example, we recommend that users preprocess their images with Pyseter, then identify animals in the pre-processed images manually or a program such as Happywhale. This second round of identification should help clean up false negative matches. As such, users following this approach might be more averse to false positive errors in the first stage. 

### Resizing images with Pillow

Here is how is one example on how to do so with the Pillow library. 

```{python}
#| eval: false
from PIL import Image

new_size = 512, 512

# files for resizing and where they'll be sent
all_images = [i for i in os.listdir(image_dir) if i.endswith('jpg')]
new_dir = os.path.join(working_dir, 'thumbnails')
os.makedirs(new_dir, exist_ok=True)

# resize and save the files 
for file in all_images:
    old_path = os.path.join(image_dir, file)
    with Image.open(old_path) as im:
        im.thumbnail(new_size)
        new_path = os.path.join(new_dir, file)
        im.save(new_path)
```


## Reference {.unnumbered}