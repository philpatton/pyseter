---
title: Evaluating *AnyDorsal*
jupyter:
  kernelspec:
    display_name: Python (Pyseter)
    language: python
    name: pyseter_env
execute:
  cache: true
---

In this notebook, we'll demonstrate how do evaluate *AnyDorsal*'s' performance on a test dataset. We'll use the [Happy Whale and Dolphin Kaggle competition dataset](https://www.kaggle.com/competitions/happy-whale-and-dolphin/data) as an example. You can download the data by following that linked page (click the big "Download all" button). FYI, you'll have to create an account first.

In the [Predicting IDs](identify.qmd) notebook, we demonstrated how to extract features for the Happywhale dataset using a set of bounding boxes. Here, we'll load the features in.

```{python}
%config InlineBackend.figure_format = 'retina'

from pyseter.sort import load_features
from pyseter.identify import predict_ids
import matplotlib.pyplot as plt
import pandas as pd

# load in the feature vectors
data_dir = '/Users/PattonP/datasets/happywhale/'
feature_dir = data_dir + '/features'

reference_path = feature_dir + '/train_features.npy'
reference_files, reference_features = load_features(reference_path)

query_path = feature_dir + '/test_features.npy'
query_files, query_features = load_features(query_path)
```

In order to evaluate the performance of *AnyDorsal* on the test set, we'll need to know the IDs of animals in the train set and the test set. 

```{python}
data_url = (
    'https://raw.githubusercontent.com/philpatton/pyseter/main/' 
    'data/happywhale-ids.csv'
)
id_df = pd.read_csv(data_url)

# fix known species errors
id_df.replace(
    {
        "globis": "short_finned_pilot_whale",
        "pilot_whale": "short_finned_pilot_whale",
        "kiler_whale": "killer_whale",
        "bottlenose_dolpin": "bottlenose_dolphin"
    }, 
    inplace=True
)

id_df.head(10)
```

```{python}
#| code-fold: true
# excel on mac corrupts the IDs (no need to do this on PC or linux)
id_df['individual_id'] = id_df['individual_id'].apply(
    lambda x: str(int(float(x))) if 'E+' in str(x) else x
)
```

Now we'll predict the IDs in the query set. 

```{python}
query_dict = dict(zip(query_files, query_features))
reference_dict = dict(zip(reference_files, reference_features))

prediction_df = predict_ids(reference_dict, query_dict, id_df, proposed_id_count=5)
prediction_df.head(20)
```

## Top 5%

We can evaluate the performance with several metrics, including Top 5%, or top 5 accuracy, which is just the percentage of times that the correct answer was one of the top five predictions. To do so, we need to compute the `rank` of the prediction, that is, the location of the correct ID in the proposed IDs for every image in the test set. 

One way to do so is to merge the ID DataFrame with the prediction DataFrame, using both `image` and ID columns as keys. We'll do a left join, such that when the true ID is one of the proposed IDs, the prediction, its rank and score are returned. When true true ID is not one of the proposed IDs, we'll get NAs for those columns. We can check to see that it worked by checking the number of rows in the result (there are `27956` images in the testing dataset).

```{python}
# add the predictions and the scores to the id dataframe
performance_df = id_df.merge(
    prediction_df,
    how='left',
    left_on=['image', 'individual_id'],
    right_on=['image', 'predicted_id']
)

# filter out the training images
train_images = pd.read_csv(data_dir + '/train.csv').image
performance_df = performance_df.loc[~performance_df.image.isin(train_images)]
print(performance_df.shape)
```

```{python}
performance_df.head(20)
```

Any image with an `NA` rank means that the true ID was not one of the `proposed_ids`. We'll fill those with some large value, say, `9999`. Then, we just need to compute the average number of times that the rank was less than 6.

```{python}
performance_df['rank'] = performance_df['rank'].fillna(9999)
performance_df['in_top_5'] = performance_df['rank'] < 6
print(f'Top 5%: {performance_df.in_top_5.mean():0.1%}')
```

We can see that the correct answer was in the top 5 predictions for 92% of test images. For fun, we can look at this performance across species.

```{python}
#| fig-cap: Percentage of images (blue dots) where the correct ID was in the top 5 proposed IDs for each species
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick 

plt.style.use('fivethirtyeight')
plt.rcParams['axes.facecolor'] = 'white'
plt.rcParams['figure.facecolor'] = 'white'

map_df = (
    performance_df.groupby('species')
        .in_top_5
        .mean()
        .rename('top5')
        .reset_index()
        .sort_values('top5')
)
map_df.species = map_df.species.str.replace('_', ' ')

fig, ax = plt.subplots(figsize=(4, 8))
ax.scatter(map_df.top5, map_df.species)
for row in map_df.itertuples():
    ax.text(row.top5 - 0.02, row.species, f'{row.top5:0.0%}', ha='right', 
            va='center', fontsize=12)
ax.xaxis.set_major_formatter(mtick.PercentFormatter(1))
ax.set_title('AnyDorsal Performance')
ax.set_xlabel('Top 5%')
ax.set_xlim((0,1.03))
plt.show()
```

We can see that the 12 of 24 species achieve a Top 5% of over 95%. Some of these species are being rewarded (or punished) by the prevalence of `new_individual`s in the dataset, since `new_individual` is almost always one of the top 5 proposed IDs. 

## Mean average precision

We can also evaluate *AnyDorsal* with Mean Average Precision (MAP), a slightly more rigorous metric. MAP evaluates a set of predictions, in this case, a set of 5 predictions. For a set of five ordered predictions, the precision score will be $1/1 = 1$ if the first prediction is correct, $1/2$ if the second is correct, and so on until $1/5$ if the fifth prediction is correct, or $0$ if none of the five predictions are correct. As such, the precision is just the reciprocal of the rank. Recall that we set the `NA` values to `9999` earlier, and $1/9999 \approx 0$.  MAP is the mean precision score for a set.

```{python}
performance_df['precision'] = 1 / performance_df['rank'] 
map5 = performance_df.precision.mean()
print(f'MAP@5: {map5:0.3f}')
```

For fun, we can look at the results by species

```{python}
#| fig-cap: Mean average precision (blue dots) of predictions for each species.

map_df = (
    performance_df.groupby('species')
        .precision
        .mean()
        .rename('MAP')
        .reset_index()
        .sort_values('MAP')
)
map_df.species = map_df.species.str.replace('_', ' ')

fig, ax = plt.subplots(figsize=(4, 8))
ax.scatter(map_df.MAP, map_df.species)
for row in map_df.itertuples():
    ax.text(row.MAP - 0.02, row.species, f'{row.MAP:0.2f}', ha='right', 
            va='center', fontsize=12)
ax.set_xlim((0,1.01))
ax.set_title('AnyDorsal Performance')
ax.set_xlabel('Mean average precision')
plt.show()
```

