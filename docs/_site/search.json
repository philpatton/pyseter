[
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Introduction to pyseter",
    "section": "",
    "text": "Pyseter is an Python package for sorting images by an automatically generated ID. The main functions of Pyseter are:\nThis notebook will walk you through each major function. First, let’s make sure that Pyseter is properly installed, and that it can access Pytorch.\nimport pyseter\n\npyseter.verify_pytorch()\n\n✓ PyTorch 2.7.1+cu126 detected\n✓ CUDA GPU available: NVIDIA RTX A4000\nIf you’re on a Mac, you should see something like\nPlease note, however, that AnyDorsal consumes quite a bit of memory. As such, only Apple Silicon devices with 16 GB or more of memory will work. Ideally, future versions of Pyseter will use a smaller model.\nIf neither Apple Silicon or an NVIDIA GPU are available, you will see a message like this.\nPackages in Python tend to be subdivided into modules based on their functions. In Pyseter, the sort module contains functions for sorting files, including other forms of file management."
  },
  {
    "objectID": "tutorial.html#optional-folder-management",
    "href": "tutorial.html#optional-folder-management",
    "title": "Introduction to pyseter",
    "section": "Optional: Folder management",
    "text": "Optional: Folder management\nThe main purpose of Pyseter is organizing images into folders. To do keep things clean and tidy, we recommend establishing a working directory with a subfolder, e.g., called, all images, that contains every image you want to be sorted (see below for a different case). Optionally, you might want to have a .csv with encounter information in the working directory. This .csv would contain two columns: one for the image name, i.e., every image in all images, and another for the encounter. As such, the working directory would look like this.\nworking directory\n├── encounter_info.csv\n├── all images\n│   └──00cef32dc62b0f.jpg\n│   └──3ecc025ea6f9bf.jpg\n│   └──9f18762a48696b.jpg\n│   └──36f78517a512dd.jpg\n│   └──470d524b4d5303.jpg\n       ...\n│   └──4511c9e5cb7acb.jpg\nSometimes, you might have your images organized into subfolders by encounter.\nworking_dir\n└── original_images\n    ├── SL_HI_006_20220616 (CROPPED)\n    │   ├── 2022-06-16_CLD500_CL_006.JPG\n    │   ├── 2022-06-16_CLD500_CL_007.JPG\n    │   ├── 2022-06-16_CLD500_CL_008.JPG\n    │   ├── 2022-06-16_CLD500_CL_021.JPG\n    │   ├── 2022-06-16_CLD500_CL_042.JPG\n...\n    ├── SL_HI_007_20220616 (CROPPED)\n    │   ├── 2022-06-16_CLD500_CL_346.JPG\n    │   ├── 2022-06-16_CLD500_CL_347.JPG\n    │   ├── 2022-06-16_CLD500_CL_371.JPG\n    │   ├── 2022-06-16_CLD500_CL_372.JPG\nIn this case, you might want to accomplish two tasks: move all these images to one folder, e.g., all_images, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The prep_images() function does just that.\n\nfrom pyseter.sort import prep_images\n\n# various directories we'll be working with\nworking_dir = '/home/pattonp/koa_scratch/id_data/working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new directory containing every image\nimage_dir = working_dir + '/all_images'\n\n# copy images to a single folder, then \nprep_images(original_image_dir, all_image_dir=image_dir)\n\nCopied 1230 images to: /home/pattonp/koa_scratch/id_data/working_dir/all_images\nSaved encounter information to: /home/pattonp/koa_scratch/id_data/working_dir/encounter_info.csv\n\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nIn python, you can concatenate strings with the + operator. This is equivalent to paste0(working_dir, '/original_images') in R.\n\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nPackages in Python tend to be subdivided into modules based on their functions. In Pyseter, the sort module contains functions for sorting files, including other forms of file management."
  },
  {
    "objectID": "tutorial.html#extracting-features",
    "href": "tutorial.html#extracting-features",
    "title": "Introduction to pyseter",
    "section": "1. Extracting features",
    "text": "1. Extracting features\nPyseter identifies individuals by extracting feature vectors from images. Feature vectors summarize three-dimensional images into one-dimensional vectors that are useful for the task at hand, in this case, individual identification.\nPyseter extracts feature vectors with AnyDorsal, an algorithm for identifying whales and dolphins of many species. AnyDorsal is the same dorsal identification algorithm that’s included with Happywhale (although not to be confused with their humpback whale fluke ID algorithm).\nBefore we extract the feature vectors, let’s first create a subfolder within our working directory to save them in. This isn’t necessary, yet keeps things tidy.\n\nimport os\n\n# in case you want to save the features after extracting them \nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nThe module, os, is part of Python’s standard library. People often refer to R and its standard libraries as “base R.” Base R includes the stats library, which provides the function rnorm. The os module has many functions for tinkering with your operating system.\n\n\nWe will extract features with the FeatureExtractor class. To do so, we first need to initialize the class. This sets up important parameters, such as the batch_size, which is the number of images that will be processed in parallel. Larger batches should run faster, although your mileage may vary. If you specify too large of a batch, you may encounter an OutOfMemoryError (see below for an example). If you encounter this error, try specifying a larger batch size. If you encounter this error with a very small batch size (say, 2), you may need to resize your images. You can do this manually by reducing the file size in an image editing software, or with Python\nOutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 5.81 GiB of which 9.06 MiB is free. Including non-PyTorch memory, this process has 5.76 GiB memory in use. Of the allocated memory 5.64 GiB is allocated by PyTorch, and 50.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nPython error messages are comically long, putting CVS receipts to shame. This is because they show the entire traceback, i.e., this error caused this error caused this error, etc. To quickly diagnose the problem, scroll to the bottom of the message. Then, you can further dissect it by scrolling up.\n\n\n\nfrom pyseter.extract import FeatureExtractor\n\n# specify the configuration for the extractor \nfe = FeatureExtractor(batch_size=4,)\n\nUsing device: cuda (NVIDIA RTX A4000)\n\n\nOnce we’ve initialize the class, we can use its associated methods (functions). In this case, the only one we are interested in is extract(), which extracts a feature vector for every image in a specified directory. This can take several minutes, so we typically save the results afterwards.\n\n\n\n\n\n\nTipR user tip\n\n\n\nClasses and methods also exist in R, but operate more behind the scenes. For example, x &lt;- data.frame() initializes an object of class data.frame, and summary(x) calls the summary method for data.frames. Python makes this relationship more explicit. For example, the equivalent (although nonsensical) Python code would be x = data.frame() and x.summary().\n\n\n\nimport numpy as np\n\n# extract the features for the input directory then save them\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves them as an numpy array\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\nLoading model...\n\n\n/home/pattonp/.conda/envs/pyseter/lib/python3.13/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_l2_ns to current tf_efficientnet_l2.ns_jft_in1k.\n  model = create_fn(\n\n\nWarning: Missing keys when loading pretrained weights: ['head.weight']\nWarning: Unexpected keys when loading pretrained weights: ['head.fc.weight']\nExtracting features...\n\n\n100%|██████████| 308/308 [01:48&lt;00:00,  2.84it/s]\n\n\nThe object features is a dictionary, whose keys are the filenames and whose values are the feature vectors associated with each filename. This helps ensure that each image is associated with the correct feature vector. Nevertheless, it can be easier to work with actual numpy arrays. To do so, convert the keys to a list, then to a numpy array.\n\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nThe R objects that Python’s dictionary most resemble is the named vector or the list. Like a list, dictionaries can hold different data types. Unlike a list, dictionaries have no order, and therefore cannot be integer indexed.\n\n\nIf you’ve already extracted and saved features, you can load them with the code below.\n\n# import numpy as np\n# out_path = feature_dir + '/features.npy'\n# features = np.load(out_path, allow_pickle=True).item()\n# filenames = np.array(list(features.keys()))\n# feature_array = np.array(list(features.values()))"
  },
  {
    "objectID": "tutorial.html#clustering-images-by-proposed-id",
    "href": "tutorial.html#clustering-images-by-proposed-id",
    "title": "Introduction to pyseter",
    "section": "2. Clustering images by proposed ID",
    "text": "2. Clustering images by proposed ID\nPyseter comes with two algorithm’s for clustering images by proposed ID. Network clustering works better for small datasets. To use network clustering, we first need to compute the similarity scores between each pair of images.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarity_scores = cosine_similarity(feature_array)\n\nThis indicates how similar the individuals in each image are.\nNext, we can perform the clustering. Each cluster represents a proposed ID. To access the “cluster labels” (these are just integers representing the cluster), we can access the cluster_idx attribute, i.e, results.cluster_idx.\n\nfrom pyseter.sort import NetworkCluster, report_cluster_results\n\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n\nFollowing clusters may contain false positives:\n['ID_0013', 'ID_0040', 'ID_0061', 'ID_0079', 'ID_0086', 'ID_0110']\n\n\nNetwork clustering has one major hyperparameter, namely, the match_threshold, which indicates whether two images should be grouped within a cluster. That is, if the similarity score between two images is above a certain threshold, we cluster them into a proposed ID. High thresholds mean that few images will be clustered together, creating many clusters. Very low thresholds mean that many images will be clustered together, creating few clusters. report_cluster_results produces a quick and dirty summary of the number of clusters created, and the size of the largest cluster (i.e., the number of images associated to the most photographed individual). This is a quick sanity check.\n\nnetwork_idx = results.cluster_idx\nreport_cluster_results(network_idx)\n\nFound 245 clusters.\nLargest cluster has 30 images.\n\n\nYou’ll also note that nc.cluster_images() warns that some clusters may contain “false positives.” False positive matches occur when two separate individuals fall under the same proposed ID. We can diagnose possible false positives by evaluating the network. In this case, the network consists of nodes (images) and edges, which represent connections between images. Two images are connected when they have a similarity score above the threshold. A blob of connected nodes (i.e., connected components) represents a proposed ID.\nSometimes, the connected components look less like a blob and more like a barbell, where two sets of images have many connections amongst each other, yet these two blobs are only connected by one link. We suspect that such clusters contain false positives, i.e., two sets of images for two individuals connected by one spurious link. We can plot the networks of the suspicious clusters with results.plot_suspicious().\n\nresults.plot_suspicious()\n\n\n\n\n\n\n\n\nID_0033 clearly has a barbell shape, suggesting that this cluster consists of images for two individuals linked together by one spurious connection.ID_0013 also seems dubious, and ID_0060 might consist of four separate individuals.\nUsers can deal with suspicious clusters in several ways. First, raising the match threshold should reduce the number of false positive matches. That said, this will increase the false negative rate, whereby images of one individual are spread across multiple proposed IDs. Alternatively, users can manually inspect the images within the clusters and, if need be, divide up the images.\nAs the number of images being clustered grows, the overall false positive rate also grows (this is analogous the multiple comparison problem in statistics). At some point,the network matching becomes untenable; all but the highest match thresholds would produce too many false positives to be useful.\nFor these cases, there is HierarchicalCluster, which relies on the Hierarchical Agglomerative Clustering algorithm provided by the popular machine learning package, scikit-learn. Note that HierarchicalCluster will run noticeably slower than the NetworkCluster.\n\nfrom pyseter.sort import HierarchicalCluster, format_ids\n\n# initialize the object then cluster away! \nhc = HierarchicalCluster(match_threshold=0.5)\nhac_idx = hc.cluster_images(feature_array)\n\nThe HierarchicalCluster results object is much simpler, in that it just returns the cluster indices for each image. We can make these labels a little prettier with the format_ids function.\n\nhac_labels = format_ids(hac_idx)\nprint(hac_labels[:5])\n\n# quick summary of the clustering results\nreport_cluster_results(hac_labels)\n\n['ID-0092', 'ID-0159', 'ID-0075', 'ID-0000', 'ID-0034']\nFound 306 clusters.\nLargest cluster has 19 images.\n\n\nHierarchicalCluster is useful for large datasets, yet will be more prone to false negative errors. In this example, it found 60 more clusters (proposed IDs) than the network matching, which may be dubious. Users will have to decide for themselves how to balance false positive versus false negative matches. For example, we recommend that users preprocess their images with Pyseter, then identify animals in the pre-processed images manually or with Happywhale. This second round of identification should help clean up false negative matches. As such, users following this approach might be more averse to false positive errors in the first stage."
  },
  {
    "objectID": "tutorial.html#sorting-images-by-proposed-id",
    "href": "tutorial.html#sorting-images-by-proposed-id",
    "title": "Introduction to pyseter",
    "section": "3. Sorting images by proposed ID",
    "text": "3. Sorting images by proposed ID\nOnce we’ve identified individuals, we can sort them into folder by proposed ID and encounter. To do so, we need to create a pandas DataFrame that indicates the proposed ID and encounter for each filename. Recall that we created the encounter_info.csv with the prep_images() function above. For this sort, we’ll use the NetworkCluster results\n\nimport pandas as pd\n\n# create a dataframe proposed id and encounter for each image\nnetwork_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})\nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nnetwork_df = network_df.merge(encounter_info)\n\nnetwork_df.head()\n\n\n\n\n\n\n\n\nimage\nproposed_id\nencounter\n\n\n\n\n0\n2022-06-25_CLD500_CL_735.jpg\nID-0000\nSL_HI_016_20220625 (CROPPED)\n\n\n1\n2021-11-17_Hilo_AP_2_UNK_0330.jpg\nID-0001\nSL_Hilo_13_20211117 (CROPPED)\n\n\n2\n2022-06-20_CLD500_CL_1166.jpg\nID-0002\nSL_HI_010_20220620 (CROPPED)\n\n\n3\n2022-06-16_D750_ANNM_062.JPG\nID-0003\nSL_HI_006_20220616 (CROPPED)\n\n\n4\n2022-06-20_CLD500_CL_453.jpg\nID-0004\nSL_HI_010_20220620 (CROPPED)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningFormatting the ID DataFrame\n\n\n\nThe ID DataFrame must have columns named image, proposed_id, and encounter. Otherwise sort_images will not work. Luckily, changing column names in pandas is straightforward.\nnetwork_df.columns = ['image', 'proposed_id', 'encounter']\n\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nPandas is a package for managing dataframes, and is a straight knock-off of R’s data.frame. In my experience, most R users lose patience not with Python, but with pandas. Pandas is just similar enough to R that one should be able to directly port R ideas over. However, key differences between pandas and R prevent this, causing immense frustration (especially for yours truly!)\n\n\nTo sort the images, we need to specify an output directory, then run the sort_images function.\n\nfrom pyseter.sort import sort_images\n\n# make an output directory \nsorted_dir = working_dir + '/sorted_images'\nos.makedirs(sorted_dir, exist_ok=True)\n\n# sort the images into folders based on proposed id\nsort_images(network_df, all_image_dir=image_dir, output_dir=sorted_dir)\n\nSorted 1230 images into 335 folders.\n\n\nWe can check to see that this worked by plotting a grid of images with matplotlib.\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# get the files associated with the first ID\nfirst_id_dir = sorted_dir + '/ID-0000'\nthis_encounter = os.listdir(first_id_dir)[0]\nencounter_dir = first_id_dir + '/' + this_encounter\nencounter_files = os.listdir(encounter_dir)\n\n# plot a grid of images \nfig, axes = plt.subplots(3, 3, tight_layout=True)\nfor i, filename in enumerate(encounter_files[:9]):\n    path = encounter_dir + '/' + filename\n    image = Image.open(path)\n    axes.flat[i].imshow(image)\n    axes.flat[i].axis('off')"
  },
  {
    "objectID": "install-r.html",
    "href": "install-r.html",
    "title": "R users who are new to Python",
    "section": "",
    "text": "We expect that most people using Pyseter will be familiar with R, and completely new to Python. This raises the question: why release Pyseter as a Python package? The answer is that Python is much more suited to deep learning than R, and most of the new developments in automated photo-identification rely on deep learning.\nAs such, Pyseter users will have to familiarize themselves with a few Python concepts, such as conda and Jupyter, before getting started.",
    "crumbs": [
      "Installation",
      "Users new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-conda",
    "href": "install-r.html#install-conda",
    "title": "R users who are new to Python",
    "section": "Install conda",
    "text": "Install conda\nConda is an important tool for managing packages in Python. While R, for the most part, handles packages for you behind the scenes, Python requires a more hands on approach.\nTo get started, we first need to install a package manager called conda. There are many forms of conda, with Anaconda being the most popular. For several reasons, we prefer another form of conda called Miniforge.\n\nDownload and install Miniforge (a form of conda)\n\nAfter installing, you can verify your installation by opening the command line interface (CLI), which will depend on your operating system. Are you on Windows? Open the “miniforge prompt” in your start menu. Are you on Mac? Open the Terminal application. Then, type the following command into the CLI and hit return.\nconda --version\nYou should see something like conda 25.5.1. Of course, Anaconda, miniconda, mamba, or any other form of conda will work too.",
    "crumbs": [
      "Installation",
      "Users new to Python"
    ]
  },
  {
    "objectID": "install-r.html#create-a-new-environment",
    "href": "install-r.html#create-a-new-environment",
    "title": "R users who are new to Python",
    "section": "Create a new environment",
    "text": "Create a new environment\nThen, you’ll create an environment for the package will live in. Environments are walled off areas where we can install packages. This allows you to have multiple versions of the same package installed on your machine, which can help prevent conflicts.\nEnter the following two commands into the CLI:\nconda create -n pyseter_env\nconda activate pyseter_env\nHere, I name (hence the -n) the environment pyseter_env, but you can call it anything you like!\nNow your environment is ready to go! Try installing your first package, pip. Pip is another way of installing Python packages, and will be helpful for installing PyTorch and Pyseter (see below). To do so, enter the following command into the CLI.\nconda install pip -y\nIn this case, we include the -y argument so we don’t have to immediately answer yes to the next question. Once this is working, you’re ready to proceed to the next section.",
    "crumbs": [
      "Installation",
      "Users new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-pytorch",
    "href": "install-r.html#install-pytorch",
    "title": "R users who are new to Python",
    "section": "Install PyTorch",
    "text": "Install PyTorch\nInstalling PyTorch will allow users to extract features from images, i.e., identify individuals in images. Extracting features should be reasonably quick for users with an NVIDIA GPU (e.g., 2 minutes per 1000 images). In our testing, feature extraction takes about 5 times longer for users with Apple Silicon (e.g., 10 minutes per 1000 images).\n\n\n\n\n\n\nWarning\n\n\n\nFeature extraction will be extremely slow for users who do not have access to an NVIDIA GPU or Apple Silicon. Users may want to check if their university or agency has access to GPUs via a high performance computing cluster (HPC). Also, Google Colab and other similar services (e.g., Kaggle) allow users to rent GPU resources.\n\n\nPyTorch installation can be a little finicky. Essentially, it depends on what operating system you’re using, and what version of CUDA you’re using. CUDA is the technology that turns your NVIDIA GPU into a deep learning machine. To install PyTorch, I recommend following these instructions.\n\nWindows users\nBelow is an example for Windows users. This relies on CUDA 12.8, which should work for most people. If you haven’t already, open the CLI (e.g., the miniforge prompt). Then activate your environment before installing.\nconda activate pyseter_env\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128\nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\nMac users\nBelow is an example for Mac users. The Mac version of PyTorch relies on MPS acceleration, which is good but not at the level of CUDA. If you haven’t already, open you’re command line interface (e.g., the miniforge prompt). Then activate your environment before installing.\nconda activate pyseter_env\npip3 install torch torchvision \nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\n\n\n\n\nWarning\n\n\n\nMac users will need to have an Apple Silicon processor, e.g., an M1 chip. Additionally, they will need at least 16 GB of memory (RAM) to use AnyDorsal.",
    "crumbs": [
      "Installation",
      "Users new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-pyseter",
    "href": "install-r.html#install-pyseter",
    "title": "R users who are new to Python",
    "section": "Install Pyseter",
    "text": "Install Pyseter\nNow, install Pyseter. If you haven’t already, activate your environment before installing.\nconda activate pyseter_env\npip3 install pyseter\nNow you’re ready to go!",
    "crumbs": [
      "Installation",
      "Users new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-vs-code-or-positron",
    "href": "install-r.html#install-vs-code-or-positron",
    "title": "R users who are new to Python",
    "section": "Install VS Code or Positron",
    "text": "Install VS Code or Positron\nMost users will interact with Pyseter via a Jupyter Notebook. There are many methods for opening, editing, running, and saving Jupyter Notebooks. We are personally biased towards VS Code. Alternatively, R users might also try out Positron. The team at Posit (formerly, R Studio) developed Positron from the open source version of VS Code, but with R users in mind. As such, it might be a nice hybrid option. That said, we have found Positron to throw confusing errors and have found VS Code to be more stable.\n\nDownload and install VS Code\n\nOpen VS Code, then click “File -&gt; Open Folder”. Navigate to wherever you’d like to work, then click “New Folder.” You can call this folder something like “learn-pyseter” or “pyseter-jobs”. Open the new folder. Click “File -&gt; New File” then select Jupyter Notebook. Click “Select Kernel” in the top right corner, select “Python environments” and then “pyseter_env”, or whatever you named your environment. For more information, check out this great overview of using Jupyter Notebooks in VS Code.\nNow you’re ready to proceed to the next section.",
    "crumbs": [
      "Installation",
      "Users new to Python"
    ]
  },
  {
    "objectID": "install-r.html#verify-the-installation",
    "href": "install-r.html#verify-the-installation",
    "title": "R users who are new to Python",
    "section": "Verify the installation",
    "text": "Verify the installation\nVerify the Pyseter installation by running the following cell in your notebook.\nimport pyseter\npyseter.verify_pytorch()\nIf you’re on a windows computer with an NVIDIA GPU, you should see something like\n✓ PyTorch 2.7.1+cu126 detected\n✓ CUDA GPU available: NVIDIA A30 MIG 2g.12gb\nOnce this is working, you’re ready to check out the “General Overview” notebook in the examples folder of this repository!",
    "crumbs": [
      "Installation",
      "Users new to Python"
    ]
  },
  {
    "objectID": "install-r.html#anydorsal-weights",
    "href": "install-r.html#anydorsal-weights",
    "title": "R users who are new to Python",
    "section": "AnyDorsal weights",
    "text": "AnyDorsal weights\nPyseter relies on the AnyDorsal algorithm to extract features from images. The first time you use the FeatureExtractor, Pyseter will download the AnyDorsal weights from Hugging Face. The weights take up roughly 4.5 GB. As such, to use the FeatureExtractor, users must have enough storage space to accommodate the weights.",
    "crumbs": [
      "Installation",
      "Users new to Python"
    ]
  },
  {
    "objectID": "api/sort.prep_images.html",
    "href": "api/sort.prep_images.html",
    "title": "sort.prep_images",
    "section": "",
    "text": "sort.prep_images(image_dir, all_image_dir)\nCopy all images to a flat directory and save a csv with encounter info.\nSome users may have their image directory structured such that each image is in a subfolder by encounter, e.g., original_images/enc1/img1.jpg. The FeatureExtractor in pyseter.extract prefers flat directories. prep_images flattens the original_images directory by copying every image to all_image_dir, then saves a csv with encounter information to the working directory.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nPath to directory containing images.\nrequired\n\n\nall_image_dir\nstr\nPath to new directory where user wants to copy all their images.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nSaves images to the all_image_dir and the encounter information to the csv in the working_dir.\n\n\n\n\n\n\nFor a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.sort import prep_images\n\n# old directory, structured by encounter\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)"
  },
  {
    "objectID": "api/sort.prep_images.html#parameters",
    "href": "api/sort.prep_images.html#parameters",
    "title": "sort.prep_images",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nPath to directory containing images.\nrequired\n\n\nall_image_dir\nstr\nPath to new directory where user wants to copy all their images.\nrequired"
  },
  {
    "objectID": "api/sort.prep_images.html#returns",
    "href": "api/sort.prep_images.html#returns",
    "title": "sort.prep_images",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nSaves images to the all_image_dir and the encounter information to the csv in the working_dir."
  },
  {
    "objectID": "api/sort.prep_images.html#examples",
    "href": "api/sort.prep_images.html#examples",
    "title": "sort.prep_images",
    "section": "",
    "text": "For a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.sort import prep_images\n\n# old directory, structured by encounter\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)"
  },
  {
    "objectID": "api/identify.predict_ids.html",
    "href": "api/identify.predict_ids.html",
    "title": "identify.predict_ids",
    "section": "",
    "text": "identify.predict_ids(reference_dict, query_dict, id_df, proposed_id_count=10)\nPredict the identities of individuals in the query set\nReturn a DataFrame of the most proposed_id_count most similar individuals in the reference set for each query image, along with their cosine similarity.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreference_dict\ndict\nDictionary where the key is the reference image’s name. The value associate with each key is a NumPy array of shape (M, ) where M is the number of features in the feature vector.\nrequired\n\n\nquery_dict\ndict\nDictionary where the key is the query image’s name. The value associate with each key is a NumPy array of shape (M, ) where M is the number of features in the feature vector.\nrequired\n\n\nid_df\npd.DataFrame\nDataFrame containing the identities, individual_id, and image file name, image, for every image in the reference set.\nrequired\n\n\nproposed_id_count\ninteger\nThe number of proposed IDs to return for each query image.\n10\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyseter.identify import predict_ids\n&gt;&gt;&gt; \n&gt;&gt;&gt; ref_dict = {'image1': np.array([0.1, 0.11])}\n&gt;&gt;&gt; query_dict = {'image2': np.array([0.1, 0.12])}\n&gt;&gt;&gt; id_df = pd.DataFrame({'image': 'image1', 'individual_id': 'a'})\n&gt;&gt;&gt; \n&gt;&gt;&gt; results = predict_ids(ref_dict, query_dict, id_df, proposed_id_count=1)\n&gt;&gt;&gt; len(results)\n1"
  },
  {
    "objectID": "api/identify.predict_ids.html#parameters",
    "href": "api/identify.predict_ids.html#parameters",
    "title": "identify.predict_ids",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nreference_dict\ndict\nDictionary where the key is the reference image’s name. The value associate with each key is a NumPy array of shape (M, ) where M is the number of features in the feature vector.\nrequired\n\n\nquery_dict\ndict\nDictionary where the key is the query image’s name. The value associate with each key is a NumPy array of shape (M, ) where M is the number of features in the feature vector.\nrequired\n\n\nid_df\npd.DataFrame\nDataFrame containing the identities, individual_id, and image file name, image, for every image in the reference set.\nrequired\n\n\nproposed_id_count\ninteger\nThe number of proposed IDs to return for each query image.\n10"
  },
  {
    "objectID": "api/identify.predict_ids.html#examples",
    "href": "api/identify.predict_ids.html#examples",
    "title": "identify.predict_ids",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyseter.identify import predict_ids\n&gt;&gt;&gt; \n&gt;&gt;&gt; ref_dict = {'image1': np.array([0.1, 0.11])}\n&gt;&gt;&gt; query_dict = {'image2': np.array([0.1, 0.12])}\n&gt;&gt;&gt; id_df = pd.DataFrame({'image': 'image1', 'individual_id': 'a'})\n&gt;&gt;&gt; \n&gt;&gt;&gt; results = predict_ids(ref_dict, query_dict, id_df, proposed_id_count=1)\n&gt;&gt;&gt; len(results)\n1"
  },
  {
    "objectID": "api/sort.process_images.html",
    "href": "api/sort.process_images.html",
    "title": "sort.process_images",
    "section": "",
    "text": "sort.process_images\nsort.process_images(image_root, all_image_dir)\nCopy all images to a temporary directory and return encounter information"
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Extracting feature vectors from images\n\n\n\nextract.FeatureExtractor\nExtract features from images.\n\n\n\n\n\n\nPredicting IDs in a query set with a reference set\n\n\n\nidentify.predict_ids\nPredict the identities of individuals in the query set\n\n\n\n\n\n\nClustering images by proposed IDs\n\n\n\nsort.HierarchicalCluster\nHierarchical clustering of images\n\n\nsort.NetworkCluster\nNetwork clustering of images\n\n\nsort.ClusterResults\nStoring NetworkCluster results.\n\n\nsort.prep_images\nCopy all images to a flat directory and save a csv with encounter info.\n\n\nsort.sort_images\nSort images into subfolders by proposed ID then encounter.\n\n\nsort.load_features\nLoad previously extracted features.\n\n\n\n\n\n\nGrade images by distinctiveness\n\n\n\ngrade.rate_distinctiveness\nGrade images by their distinctiveness.\n\n\n\n\n\n\nFunctions to make sure PyTorch is working\n\n\n\nget_best_device\nSelect torch device based on expected performance.\n\n\nverify_pytorch\nVerify PyTorch installation and show device options.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api/index.html#feature-extraction",
    "href": "api/index.html#feature-extraction",
    "title": "API Reference",
    "section": "",
    "text": "Extracting feature vectors from images\n\n\n\nextract.FeatureExtractor\nExtract features from images.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api/index.html#identify",
    "href": "api/index.html#identify",
    "title": "API Reference",
    "section": "",
    "text": "Predicting IDs in a query set with a reference set\n\n\n\nidentify.predict_ids\nPredict the identities of individuals in the query set",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api/index.html#sorting-and-clustering",
    "href": "api/index.html#sorting-and-clustering",
    "title": "API Reference",
    "section": "",
    "text": "Clustering images by proposed IDs\n\n\n\nsort.HierarchicalCluster\nHierarchical clustering of images\n\n\nsort.NetworkCluster\nNetwork clustering of images\n\n\nsort.ClusterResults\nStoring NetworkCluster results.\n\n\nsort.prep_images\nCopy all images to a flat directory and save a csv with encounter info.\n\n\nsort.sort_images\nSort images into subfolders by proposed ID then encounter.\n\n\nsort.load_features\nLoad previously extracted features.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api/index.html#grading",
    "href": "api/index.html#grading",
    "title": "API Reference",
    "section": "",
    "text": "Grade images by distinctiveness\n\n\n\ngrade.rate_distinctiveness\nGrade images by their distinctiveness.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api/index.html#verifying-installation",
    "href": "api/index.html#verifying-installation",
    "title": "API Reference",
    "section": "",
    "text": "Functions to make sure PyTorch is working\n\n\n\nget_best_device\nSelect torch device based on expected performance.\n\n\nverify_pytorch\nVerify PyTorch installation and show device options.",
    "crumbs": [
      "API Reference"
    ]
  },
  {
    "objectID": "api/verify_pytorch.html",
    "href": "api/verify_pytorch.html",
    "title": "verify_pytorch",
    "section": "",
    "text": "verify_pytorch\nverify_pytorch()\nVerify PyTorch installation and show device options."
  },
  {
    "objectID": "api/grade.rate_distinctiveness.html",
    "href": "api/grade.rate_distinctiveness.html",
    "title": "grade.rate_distinctiveness",
    "section": "",
    "text": "grade.rate_distinctiveness(features, match_threshold=0.6)\nGrade images by their distinctiveness.\nCompute the embedding recognizability score (ERS) for each image in the feature array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nNumPy array of shape (image_count, feature_count) containing feature vectors for each image\nrequired\n\n\nmatch_threshold\nfloat\nThe threshold above which two images are considered a match. Must be between (0, 1)\n0.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nEmbedding recognizability score (ERS), a measure of distinctiveness, for every image in the dataset."
  },
  {
    "objectID": "api/grade.rate_distinctiveness.html#parameters",
    "href": "api/grade.rate_distinctiveness.html#parameters",
    "title": "grade.rate_distinctiveness",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nNumPy array of shape (image_count, feature_count) containing feature vectors for each image\nrequired\n\n\nmatch_threshold\nfloat\nThe threshold above which two images are considered a match. Must be between (0, 1)\n0.6"
  },
  {
    "objectID": "api/grade.rate_distinctiveness.html#returns",
    "href": "api/grade.rate_distinctiveness.html#returns",
    "title": "grade.rate_distinctiveness",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.array\nEmbedding recognizability score (ERS), a measure of distinctiveness, for every image in the dataset."
  },
  {
    "objectID": "api/sort.load_features.html",
    "href": "api/sort.load_features.html",
    "title": "sort.load_features",
    "section": "",
    "text": "sort.load_features(feature_path)\nLoad previously extracted features.\nLoad features produced by the FeatureExtractor.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeature_path\nstr\nPath to .npy file\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nfilenames, feature_array: np.array, np.array\nThe filenames array correspond to each row of the feature_array, which has shape (image_count, feature_count)"
  },
  {
    "objectID": "api/sort.load_features.html#parameters",
    "href": "api/sort.load_features.html#parameters",
    "title": "sort.load_features",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfeature_path\nstr\nPath to .npy file\nrequired"
  },
  {
    "objectID": "api/sort.load_features.html#returns",
    "href": "api/sort.load_features.html#returns",
    "title": "sort.load_features",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nfilenames, feature_array: np.array, np.array\nThe filenames array correspond to each row of the feature_array, which has shape (image_count, feature_count)"
  },
  {
    "objectID": "identify.html",
    "href": "identify.html",
    "title": "Identifying animals with a reference set",
    "section": "",
    "text": "In this notebook, we’ll demonstrate how do identify animals in a query set using a catalog of known individuals (i.e., a reference set). We’ll use the Happy Whale and Dolphin Kaggle competition dataset as an example. You can download the data by following that linked page (click the big “Download all” button). FYI, you’ll have to create an account first.\nThere are three components of the Happywhale dataset that we’ll focus on:\nIn this case, we’re treating the training dataset as the reference set, since we know the true identities.",
    "crumbs": [
      "Examples",
      "Using a reference set"
    ]
  },
  {
    "objectID": "identify.html#set-up",
    "href": "identify.html#set-up",
    "title": "Identifying animals with a reference set",
    "section": "Set up",
    "text": "Set up\nFeel free to place the data anywhere you like, e.g., within a pyseter_jobs folder or something. I frequently come back to the Happywhale dataset, so I have it saved locally.\n\n%config InlineBackend.figure_format = 'retina'\nimport os\n\nfrom pyseter.extract import FeatureExtractor\nfrom pyseter.sort import load_features\nfrom pyseter.identify import predict_ids, update_reference_features\nimport numpy as np\nimport pandas as pd\n\ndata_dir = '/Users/PattonP/datasets/happywhale/'",
    "crumbs": [
      "Examples",
      "Using a reference set"
    ]
  },
  {
    "objectID": "identify.html#extracting-features-using-bounding-boxes",
    "href": "identify.html#extracting-features-using-bounding-boxes",
    "title": "Identifying animals with a reference set",
    "section": "Extracting features using bounding boxes",
    "text": "Extracting features using bounding boxes\nNow that we’ve downloaded the data, we’ll get ready to extract the feature vectors by initializing the FeatureExtractor. Some of the images in the Happywhale dataset are pretty big, so we’ll set the batch_size to a low value, 4.\n\n# we'll save the results in the feature_dir\nfeature_dir = data_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n# initialize the extractor \nfe = FeatureExtractor(batch_size=4)\n\nUsing device: mps (Apple Silicon GPU)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThere are about 75,000 images in the Happywhale dataset. In my testing, on an NVIDIA GPU, it takes about 45 minutes to extract the features for the 50,000 reference images, and 25 minutes to extract the features for the 25,000 query images. On my Apple M4 MacBook, it takes about 3 hours and 30 minutes for the reference images and about 2 hours for the query images.\n\n\nAdditionally, we’ll need to supply bounding boxes to the feature extractor. Many of the Happywhale images are taken from far away, so we need to crop the image to just the animal. To do so, we’ll supply the path to the bounding box .csv as to the argument, bbox_csv. The .csv needs to have columns named: ['image', 'xmin', 'xmax', 'ymin', ymax'] that contain the image name and the coordinates for the corners of the box.\n\n\n\n\n\n\n\n\n\n\nbbox_url = 'https://raw.githubusercontent.com/philpatton/pyseter/main/data/happywhale-charm-boxes.csv'\n\ntrain_dir = data_dir + '/train_images'\ntrain_features = fe.extract(image_dir=train_dir, bbox_csv=bbox_url)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/train_features.npy'\nnp.save(out_path, train_features)\n\n# now do the test images\ntest_dir = data_dir + '/test_images'\ntest_features = fe.extract(image_dir=test_dir, bbox_csv=bbox_url)\n\nout_path = feature_dir + '/test_features.npy'\nnp.save(out_path, test_features)\n\nIf you’ve already extracted the features, you can load them back into your session.\n\nreference_path = feature_dir + '/train_features.npy'\nreference_files, reference_features = load_features(reference_path)\n\nquery_path = feature_dir + '/test_features.npy'\nquery_files, query_features = load_features(query_path)",
    "crumbs": [
      "Examples",
      "Using a reference set"
    ]
  },
  {
    "objectID": "identify.html#identifying-animals",
    "href": "identify.html#identifying-animals",
    "title": "Identifying animals with a reference set",
    "section": "Identifying animals",
    "text": "Identifying animals\nFirst, we’ll create two dictionaries. Dictionaries are similar to a named list in R, where we can access the value in the dictionary by providing it’s key. In this case, the key will be the image name and the value will be the feature vector for that image.\nWe’ll also need a DataFrame that tells us the identity of every individual in the reference set. This comes with the Kaggle dataset, in the train.csv file.\n\nquery_dict = dict(zip(query_files, query_features))\nreference_dict = dict(zip(reference_files, reference_features))\n\nid_df = pd.read_csv(data_dir + '/train.csv')\nid_df.head()\n\n\n\n\n\n\n\n\nimage\nspecies\nindividual_id\n\n\n\n\n0\n00021adfb725ed.jpg\nmelon_headed_whale\ncadddb1636b9\n\n\n1\n000562241d384d.jpg\nhumpback_whale\n1a71fbb72250\n\n\n2\n0007c33415ce37.jpg\nfalse_killer_whale\n60008f293a2b\n\n\n3\n0007d9bca26a99.jpg\nbottlenose_dolphin\n4b00fe572063\n\n\n4\n00087baf5cef7a.jpg\nhumpback_whale\n8e5253662392\n\n\n\n\n\n\n\nAnd now we’re ready to make predictions! By default, predict_ids returns 10 proposed IDs. Here we’ll show just 2 so for the sake of variety.\n\nprediction_df = predict_ids(reference_dict, query_dict, id_df, proposed_id_count=2)\nprediction_df.head(20)\n\n\n\n\n\n\n\n\nimage\nrank\npredicted_id\nscore\n\n\n\n\n0\na704da09e32dc3.jpg\n1\n5f2296c18e26\n0.500233\n\n\n1\na704da09e32dc3.jpg\n2\nnew_individual\n0.500000\n\n\n2\nde1569496d42f4.jpg\n1\ned237f7c2165\n0.826259\n\n\n3\nde1569496d42f4.jpg\n2\nnew_individual\n0.500000\n\n\n4\n4ab51dd663dd29.jpg\n1\nb9b24be2d5ae\n0.680653\n\n\n5\n4ab51dd663dd29.jpg\n2\n31f748b822f4\n0.503390\n\n\n6\nda27c3f9f96504.jpg\n1\nc02b7ad6faa0\n0.937102\n\n\n7\nda27c3f9f96504.jpg\n2\nnew_individual\n0.500000\n\n\n8\n0df089463bfd6b.jpg\n1\nf7b322faeeb5\n0.538287\n\n\n9\n0df089463bfd6b.jpg\n2\nae9cca8f13ca\n0.504653\n\n\n10\n813892efb592e0.jpg\n1\nc22d65f2d2f0\n0.808234\n\n\n11\n813892efb592e0.jpg\n2\nnew_individual\n0.500000\n\n\n12\n0c9304ddd0ba35.jpg\n1\n2df99dc71d85\n0.852067\n\n\n13\n0c9304ddd0ba35.jpg\n2\nnew_individual\n0.500000\n\n\n14\n14718a369776c5.jpg\n1\ne8d3c0ff0951\n0.800375\n\n\n15\n14718a369776c5.jpg\n2\nnew_individual\n0.500000\n\n\n16\n65653992318202.jpg\n1\nc4e546efa5ca\n0.842364\n\n\n17\n65653992318202.jpg\n2\nnew_individual\n0.500000\n\n\n18\n9857340b9e8c8e.jpg\n1\n1a20c92ffe68\n0.813362\n\n\n19\n9857340b9e8c8e.jpg\n2\nnew_individual\n0.500000\n\n\n\n\n\n\n\nBy default, predict_ids inserts a dummy prediction “new_individual” at 0.5. This makes it easy to evaluate the algorithm with metrics like MAP@5, or calculate the false negative rate.\nYou can save the results with to_csv from pandas.\n\nprediction_df.to_csv('predicted_ids.csv', index=False)",
    "crumbs": [
      "Examples",
      "Using a reference set"
    ]
  },
  {
    "objectID": "identify.html#updating-the-reference-set",
    "href": "identify.html#updating-the-reference-set",
    "title": "Identifying animals with a reference set",
    "section": "Updating the reference set",
    "text": "Updating the reference set\nLet’s say you’ve gone through and confirmed all the matches in your query set, e.g., with the AnyDorsal ID app. Now you would like to update your reference set with the new IDs.\nHere, we’ll take the naive approach that the algorithm’s first choice was always correct and update our reference set accordingly.\n\n# select the first match as the correct one\nconfirmed_matches = prediction_df.loc[prediction_df['rank'] == 1]\n\nNow we’ll want to update two things: id_df, which contains the true ids for all our reference images, and reference_features, which contains the feature vectors for every reference image. update_reference_features allows us to update the reference features, such that we can easily import them later with load_features().\n\n# a dataframe for every image with a confirmed id\nconfirmed_match_df = confirmed_matches[['image', 'predicted_id']]\nconfirmed_match_df.columns = ['image', 'individual_id']\n\n# create a new reference dict \nupdated_reference_dict = update_reference_features(\n    reference_dict, query_dict, confirmed_match_df\n)\n\n# save the output so we can load them later \nout_path = feature_dir + '/updated_features.npy'\nnp.save(out_path, updated_reference_dict)\n\nWe can also update our id_df. To do so, we need to “union” it (SQL jargon) with the confirmed matches. We do this with pd.concat(), which is similar to R’s rbind. Note that that we don’t have the species classification for the individuals in the query set.\n\n# union with the id_df \nupdated_id_df = pd.concat((id_df, confirmed_match_df)).reset_index(drop=True)\nupdated_id_df.to_csv(data_dir + 'updated_ids.csv', index=False)\nupdated_id_df.head()\n\n\n\n\n\n\n\n\nimage\nspecies\nindividual_id\n\n\n\n\n0\n00021adfb725ed.jpg\nmelon_headed_whale\ncadddb1636b9\n\n\n1\n000562241d384d.jpg\nhumpback_whale\n1a71fbb72250\n\n\n2\n0007c33415ce37.jpg\nfalse_killer_whale\n60008f293a2b\n\n\n3\n0007d9bca26a99.jpg\nbottlenose_dolphin\n4b00fe572063\n\n\n4\n00087baf5cef7a.jpg\nhumpback_whale\n8e5253662392",
    "crumbs": [
      "Examples",
      "Using a reference set"
    ]
  },
  {
    "objectID": "app.html",
    "href": "app.html",
    "title": "AnyDorsal ID app",
    "section": "",
    "text": "Pyseter ships with an experimental app that allows users to click through proposed IDs, identify matches, and export the results of the identification via a .csv. It’s worth reiterating that this is an experimental feature and, as such, is pretty bare bones. Nevertheless, we hope people will find it useful and suggest ways it could be improved.\nThis notebook will demonstrate how to run the app locally, and show users a demo version of the app. Running the code below will launch a version of the app based on the Happywhale data, where the test images are the “query set” and the training images are the “reference set”.\nYou can also check out the demo version Hugging Face. The demo version contains a subset of the Happywhale dataset.",
    "crumbs": [
      "Experimental",
      "*AnyDorsal* ID app"
    ]
  },
  {
    "objectID": "app.html#launching-the-app-locally",
    "href": "app.html#launching-the-app-locally",
    "title": "AnyDorsal ID app",
    "section": "Launching the app locally",
    "text": "Launching the app locally\nHere we’ll assume that you’ve already extracted the features for the query images and the reference images. As such, we can just load them in.\n\nfrom pyseter.experimental import launch_review\nfrom pyseter.identify import predict_ids\nfrom pyseter.sort import load_features\nimport pandas as pd\n\ndata_dir = '/Users/PattonP/datasets/happywhale/'\n\nfeature_dir = data_dir + '/features'\n\nreference_path = feature_dir + '/train_features.npy'\nreference_files, reference_features = load_features(reference_path)\n\nquery_path = feature_dir + '/test_features.npy'\nquery_files, query_features = load_features(query_path)\n\nid_df = pd.read_csv(data_dir + '/train.csv')\n\nThen we’ll create dictionaries for the reference set and the query set. These dictionaries map the file names to the feature vectors. Once we’ve done that, we can predict the 5 closest IDs in the query set to that of the reference set.\n\nquery_dict = dict(zip(query_files, query_features))\nreference_dict = dict(zip(reference_files, reference_features))\n\nprediction_df = predict_ids(reference_dict, query_dict, id_df, proposed_id_count=5)\n\nNow we have all we need to launch the app: the data frame containing the predictions, prediction_df; the data frame containing the IDs for images in the reference set, id_df; the directory containing the query images; and the directory containing the test images.\n\nlaunch_review(\n    prediction_df,\n    id_df,\n    data_dir + '/test_images'\n    data_dir + '/train_images'\n)",
    "crumbs": [
      "Experimental",
      "*AnyDorsal* ID app"
    ]
  },
  {
    "objectID": "app.html#demo-version",
    "href": "app.html#demo-version",
    "title": "AnyDorsal ID app",
    "section": "Demo version",
    "text": "Demo version\nHere, we demonstrate what the app looks like when you run it locally. This demo version contains 68 images from the Happywhale dataset, and represents a roughly even sample of every catalog in the dataset. You can also check out the demo version Hugging Face.\nHere are a few pointers on how to use the app\n\nNext and Prev navigate between query images\nYou can navigate between reference images of proposed IDs with the left and right arrow keys\nThe radio buttons under Select correct match allow you to select which proposed ID best matches the query image, if any\nConfirm match locks in your choice\nDownload csv downloads a .csv where one column is the query image and the other column is the confirmed ID\nClicking the X in the top right corner brings up a grid of images. Clicking an image on the grid returns to the single image viewer.",
    "crumbs": [
      "Experimental",
      "*AnyDorsal* ID app"
    ]
  },
  {
    "objectID": "distinct.html",
    "href": "distinct.html",
    "title": "Grading distinctiveness",
    "section": "",
    "text": "Pyseter comes with an experimental algorithm for grading individual distinctiveness. This can be useful for partially marked populations, e.g., spinner dolphins.",
    "crumbs": [
      "Experimental",
      "Grading distinctiveness"
    ]
  },
  {
    "objectID": "distinct.html#background",
    "href": "distinct.html#background",
    "title": "Grading distinctiveness",
    "section": "Background",
    "text": "Background\nTo understand the distinctiveness algorithm, it can be helpful to first introduce one of Pyseter’s clustering algorithms, NetworkCluster. Network clustering works with similarity scores, which represent the similarity between two individuals in a pair of images. We can define a threshold score, the match_threshold, above which we consider two individuals to be the same. That is, if the similarity score between two images is above a certain threshold, we cluster them into a proposed ID. As such, network clustering works by treating the query set as a network, where the nodes are images and the edges are similarity scores above a threshold. Each set of connected components, i.e., images whose similarity scores are above the match threshold, represents a proposed ID.\nWe might expect the indistinct individuals to cluster together. In the context of facial recognition, Deng et al. (2023) observed that “unrecognizable identities”, e.g., extremely blurry or masked faces, tend to cluster together. As such, for partially marked populations, the largest cluster in the query set may represent every indistinct individual. Following Deng et al. (2023), we can compute the average feature vector for this cluster. The distance between this average feature vector and the feature vector for each image is the distinctiveness score for that image. As such, the score applies to the image, not the animal. To get a score for an animal, users could average the distinctiveness scores across images for that animal.",
    "crumbs": [
      "Experimental",
      "Grading distinctiveness"
    ]
  },
  {
    "objectID": "distinct.html#spinner-dolphin-example",
    "href": "distinct.html#spinner-dolphin-example",
    "title": "Grading distinctiveness",
    "section": "Spinner dolphin example",
    "text": "Spinner dolphin example\nThe images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi. We’ll load in the saved feature vectors from before.\n\n%config InlineBackend.figure_format = 'retina'\n\nfrom pyseter.grade import rate_distinctiveness\nfrom pyseter.sort import load_features\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# load the features\nfeature_dir = 'working_dir/features'\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)\n\nWe need to supply two arguments to rate_distinctiveness: the feature_array, and the match_threshold. The lower the match threshold, the more individuals will end up in the unrecognizable identity cluster, potentially including distinct individuals. Conversely, a high match threshold might split the indistinct individuals into many clusters.\n\ndistinctiveness = rate_distinctiveness(feature_array, match_threshold=0.5)\n\nUnrecognizable identity cluster consists of 196 images.\n\n\n/Users/PattonP/miniforge3/envs/pyseter_env/lib/python3.14/site-packages/pyseter/grade.py:35: UserWarning: Distinctiveness grades are experimental and should be verified.\n  warn(UserWarning('Distinctiveness grades are experimental and should be verified.'))\n\n\nrate_distinctiveness warns you that this is experimental, and lets you know how many individuals ended up in the unrecognizable identity. This should be a quick sanity check.\nWe can plot the results of the score with the receiver operator characteristic (ROC) curve. This treats the distinctiveness grade as a classifier probability. The area under the curve tells us how good the classifier is, i.e., in terms of the number of false positives and false negatives.\n\n# download the true distinctiveness scores\ndata_url = (\n    'https://raw.githubusercontent.com/philpatton/pyseter/main/' \n    'data/spinner-distinct.csv'\n)\nspinner_distinct = pd.read_csv(data_url)\n\n# merge with the predicted distinctiveness scores\ners_df = pd.DataFrame({'image': filenames, 'ers': 1 - distinctiveness})\ners_df = ers_df.merge(spinner_distinct)\n\n# compute the curve first, which will get displayed\ny_score = ers_df['ers']\ny_test, _ = ers_df.distinctiveness.factorize()\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nfig, ax = plt.subplots()\n\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nroc_auc = roc_auc_score(y_test, y_score)\nax.text(0.95, 0.6, f'AUC={roc_auc:0.3f}', ha='right', va='top')\nimport numpy as np\nax.plot(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), linestyle='--', c='tab:grey')\nax.set_title('ROC Curve for \\nDistinctiveness Classifier')\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that, for this example, the distinctiveness score is better than useless (gray dashed line).",
    "crumbs": [
      "Experimental",
      "Grading distinctiveness"
    ]
  },
  {
    "objectID": "extract.html",
    "href": "extract.html",
    "title": "Extracting features",
    "section": "",
    "text": "Here we present a quick overview on how to extract feature vectors with Pyseter. Feature vectors are summaries of images that are useful for identifying individuals. If AnyDorsal is working, then the similarity between two images should indicate that the individuals within the images look similar.\nimport os\n\nfrom pyseter.extract import FeatureExtractor\nfrom pyseter.sort import load_features, prep_images\nimport gdown\nimport numpy as np\nimport pyseter\nimport zipfile",
    "crumbs": [
      "Examples",
      "Extracting features"
    ]
  },
  {
    "objectID": "extract.html#dataset",
    "href": "extract.html#dataset",
    "title": "Extracting features",
    "section": "Dataset",
    "text": "Dataset\nThe images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi. We can download the data with gdown, which pulls data from Google Drive. We’ll unzip the file to the working_dir.\n\n# download the demo data\nfile_id = '1puM7YBTVFbIAT3xNBQV1g09K0bMGLk1y' \nfile_url = f'https://drive.google.com/uc?id={file_id}'\n\ngdown.download(file_url, quiet=False, use_cookies=False)\n\n# extract the files to the working directory\nwith zipfile.ZipFile('original_images.zip', 'r') as zip_ref:\n    zip_ref.extractall('working_dir')\n\nThe demo dataset is organized into subfolders by encounter. Our lives will be a little easier if we move all these images to a flat folder. The prep_images() function does just that.\n\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)\n\nCopied 1251 images to: working_dir/all_images\nSaved encounter information to: /Users/PattonP/source/repos/pyseter/docs/working_dir/encounter_info.csv",
    "crumbs": [
      "Examples",
      "Extracting features"
    ]
  },
  {
    "objectID": "extract.html#extracting-features",
    "href": "extract.html#extracting-features",
    "title": "Extracting features",
    "section": "Extracting features",
    "text": "Extracting features\nExtracting features, like all modern AI, depends on GPUs. Users with an NVIDIA GPU can expect fast feature extraction. Extracting features for the ~1200 images in the demo dataset, for example, will take about two minutes on an NVIDIA GPU. Most university or governmental high performance computing clusters (HPCs) will have GPUs available.\nFeature extraction also works reasonably quickly on Apple Silicon (i.e., M1-M4). Extracting features for in the demo dataset, for example, will take about 10 minutes on Apple Silicon, depending on the model.\nWe can verify the Pyseter installation and the acceleration with verify_pytorch.\n\npyseter.verify_pytorch()\n\n:) PyTorch 2.10.0 detected\n:) Apple Silicon (MPS) GPU available\n\n\nWe also need to initialize the FeatureExtractor. The only argument is the batch_size, which we recommend setting to something low, like 4.\n\n# we'll save the results in the feature_dir\nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n# initialize the extractor \nfe = FeatureExtractor(batch_size=4)\n\nUsing device: mps (Apple Silicon GPU)\n\n\nThe first time you extract features with Pyseter, it will download AnyDorsal to your machine. AnyDorsal is huge (4.5GB), so be prepared!\nThis will take a minute, so we recommend saving the results afterwards.\n\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\n# convert keys and values to numpy arrays\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n\nYou can load previously saved features with load_features\n\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)",
    "crumbs": [
      "Examples",
      "Extracting features"
    ]
  },
  {
    "objectID": "extract.html#command-line-interface",
    "href": "extract.html#command-line-interface",
    "title": "Extracting features",
    "section": "Command line interface",
    "text": "Command line interface\nUsers can also extract features using the command line interface (e.g., terminal). This can be especially helpful when running on your university’s or your agency’s high performance computing cluster (HPC).\nTo do so, open the terminal and activate your Pyseter environment.\n\n$ cd happywhale\n$ conda activate pyseter_env \n$ python -m pyseter.extract --dir test_images --bbox_csv happywhale-charm-boxes.csv\n\nThere are two arguments:\n\ndir specifies the flat directory containing the images from which to extract features\nbbox_csv optionally indicates where to find the .csv with bounding boxes.\n\nThe features will save to the file, features/features.npy, in the parent directory of the dir you specify. In the example above, the test images live in the happywhale/test_images folder, and the features are saved to the happywhale/features folder.",
    "crumbs": [
      "Examples",
      "Extracting features"
    ]
  },
  {
    "objectID": "evaluate.html",
    "href": "evaluate.html",
    "title": "Evaluating AnyDorsal",
    "section": "",
    "text": "In this notebook, we’ll demonstrate how do evaluate AnyDorsal’s’ performance on a test dataset. We’ll use the Happy Whale and Dolphin Kaggle competition dataset as an example. You can download the data by following that linked page (click the big “Download all” button). FYI, you’ll have to create an account first.\nIn the Predicting IDs notebook, we demonstrated how to extract features for the Happywhale dataset using a set of bounding boxes. Here, we’ll load the features in.\n%config InlineBackend.figure_format = 'retina'\n\nfrom pyseter.sort import load_features\nfrom pyseter.identify import predict_ids\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# load in the feature vectors\ndata_dir = '/Users/PattonP/datasets/happywhale/'\nfeature_dir = data_dir + '/features'\n\nreference_path = feature_dir + '/train_features.npy'\nreference_files, reference_features = load_features(reference_path)\n\nquery_path = feature_dir + '/test_features.npy'\nquery_files, query_features = load_features(query_path)\nIn order to evaluate the performance of AnyDorsal on the test set, we’ll need to know the IDs of animals in the train set and the test set.\ndata_url = (\n    'https://raw.githubusercontent.com/philpatton/pyseter/main/' \n    'data/happywhale-ids.csv'\n)\nid_df = pd.read_csv(data_url)\n\nid_df.head(10)\n\n\n\n\n\n\n\n\nimage\nspecies\nindividual_id\n\n\n\n\n0\n000110707af0ba.jpg\ngray_whale\nfbe2b15b5481\n\n\n1\n00021adfb725ed.jpg\nmelon_headed_whale\ncadddb1636b9\n\n\n2\n000562241d384d.jpg\nhumpback_whale\n1a71fbb72250\n\n\n3\n0006287ec424cb.jpg\nfalse_killer_whale\n1424c7fec826\n\n\n4\n0007c33415ce37.jpg\nfalse_killer_whale\n60008f293a2b\n\n\n5\n0007d9bca26a99.jpg\nbottlenose_dolphin\n4b00fe572063\n\n\n6\n000809ecb2ccad.jpg\nbeluga\n1ce3ba6a3c29\n\n\n7\n00087baf5cef7a.jpg\nhumpback_whale\n8e5253662392\n\n\n8\n00098d1376dab2.jpg\nhumpback_whale\nc4274d90be60\n\n\n9\n000a8f2d5c316a.jpg\nbottlenose_dolphin\nb9907151f66e\nCode\n# excel on mac corrupts the IDs (no need to do this on PC or linux)\nid_df['individual_id'] = id_df['individual_id'].apply(\n    lambda x: str(int(float(x))) if 'E+' in str(x) else x\n)\nNow we’ll predict the IDs in the query set.\nquery_dict = dict(zip(query_files, query_features))\nreference_dict = dict(zip(reference_files, reference_features))\n\nprediction_df = predict_ids(reference_dict, query_dict, id_df, proposed_id_count=5)\nprediction_df.head(20)\n\n\n\n\n\n\n\n\nimage\nrank\npredicted_id\nscore\n\n\n\n\n0\na704da09e32dc3.jpg\n1\n5f2296c18e26\n0.500233\n\n\n1\na704da09e32dc3.jpg\n2\nnew_individual\n0.500000\n\n\n2\na704da09e32dc3.jpg\n3\n61f9e4cd30eb\n0.432061\n\n\n3\na704da09e32dc3.jpg\n4\ncb372e9b2c48\n0.411830\n\n\n4\na704da09e32dc3.jpg\n5\n43dad7ffa3c7\n0.405389\n\n\n5\nde1569496d42f4.jpg\n1\ned237f7c2165\n0.826259\n\n\n6\nde1569496d42f4.jpg\n2\nnew_individual\n0.500000\n\n\n7\nde1569496d42f4.jpg\n3\n8c4a71fd3eb1\n0.446297\n\n\n8\nde1569496d42f4.jpg\n4\n7d4deec3b721\n0.424570\n\n\n9\nde1569496d42f4.jpg\n5\nfcc7ade0c50a\n0.376511\n\n\n10\n4ab51dd663dd29.jpg\n1\nb9b24be2d5ae\n0.680653\n\n\n11\n4ab51dd663dd29.jpg\n2\n31f748b822f4\n0.503390\n\n\n12\n4ab51dd663dd29.jpg\n3\nnew_individual\n0.500000\n\n\n13\n4ab51dd663dd29.jpg\n4\n7845337998d6\n0.497671\n\n\n14\n4ab51dd663dd29.jpg\n5\nefda8f368763\n0.455688\n\n\n15\nda27c3f9f96504.jpg\n1\nc02b7ad6faa0\n0.937102\n\n\n16\nda27c3f9f96504.jpg\n2\nnew_individual\n0.500000\n\n\n17\nda27c3f9f96504.jpg\n3\n70858f1edf62\n0.375488\n\n\n18\nda27c3f9f96504.jpg\n4\n72b0033dd4fd\n0.367204\n\n\n19\nda27c3f9f96504.jpg\n5\n749e56a8ec71\n0.354002",
    "crumbs": [
      "Examples",
      "Evaluating *AnyDorsal*"
    ]
  },
  {
    "objectID": "evaluate.html#mean-average-precision",
    "href": "evaluate.html#mean-average-precision",
    "title": "Evaluating AnyDorsal",
    "section": "Mean average precision",
    "text": "Mean average precision\nWe are going to evaluate AnyDorsal with Mean Average Precision (MAP). MAP evaluates a set of predictions, in this case, a set of 5 predictions. For a set of five ordered predictions, the precision score will be \\(1/1 = 1\\) if the first prediction is correct, \\(1/2\\) if the second is correct, and so on until \\(1/5\\) if the fifth prediction is correct, or \\(0\\) if none of the five predictions are correct. MAP is the mean precision score for a set.\nSo we need to get the rank of the correct ID for every image in the test set. One way to do so is to merge the ID DataFrame with the prediction DataFrame. We only want one row per test image, so we need to merge on both the image and ID columns. We can check to see that it worked by checking the number of rows in the result (there are 27956 images in the testing dataset).\n\n# add the predictions and the scores to the id dataframe\nperformance_df = id_df.merge(\n    prediction_df,\n    how='left',\n    left_on=['image', 'individual_id'],\n    right_on=['image', 'predicted_id']\n)\n\n# filter out the training images\ntrain_images = pd.read_csv(data_dir + '/train.csv').image\nperformance_df = performance_df.loc[~performance_df.image.isin(train_images)]\nprint(performance_df.shape)\n\n(27956, 6)\n\n\n\nperformance_df.head(20)\n\n\n\n\n\n\n\n\nimage\nspecies\nindividual_id\nrank\npredicted_id\nscore\n\n\n\n\n0\n000110707af0ba.jpg\ngray_whale\nfbe2b15b5481\n1.0\nfbe2b15b5481\n0.871143\n\n\n3\n0006287ec424cb.jpg\nfalse_killer_whale\n1424c7fec826\n1.0\n1424c7fec826\n0.794225\n\n\n6\n000809ecb2ccad.jpg\nbeluga\n1ce3ba6a3c29\n1.0\n1ce3ba6a3c29\n0.797251\n\n\n8\n00098d1376dab2.jpg\nhumpback_whale\nc4274d90be60\n1.0\nc4274d90be60\n0.816240\n\n\n10\n000b8d89c738bd.jpg\ndusky_dolphin\nnew_individual\n1.0\nnew_individual\n0.500000\n\n\n15\n000e246888710c.jpg\nmelon_headed_whale\nnew_individual\n2.0\nnew_individual\n0.500000\n\n\n16\n000eb6e73a31a5.jpg\nbottlenose_dolphin\n77410a623426\n1.0\n77410a623426\n0.709175\n\n\n17\n000fe6ebfc9893.jpg\nspinner_dolphin\n8805324885f2\n1.0\n8805324885f2\n0.942707\n\n\n20\n0011f7a65044e4.jpg\nspinner_dolphin\nd5dcbb35777c\n1.0\nd5dcbb35777c\n0.676094\n\n\n21\n0012ff300032e3.jpg\nbeluga\n19b638e11443\n1.0\n19b638e11443\n0.950292\n\n\n23\n00150406ce5395.jpg\nfalse_killer_whale\n2280b5fcc6c2\n1.0\n2280b5fcc6c2\n0.837772\n\n\n25\n0016cd18d6410e.jpg\ncuviers_beaked_whale\n65620eadc0b4\n1.0\n65620eadc0b4\n0.582929\n\n\n27\n0017a9c11c61a8.jpg\nbeluga\nnew_individual\n2.0\nnew_individual\n0.500000\n\n\n29\n0017f7e0de07b1.jpg\nbottlenose_dolphin\n81bec36eb86e\n1.0\n81bec36eb86e\n0.840179\n\n\n32\n001c4c6d532419.jpg\nbottlenose_dolphin\n2efee119d786\n1.0\n2efee119d786\n0.748652\n\n\n34\n001dc45839b1b5.jpg\nsouthern_right_whale\ncd2e1d83c6a1\n1.0\ncd2e1d83c6a1\n0.791898\n\n\n35\n001ea84d905ced.jpg\nhumpback_whale\n02f5c5ee9c2a\n1.0\n02f5c5ee9c2a\n0.680320\n\n\n43\n00248b9385e2e0.jpg\nblue_whale\n67b7b6934db9\n1.0\n67b7b6934db9\n0.509903\n\n\n48\n002c7034834e2c.jpg\nkiller_whale\ned15fd01efbe\n1.0\ned15fd01efbe\n0.815825\n\n\n49\n002d7a5f6d6765.jpg\nbeluga\n3a055fad1478\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nAny image with an NA rank means that the true ID was not one of the proposed_ids.\nWe can compute the precision by taking the reciprocal of the rank. After we do that, we’ll fill the NA values with zero, since none of the five predictions were correct.\n\nperformance_df['precision'] = 1 / performance_df['rank'] \nperformance_df['precision'] = performance_df['precision'].fillna(0)\nmap5 = performance_df.precision.mean()\nprint(f'MAP@5: {map5:0.3f}')\n\nMAP@5: 0.863\n\n\nFor fun, we can look at the results by species\n\nimport matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['figure.facecolor'] = 'white'\n\nmap_df = (\n    performance_df.groupby('species')\n        .precision\n        .mean()\n        .rename('MAP')\n        .reset_index()\n        .sort_values('MAP')\n)\n\nfig, ax = plt.subplots(figsize=(4, 8))\nax.scatter(map_df.MAP, map_df.species)\nfor row in map_df.itertuples():\n    ax.text(row.MAP - 0.02, row.species, f'{row.MAP:0.2f}', ha='right', \n            va='center', fontsize=12)\nax.set_xlim((0,1.01))\nax.set_title('AnyDorsal Performance')\nax.set_xlabel('Mean average precision')\nplt.show()",
    "crumbs": [
      "Examples",
      "Evaluating *AnyDorsal*"
    ]
  },
  {
    "objectID": "cluster.html",
    "href": "cluster.html",
    "title": "Clustering individuals",
    "section": "",
    "text": "Here we present a quick overview on how to cluster individuals with Pyseter. Clustering individuals is useful when you don’t want to match against a reference set. For example, you might want to quickly sort individuals from the field into folders of proposed IDs, such that you can quickly grade the images for quality and distinctiveness.\nPyseter comes with two ways to cluster individuals: NetworkCluster and HierarchicalCluster. NetworkCluster is most useful for smaller datasets, and should be intuitive for anyone who has worked with similarity scores before. HierarchicalCluster works better for larger datasets.\nimport os\n\nfrom pyseter import sort\nfrom sklearn.metrics.pairwise import cosine_similarity",
    "crumbs": [
      "Examples",
      "Clustering individuals"
    ]
  },
  {
    "objectID": "cluster.html#dataset",
    "href": "cluster.html#dataset",
    "title": "Clustering individuals",
    "section": "Dataset",
    "text": "Dataset\nTo demonstrate clustering, we’ll use the spinner dolphin example dataset. The images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi. We demonstrate how to extract the features and save them for this dataset in Extracting Features. As such, we’ll load the previously saved features here.\n\nfeature_dir = 'working_dir/features'\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = sort.load_features(out_path)",
    "crumbs": [
      "Examples",
      "Clustering individuals"
    ]
  },
  {
    "objectID": "cluster.html#network-clustering",
    "href": "cluster.html#network-clustering",
    "title": "Clustering individuals",
    "section": "Network clustering",
    "text": "Network clustering\nIf an ID algorithm is calibrated correctly, then a high similarity score between two images should indicate a high probability that they contain the same individual. As such, we might define a threshold, over which we assume that two images contain the same individual. Once we compute the similarity scores between all pairs of images, we can look to see which images are “connected”, that is, which pairs of images have a similarity score over a threshold. When multiple images are connected to one another in a blob, we can consider them a proposed ID. The end result of all this is a network, where the images are the “nodes” and similarity scores over the threshold are the “edges”. The number of clusters, that is, the number of sets of nodes that have connections between them, represents the number of proposed IDs.\nThe number of clusters depends on the match_threshold. Low thresholds will create many connections and therefore few clusters. High thresholds will create few connections and therefore many clusters. The ideal threshold will depend on the dataset. We think 0.55 is a nice middle ground for most applications.\nTo do the network matching, we just need to compute the pairwise similarity between images. We will use cosine_similarity from sklearn.metrics.pairwise to do so. cosine_similarity similarity returns an \\(m\\) by \\(m\\) matrix of similarity scores where \\(m\\) is the number of proposed IDs.\n\nsimilarity_scores = cosine_similarity(feature_array)\nnc = sort.NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n\nFollowing clusters may contain false positives:\n['ID_0001', 'ID_0006', 'ID_0008', 'ID_0021', 'ID_0110']\n\n\nWe can do a quick sanity check on our match threshold with report_cluster_results, which tells us how many clusters there are and which one is the largest.\n\nnetwork_idx = results.cluster_idx\nsort.report_cluster_results(network_idx)\n\nFound 208 clusters.\nLargest cluster has 128 images.\n\n\ncluster_images tells us that some of the proposed IDs (i.e., clusters) might contain false positives. False positives occur when two distinct individuals are grouped within the same cluster. While we don’t know the truth, we can look for evidence of such errors. Imagine that two images looks somewhat similar to each other, and are therefore connected. But each one, in turn, looks very similar to other images. As such, the cluster looks like a barbell, with one spurious connection between two highly connected blobs. We can look for such clusters with plot_suspicious\n\nresults.plot_suspicious()\n\n\n\n\n\n\n\n\nOf these, cluster ID_0001 is probably the most suspicious, and worthy of investigating. Note that dataset includes indistinct individuals, which tend to cluster together. ID_0008 most likely contains many of the indistinct individuals.\nAs the number of image in the dataset grows, network clustering becomes impractical because false positives and suspicious clusters will pervade. For larger datasets, we recommend hierarchical clustering",
    "crumbs": [
      "Examples",
      "Clustering individuals"
    ]
  },
  {
    "objectID": "cluster.html#hierarchical-clustering",
    "href": "cluster.html#hierarchical-clustering",
    "title": "Clustering individuals",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nHierarchical clustering works better for larger datasets. Hierarchical clustering works by clustering feature vectors together with a dendrogram. You can read more about hierarchical clustering in the scikit-learn user guide, which has a thorough guide to clustering algorithms. Pyseter’s HierarchicalCluster is a wrapper for scikit-learn’s implementation\nWe recommend setting the match_threshold threshold slightly lower for HierarchicalCluster, e.g., 0.5\n\nhc = sort.HierarchicalCluster(match_threshold=0.5)\nhac_idx = hc.cluster_images(feature_array)\n\n# format_ids converts the integer labels to something like 'ID-0001'\nhac_labels = sort.format_ids(hac_idx)\nsort.report_cluster_results(hac_labels)\n\nFound 299 clusters.\nLargest cluster has 27 images.\n\n\nWe see that HierarchicalCluster produces more clusters, i.e., more proposed IDs. As such, expect HierarchicalCluster to be much sparser in most cases.",
    "crumbs": [
      "Examples",
      "Clustering individuals"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pyseter",
    "section": "",
    "text": "Pyseter is an open-source Python package for automating photo-identification. Pyseter includes functions for:",
    "crumbs": [
      "Pyseter"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Pyseter",
    "section": "Introduction",
    "text": "Introduction\nPlease see the Getting Started page for a demonstration of the basic functionality of Pyseter using an example of spinner dolphins.",
    "crumbs": [
      "Pyseter"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Pyseter",
    "section": "Installation",
    "text": "Installation\nUsers can install Pyseter with pip, uv, or pixi. For example,\npip install pyseter\nWe imagine that most Pyseter users will be more familiar with R than Python. If this is you, please see the R users who are new to Python page for detailed instructions on the steps needed to work with Pyseter.",
    "crumbs": [
      "Pyseter"
    ]
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "Python users",
    "section": "",
    "text": "This page has information on how to install for users who have some familiarity with Python. R users should see this page, which has more thorough instructions and introductions to Python concepts.",
    "crumbs": [
      "Installation",
      "Python users"
    ]
  },
  {
    "objectID": "install.html#installation",
    "href": "install.html#installation",
    "title": "Python users",
    "section": "Installation",
    "text": "Installation\nUsers can use whatever package management utility they like, such as conda, venv, pixi, or uv. Both PyTorch and Pyseter can be installed with pip. We’ll assume you’re using conda.\nOnce you’ve installed conda, open the command line interface. Then you’ll need to do several things:\n\nCreate the conda environment you’ll be using\nActive the environment and install pip\nInstall packages\n\nPytorch, which will depend on your operating system and GPU availability\ngdown (for downloading the the dataset)\nipykernel (optional, useful if using Jupyter Lab or Jupyter Notebook)\npyseter\n\n\nBelow is an example of the commands necessary to install on a Windows (or Linux) machine equipped with an NVIDIA GPU.\nconda create -n pyseter_env -y  # create new environment \nsource activate pyseter_env     # activate it\nconda install pip -y            # pip is necessary to install torch and pyseter\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu128 \npip install pyseter gdown ipykernel==6.30.1 ipywidgets\npython -m ipykernel install --user --name pyseter_env --display-name \"Python (Pyseter)\"",
    "crumbs": [
      "Installation",
      "Python users"
    ]
  },
  {
    "objectID": "install.html#verify-the-installation",
    "href": "install.html#verify-the-installation",
    "title": "Python users",
    "section": "Verify the installation",
    "text": "Verify the installation\nVerify the Pyseter installation by running the following command.\nimport pyseter\npyseter.verify_pytorch()\nIf you have access to an NVIDIA GPU, you should see something like\n✓ PyTorch 2.7.1+cu126 detected\n✓ CUDA GPU available: NVIDIA A30 MIG 2g.12gb",
    "crumbs": [
      "Installation",
      "Python users"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting started with Pyseter",
    "section": "",
    "text": "Here we present a quick overview of the main functions of Pyseter with a demo dataset.",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started.html#installation",
    "href": "getting-started.html#installation",
    "title": "Getting started with Pyseter",
    "section": "Installation",
    "text": "Installation\nIf you haven’t already, install some form of conda. Anaconda is probably the easiest to install, but we are personally partial to miniforge. Of course, you are also welcome to use any other package management tool, such as uv, pixi, or good old fashioned venv.\nOnce you’ve installed conda, open the command line interface (e.g., the miniforge prompt or the terminal). Then you’ll need to do several things:\n\nCreate the conda environment you’ll be using\nActive the environment and install pip\nInstall packages\n\nPytorch, which will depend on your operating system and GPU availability\ngdown (for downloading the the dataset)\nipykernel (optional, useful if using Jupyter Lab or Jupyter Notebook)\npyseter\n\n\nBelow is an example of the commands necessary to install on a Windows machine equipped with an NVIDIA GPU.\nconda create -n pyseter_env -y  # create new environment \nsource activate pyseter_env     # activate it\nconda install pip -y            # pip is necessary to install torch and pyseter\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu128 \npip install pyseter gdown ipykernel==6.30.1 ipywidgets\npython -m ipykernel install --user --name pyseter_env --display-name \"Python (Pyseter)\"",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started.html#dataset",
    "href": "getting-started.html#dataset",
    "title": "Getting started with Pyseter",
    "section": "Dataset",
    "text": "Dataset\nThe images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi. Every image collected during the study was graded for quality and distinctiveness. This example dataset only includes images of sufficient quality that have been cropped to the identifying mark—the dorsal fin. This example includes 208 images of animals without distinctive markings, and 1043 images of animals with distinctive markings.\nOpen a Jupyter notebook and download the data with gdown, which pulls data from Google Drive.\n\nimport gdown\nimport zipfile\n\n# make the file url with an f string\nfile_id = '1puM7YBTVFbIAT3xNBQV1g09K0bMGLk1y' \nfile_url = f'https://drive.google.com/uc?id={file_id}'\n\n# download the demo data\ngdown.download(file_url, quiet=False, use_cookies=False)\n\n# extract the files to the working directory\nwith zipfile.ZipFile('original_images.zip', 'r') as zip_ref:\n    zip_ref.extractall('working_dir')\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nIn R, the above code block would look something like\nlibrary(gdown)\ndownload()\nImports work a little differently in Python. First, we need tell Python that this package is available for imports, import pyseter, then we need to explicitly call the function from the library pyseter.verify_pytorch(). To an R user, this can feel overly wordy. Nevertheless, this wordiness helps keep the global environment clean. Whereas R sessions frequently have to deal with masking names, this rarely happens in Python.\n\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nThis block uses two fancy Python concepts, with (or the context manager), and the f'' string. The f'' string allows us to substitute the file_id into the file_url. The with statement is a way to safely open and close a connection to a file.",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started.html#working-with-pyseter",
    "href": "getting-started.html#working-with-pyseter",
    "title": "Getting started with Pyseter",
    "section": "Working with Pyseter",
    "text": "Working with Pyseter\nNow we’re ready to work in Pyseter. First, verify that your PyTorch installation. This will also let you know how fast (or slow) you can expect Pyseter to be.\n\nimport pyseter\npyseter.verify_pytorch()\n\n:) PyTorch 2.10.0 detected\n:) Apple Silicon (MPS) GPU available\n\n\nThe demo dataset is organized into subfolders by encounter.\nworking_dir\n└── original_images\n    ├── enc0\n    │   ├── 0a49385ef8f1e74a.jpg\n        ├── 1e105f9659c12a66.jpg\n            ...\n    │   └── f5093b3089b44e67.jpg\n    └── enc12\n        ├── 0b5c44f167d89d6c.jpg\n        ├── 1e0c186da31a53c4.jpg\n            ...\n        └── f9bb41e7ce0d672d.jpg\nOur lives will be a little easier if we do two things: move all these images to a flat folder, i.e., with no subfolders, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The prep_images() function does just that.\n\nfrom pyseter.sort import prep_images\n\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)\n\nCopied 1251 images to: working_dir/all_images\nSaved encounter information to: /Users/PattonP/source/repos/pyseter/docs/working_dir/encounter_info.csv\n\n\nWe can look at a few random images with matplotlib.\n\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# select a random id to plot\nrng = np.random.default_rng(seed=17)\nimages = os.listdir(image_dir)\n\n# create a figure where each subplot is an image \ncolumn_count = 3\nrow_count = 7\nfig, axes = plt.subplots(row_count, column_count, tight_layout=True, \n                         figsize=(9, row_count * 3))\n\n# plot each image\nfor i in range(column_count * row_count):\n    ax = axes.flat[i]\n    file_name = images[i]\n    path = f'working_dir/all_images/{file_name}'\n    image = Image.open(path)\n    ax.imshow(image)\n    ax.axis('off')\n\n\n\n\n\n\n\n\n\nFeature extraction\nIdentifying animals in images requires extracting feature vectors. The distance between two feature vectors tells us similarity between two images. Similar images likely contain the same individual, if the model is working.\nTo do so, we’ll initialize the FeatureExtractor. The only argument is the batch size, which we recommend setting to something low, like 4.\n\nimport os\nfrom pyseter.extract import FeatureExtractor\n\n# we'll save the results in the feature_dir\nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n# initialize the extractor \nfe = FeatureExtractor(batch_size=4)\n\nUsing device: mps (Apple Silicon GPU)\n\n\nNow we can extract features. This will take a while, depending on hardware. On a machine with an NVIDIA GPU, this will take a minute or two. On a Mac, this will take around 10 minutes.\n\n\n\n\n\n\nWarning\n\n\n\nFeature extraction will be extremely slow for users who do not have access to an NVIDIA GPU or Apple Silicon. Users may want to check if their university or agency has access to GPUs via a high performance computing cluster (HPC). Also, Google Colab and other similar services (e.g., Kaggle) allow users to rent GPU resources.\n\n\nThe first time you extract features with Pyseter, it will download AnyDorsal to your machine. AnyDorsal is huge (4.5GB), so be prepared!\n\nimport numpy as np\n\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\n# convert keys and values to numpy arrays\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nIn this case, FeatureExtractor is a class and extract() is a method of that class. Classes and methods also exist in R, but operate more behind the scenes. For example, x &lt;- data.frame() initializes an object of class data.frame, and summary(x) calls the summary method for data.frames. Python makes this relationship more explicit. For example, the equivalent Python code would be x = pandas.DataFrame() and x.summary(). The pandas library provides data frames in Python.\n\n\nYou can also load in features from a previously saved session.\n\n# alternatively, load in the feature dictionary from file \nimport numpy as np \nfrom pyseter.sort import load_features\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started.html#clustering-individuals",
    "href": "getting-started.html#clustering-individuals",
    "title": "Getting started with Pyseter",
    "section": "Clustering individuals",
    "text": "Clustering individuals\nNext, we can cluster individuals into proposed IDs. One simple method for doing so is to say that any two images with similarity scores above a match_threshold belong to the same individual (note that the similarity score is one minus the distance between two feature vectors). This creates a network where each node is an image and the connected nodes represent one proposed ID. Hence, we call this NetworkCluster\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pyseter.sort import NetworkCluster, report_cluster_results\n\nsimilarity_scores = cosine_similarity(feature_array)\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n\nFollowing clusters may contain false positives:\n['ID_0001', 'ID_0006', 'ID_0008', 'ID_0021', 'ID_0110']\n\n\nYou can look at the results with report_cluster_results.\n\nnetwork_idx = results.cluster_idx\nreport_cluster_results(network_idx)\n\nFound 208 clusters.\nLargest cluster has 128 images.\n\n\nIn an ideal world, every image of the same individual would be connected in the network, and not connected to images of any other individuals. As such, the ideal network would look like a bunch of blobs. As such, we might be suspicious of clusters that look like barbells, i.e., two blobs connected by one link. That one link might be a false positive, linking two distinct identities. The plot_suspicious function plots things that might look like barbells.\n\nresults.plot_suspicious()",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started.html#sorting-individuals-into-folders",
    "href": "getting-started.html#sorting-individuals-into-folders",
    "title": "Getting started with Pyseter",
    "section": "Sorting individuals into folders",
    "text": "Sorting individuals into folders\nNow, we might want to inspect these proposed ID. One method is to chuck the IDs into a table, i.e., a DataFrame.\n\nimport pandas as pd\n\nid_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})\n\n# join with the encounter information using \"encounter\" as a key \nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nid_df = id_df.merge(encounter_info)\nid_df.head()\n\n\n\n\n\n\n\n\nimage\nproposed_id\nencounter\n\n\n\n\n0\n2c8750b066372ab5.jpg\nID-0000\nenc8\n\n\n1\n568fc1d376b616a6.jpg\nID-0001\nenc8\n\n\n2\nddeb347716d7861c.jpg\nID-0002\nenc6\n\n\n3\nb65f9334b05f48f4.jpg\nID-0003\nenc0\n\n\n4\nd84aefa4d99d6f9a.jpg\nID-0004\nenc4\n\n\n\n\n\n\n\nBut this isn’t particularly satisfying, since we can’t actually see the IDs. Probably the easiest way to to sort the images into folders then explore them with the file explore (e.g., Finder on Mac), or with a program like ACDSee. We can do so with sort_images.\n\nfrom pyseter.sort import sort_images\n\n# make an output directory \nsorted_dir = working_dir + '/sorted_images'\nos.makedirs(sorted_dir, exist_ok=True)\n\nsort_images(id_df, all_image_dir=image_dir, output_dir=sorted_dir)\n\nSorted 1251 images into 311 folders.\n\n\nWe can also plot some images with matplotlib, if that’s your thing.\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# select a random id to plot\nrng = np.random.default_rng(seed=17)\nrandom_id = rng.choice(id_df.proposed_id)\n\n# collect all the images for that id\nid_info = id_df.loc[id_df.proposed_id == random_id].sort_values('encounter').reset_index(drop=True)\nimage_count = len(id_info)\n\n# create a figure where each subplot is an image \ncolumn_count = 3\nrow_count = (image_count + column_count - 1) // column_count\nfig, axes = plt.subplots(row_count, column_count, tight_layout=True, \n                         figsize=(9, row_count * 3))\n\n# plot each image\nfor i, row in id_info.iterrows():\n    ax = axes.flat[i]\n    path = f'working_dir/all_images/{row.image}'\n    image = Image.open(path)\n    ax.imshow(image)\n    ax.set_title(row.encounter)\n    ax.axis('off')\n\nfig.text(0.5, 1.0, f'Proposed ID: {random_id}', ha='center', va='bottom',\n         fontsize=16)\n\n# delete any unnecessary subplots\nfor i in range(image_count, row_count * column_count):\n    axes.flat[i].remove()",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "manuscript.html",
    "href": "manuscript.html",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "",
    "text": "Photographic identification (photo-ID) is an effective and non-invasive method for studying many aspects of ecology, including movement (Gardiner et al. 2014), social behavior (Bejder et al. 1998), abundance (McPherson et al. 2024), survival (Morrison et al. 2011), distribution (McGuire et al. 2020), recruitment (Setyawan et al. 2022), migration (Hill et al. 2020), and resource-selection (Patton 2025). Photo-ID is a multi-step process that culminates in comparing a query set—images of animals whose identity we wish to know—against a reference set—images of known individuals. This last step can be accomplished by hand (Karanth 1995), with a database (Adams et al. 2006), or with individual identification software that comes with database management, such as Happywhale (Cheeseman et al. 2021).\nCurating a query set from a batch of field images involves several steps that are often manual and labor-intensive. These include selecting images with animals (Beery et al. 2019), selecting images of sufficient quality (Urian et al. 2015), and selecting images of animals with distinctive markings (Rosel et al. 2011), which is necessary in partially marked populations. Additionally, users may want to limit the query set to only the highest quality images of each individual from a single encounter (e.g., a burst of images from a camera trap). This especially helpful for manual photo-ID because it can dramatically reduce the number of comparisons between the query and the reference set. Further, limiting the query set to only the best images from each encounter may reduce the chance of a missed match, i.e., a false negative error (Urian et al. 2015). One way to curate the query set this way is to “sort” the images by apparent individual and encounter (Figure 1).Sorting, however, is a labor-intensive process in itself, in that it requires \\(\\binom{m}{2}=m(m-1)/2\\) comparisons, where \\(m\\) is the number of images in the query set. As such, a query set of 500 images requires 124,750 comparisons to complete the sort.\n\n\n\n\n\n\nFigure 1: Example of sorted images. Before sorting, the entire query set is in one directory. After sorting, the query set is divided into many directories. In this example, the second level of directories are the temporary IDs and the third level of directories are the encounters\n\n\n\nWe developed a Python package, Pyseter, to help automate several of these steps. For example, Pyseter’s extract module extracts feature vectors from images, which are useful for estimating similarity scores and identifying individuals (Miele et al. 2021). The grade module includes functions for evaluating the distinctiveness of an animal’s markings. Finally, the sort module contains two clustering algorithms for classifying individuals into proposed identities. Additionally, sort includes functions for sorting images into proposed identities and encounters Figure 1. Pyseter currently lacks functions for detecting animals in images (Beery et al. 2019) or grading the quality of images (Rosel et al. 2011). We plan on adding these functions as the technology advances."
  },
  {
    "objectID": "manuscript.html#introduction",
    "href": "manuscript.html#introduction",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "",
    "text": "Photographic identification (photo-ID) is an effective and non-invasive method for studying many aspects of ecology, including movement (Gardiner et al. 2014), social behavior (Bejder et al. 1998), abundance (McPherson et al. 2024), survival (Morrison et al. 2011), distribution (McGuire et al. 2020), recruitment (Setyawan et al. 2022), migration (Hill et al. 2020), and resource-selection (Patton 2025). Photo-ID is a multi-step process that culminates in comparing a query set—images of animals whose identity we wish to know—against a reference set—images of known individuals. This last step can be accomplished by hand (Karanth 1995), with a database (Adams et al. 2006), or with individual identification software that comes with database management, such as Happywhale (Cheeseman et al. 2021).\nCurating a query set from a batch of field images involves several steps that are often manual and labor-intensive. These include selecting images with animals (Beery et al. 2019), selecting images of sufficient quality (Urian et al. 2015), and selecting images of animals with distinctive markings (Rosel et al. 2011), which is necessary in partially marked populations. Additionally, users may want to limit the query set to only the highest quality images of each individual from a single encounter (e.g., a burst of images from a camera trap). This especially helpful for manual photo-ID because it can dramatically reduce the number of comparisons between the query and the reference set. Further, limiting the query set to only the best images from each encounter may reduce the chance of a missed match, i.e., a false negative error (Urian et al. 2015). One way to curate the query set this way is to “sort” the images by apparent individual and encounter (Figure 1).Sorting, however, is a labor-intensive process in itself, in that it requires \\(\\binom{m}{2}=m(m-1)/2\\) comparisons, where \\(m\\) is the number of images in the query set. As such, a query set of 500 images requires 124,750 comparisons to complete the sort.\n\n\n\n\n\n\nFigure 1: Example of sorted images. Before sorting, the entire query set is in one directory. After sorting, the query set is divided into many directories. In this example, the second level of directories are the temporary IDs and the third level of directories are the encounters\n\n\n\nWe developed a Python package, Pyseter, to help automate several of these steps. For example, Pyseter’s extract module extracts feature vectors from images, which are useful for estimating similarity scores and identifying individuals (Miele et al. 2021). The grade module includes functions for evaluating the distinctiveness of an animal’s markings. Finally, the sort module contains two clustering algorithms for classifying individuals into proposed identities. Additionally, sort includes functions for sorting images into proposed identities and encounters Figure 1. Pyseter currently lacks functions for detecting animals in images (Beery et al. 2019) or grading the quality of images (Rosel et al. 2011). We plan on adding these functions as the technology advances."
  },
  {
    "objectID": "manuscript.html#installation",
    "href": "manuscript.html#installation",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "2 Installation",
    "text": "2 Installation\nHere, we assume some familiarity with Python, conda, and pip. Users coming from R, who may be less familiar with these concepts, should reference the “General Overview” Notebook that’s included in this manuscript’s attendant Zenodo repository. [Reviewers will find it in the Anonymous GitHub repository.]\nIf using conda, we recommend creating a fresh conda environment. Additionally, before installing Pyseter, users must install Pytorch. Follow the directions on the Pytorch website, which will vary based on your operating system and how you plan to use GPU acceleration. Users who plan on extracting features should have an NVIDIA GPU that is CUDA compatible, or a Mac with at least 16 GB of RAM. Below, we demonstrate the bash commands necessary to install Pytorch. These can be executed in a Jupyter Notebook (as below) or a command line interface (e.g., the miniforge prompt in Windows or the Terminal application in a Unix-like OS). After installing Pytorch, users can install Pyseter from PyPI.\n\n%%bash\nconda create -n pyseter_env  \nconda activate pyseter_env\nconda install pip3\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128\npip3 install pyseter\n\nUsers can verify the installation by running the following Python commands in, say, a Jupyter Notebook,\n\nimport pyseter\npyseter.verify_pytorch()\n\n✓ PyTorch 2.9.1 detected\n✓ Apple Silicon (MPS) GPU available\n\n\nwhich will tell the user which form of GPU acceleration is available, if any."
  },
  {
    "objectID": "manuscript.html#spinner-dolphin-example",
    "href": "manuscript.html#spinner-dolphin-example",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "3 Spinner dolphin example",
    "text": "3 Spinner dolphin example\nWe demonstrate the core modules and functions of Pyseter with a example using spinner dolphins (Stenella longirostris). Theoretically, Pyseter is taxon agnostic. Functions in the grade and sort modules work with similarity scores, which can be generated from any individual identification algorithm (Miele et al. 2021). The extract module, however, only includes one individual identification algorithm, namely, AnyDorsal, which is only suitable for cetacean dorsal images (see below) (Patton et al. 2023). As such, we chose a cetacean example to demonstrate the package’s full capabilities. The images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi (Lacey et al. 2025). Every image collected during the study was graded for quality and distinctiveness Lacey et al. (2025). This example dataset only includes images of sufficient quality that have been cropped to the identifying mark—the dorsal fin (Rosel et al. 2011) (Figure 2). This example includes 208 images of animals without distinctive markings, and 1043 images of animals with distinctive markings.\n\n\nCode\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# grab the first nine images in the dataset\nnrow = ncol = 3\ndemo_dir = 'working_dir/all_images'\ndemo_images = os.listdir(demo_dir)[:(nrow * ncol)]\n\n# plot a grid of images \nfig, axes = plt.subplots(nrow, ncol, figsize=(6, 6), tight_layout=True)\nfor i, filename in enumerate(demo_images):\n    path = os.path.join(demo_dir, filename)\n    image = Image.open(path)\n    axes.flat[i].imshow(image)\n    axes.flat[i].axis('off')\n\nfig.savefig('images/fig-demo.png', transparent=False, bbox_inches='tight', dpi=600)\n\n\n\n\n\n\n\n\nFigure 2: Nine images from the spinner dolphin example (Lacey et al. 2025)\n\n\n\n\n\n\n3.1 Folder management\nTo do keep things clean and tidy, we recommend establishing a working_directory with a subfolder, e.g., called, all_images, that contains every image that needs to be sorted (see below for a different case). The working directory should also contain a .csv with encounter information. This .csv would contain two columns: one for the image name, i.e., every image in all_images, and another for the encounter. As such, the working directory would look like this.\nworking_dir\n├── all_images\n│   ├── 0a49385ef8f1e74a.jpg\n│   ├── 0aca671c4afbd9b9.jpg\n        ...\n│   └── ffa8759a92174857.jpg\n└── encounter_info.csv\nAlternatively, you might have your images organized into subfolders by encounter.\nworking_dir\n└── original_images\n    ├── enc0\n    │   ├── 0a49385ef8f1e74a.jpg\n        ├── 1e105f9659c12a66.jpg\n            ...\n    │   └── f5093b3089b44e67.jpg\n    └── enc12\n        ├── 0b5c44f167d89d6c.jpg\n        ├── 1e0c186da31a53c4.jpg\n            ...\n        └── f9bb41e7ce0d672d.jpg\nIn this case, you might want to accomplish two tasks: move all these images to one folder, e.g., all_images, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The prep_images() function does just that.\n\nfrom pyseter.sort import prep_images\n\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)\n\nCopied 1251 images to: working_dir/all_images\nSaved encounter information to: /Users/PattonP/source/repos/pyseter/docs/working_dir/encounter_info.csv\n\n\n\n\n3.2 Extracting features\nPyseter’s extract module includes the AnyDorsal algorithm, which was trained to identify cetaceans of 24 species (Patton et al. 2023). AnyDorsal’s identifying performance varies by species. Species primarily identified by nicks and notches along the dorsal fin will perform best (Patton et al. 2023).\nPyseter extracts features with the FeatureExtractor class, which needs to be initialized by setting the batch_size. The batch_size dictates how many images will be processed by the GPU at once. Larger batch sizes might run faster yet might also produce an OutOfMemoryError. For lower memory GPUs, we recommend smaller batch sizes. If you encounter an OutOfMemoryError with batch_size=1, then you will have to reduce the size of your images. See the Supplement for how to do so with the Pillow library.\n\nfrom pyseter.extract import FeatureExtractor\n\n# feature extraction can take time so it's useful to save the result \nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\nfe = FeatureExtractor(batch_size=4)\n\nUsing device: mps (Apple Silicon GPU)\n\n\nThe extract() method of the FeatureExtractor class only takes one argument, image_dir, the flattened directory containing every image to be processed. Feature extraction can take several minutes, depending on the number of files and the GPU, so we recommend saving the results afterwards. The extract() function returns a Python dictionary where the filenames are the keys, and the features are the values. It can be useful to convert these to NumPy arrays\n\nimport numpy as np\n\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\n# convert keys and values to numpy arrays\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n\nAlternatively, we can load previously saved results with the load_features() function. In either case, feature_array will be a two-dimensional matrix of shape (n, 5504), where n is the number of images and 5504 is the number of features returned by AnyDorsal.\n\n# alternatively, load in the feature dictionary from file \nimport numpy as np \nfrom pyseter.sort import load_features\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)\n\n\n\n3.3 Grading individuals by distinctiveness\nHere, we introduce one of Pyseter’s clustering algorithms, NetworkCluster, because doing so helps understand the distinctiveness grading algorithm (see Section 3.4 for a more thorough description of NetworkCluster). Network clustering works with similarity scores, which represent the similarity between two individuals in a pair of images. We can define a threshold score, the match_threshold, above which we consider two individuals to be the same. That is, if the similarity score between two images is above a certain threshold, we cluster them into a proposed ID. As such, network clustering works by treating the query set as a network, where the nodes are images and the edges are similarity scores above a threshold. Each set of connected components, i.e., images whose similarity scores are above the match threshold, represents a proposed ID.\nWe might expect the indistinct individuals to cluster together. In the context of facial recognition, Deng et al. (2023) observed that “unrecognizable identities”, e.g., extremely blurry or masked faces, tend to cluster together. As such, for partially marked populations, the largest cluster in the query set may represent every indistinct individual. Following Deng et al. (2023), we can compute the average feature vector for this cluster. The distance between this average feature vector and the feature vector for each image is the distinctiveness score for that image. As such, the score applies to the image, not the animal. To get a score for an animal, users could average the distinctiveness scores across images for that animal.\n\nfrom pyseter.grade import rate_distinctiveness\ndistinctiveness = rate_distinctiveness(feature_array, match_threshold=0.5)\n\nUnrecognizable identity cluster consists of 196 images.\n\n\n/Users/PattonP/miniforge3/envs/pyseter_env/lib/python3.14/site-packages/pyseter/grade.py:35: UserWarning: Distinctiveness grades are experimental and should be verified.\n  warn(UserWarning('Distinctiveness grades are experimental and should be verified.'))\n\n\nWe can evaluate the effectiveness of these scores with AUC (i.e., the area under the receiver operating curve). AUC is a metric for evaluating a classifier’s ability to balance true positive and false positive rates. In this example, a classifier built from the distinctiveness grades achieves an AUC of 0.925 Figure 3.\n\n\nCode\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\nimport pandas as pd\n\n# merge the ers scores with the true values to compare for each image\nmapping = pd.read_csv('/Users/PattonP/datasets/pyseter-data/file_mapping.csv').iloc[:, 1:]\ners_df = pd.DataFrame({'image_new': filenames, 'ers': 1 - distinctiveness})\ners_df = ers_df.merge(mapping)\n\n# compute the curve first, which will get displayed\ny_score = ers_df['ers']\ny_test, _ = ers_df.distinctiveness.factorize()\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nfig, ax = plt.subplots(figsize=(4, 3))\n\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nroc_auc = roc_auc_score(y_test, y_score)\nax.text(0.95, 0.6, f'AUC={roc_auc:0.3f}', ha='right', va='top')\n\nax.set_title('ROC Curve for \\nDistinctiveness Classifier')\n\nfig.savefig('images/fig-auc.png', transparent=False, bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\nFigure 3: Receiving operator characteristic (ROC) curve for a classifier based on the ERS. The area under the curve (AUC), a measure of overall performance, is listed in the middle.\n\n\n\n\n\nThese distinctiveness scores are experimental in that have not been robustly tested across a variety of scenarios. Nevertheless, users might use them as a guide, potentially making distinctiveness grading somewhat easier.\n\n\n3.4 Clustering images into proposed IDs\nTo use NetworkCluster, users must first compute the similarity scores between each pair of images. After computing the scores, we cluster the images, where each cluster represents a proposed identity.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pyseter.sort import NetworkCluster, report_cluster_results\n\nsimilarity_scores = cosine_similarity(feature_array)\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n\nFollowing clusters may contain false positives:\n['ID_0001', 'ID_0006', 'ID_0008', 'ID_0021', 'ID_0110']\n\n\nAs mentioned above, network clustering has one major hyperparameter, match_threshold, which indicates whether two images should be grouped within a cluster. High thresholds mean that few images will be clustered together, creating many clusters. Very low thresholds mean that many images will be clustered together, creating few clusters. The report_cluster_results() function produces a quick and dirty summary of the number of clusters created, and the size of the largest cluster (i.e., the number of images associated to the most photographed individual). This is a quick sanity check. The results object has several useful attributes and methods (see below). For example, results.cluster_idx contains the proposed ID for each image.\n\nnetwork_idx = results.cluster_idx\nreport_cluster_results(network_idx)\n\nFound 208 clusters.\nLargest cluster has 128 images.\n\n\nIn this example, the nc.cluster_images() function warned that some clusters may contain “false positives.” False positive matches occur when two separate individuals fall under the same proposed ID. We can diagnose possible false positives by evaluating the network. Recall that, in the network, a blob of connected nodes (i.e., connected components) represents a proposed ID. These connected components, however, can sometimes look less like a blob and more like a barbell, where two sets of images have many connections amongst each other, yet the two blobs are only connected by a single link. We suspect that such clusters represent false positives, i.e., two sets of images for two individuals connected by one spurious link. We can plot the networks of the suspicious clusters with the results.plot_suspicious() function.\n\nresults.plot_suspicious()\n\n\n\n\n\n\n\nFigure 4: Proposed IDs that may contain false positive errors. Each image (colored) circle is linked to another image (gray line) if similarity between them exceeds a threshold. The colors of each circle represent potential IDs within the proposed ID.\n\n\n\n\n\nBoth ID_0011 and ID_0150 appear to have spurious links combining two separate IDs. To deal with this, we could increase manually separate these clusters, or increase the match threshold. Cluster ID_0002 represents the “unrecognizable individual” cluster (see Section 3.3).\nAs the number of images being clustered grows, the overall false positive rate also grows (this is analogous the multiple comparison problem in statistics). At some point, network matching becomes untenable; all but the highest match thresholds would produce too many false positives to be useful. For these cases, there is HierarchicalCluster, which relies on the Hierarchical Agglomerative Clustering algorithm provided by the popular machine learning package, scikit-learn (Pedregosa et al. 2011). Note that HierarchicalCluster will run noticeably slower than NetworkCluster. See the supplement for an example with HierarchicalCluster.\n\n\n3.5 Sorting images by proposed ID then encounter\nWith these cluster results, we can sort images by proposed ID then encounter. To do so, we need to create a Pandas DataFrame that indicates the proposed ID and encounter for each filename (McKinney 2010). Recall that we created the encounter_info.csv with the prep_images() function above.\n\nimport pandas as pd\n\nid_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})\n\n# join with the encounter information using \"encounter\" as a key \nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nid_df = id_df.merge(encounter_info)\nid_df.head()\n\n\n\n\n\n\n\n\nimage\nproposed_id\nencounter\n\n\n\n\n0\n2c8750b066372ab5.jpg\nID-0000\nenc8\n\n\n1\n568fc1d376b616a6.jpg\nID-0001\nenc8\n\n\n2\nddeb347716d7861c.jpg\nID-0002\nenc6\n\n\n3\nb65f9334b05f48f4.jpg\nID-0003\nenc0\n\n\n4\nd84aefa4d99d6f9a.jpg\nID-0004\nenc4\n\n\n\n\n\n\n\nFinally, to sort the images, we need to specify an output directory, then run the sort_images() function. Note that the ID DataFrame must have columns named image, proposed_id, and encounter. Otherwise sort_images() will not work.\n\nfrom pyseter.sort import sort_images\n\n# make an output directory \nsorted_dir = working_dir + '/sorted_images'\nos.makedirs(sorted_dir, exist_ok=True)\n\nsort_images(id_df, all_image_dir=image_dir, output_dir=sorted_dir)\n\nSorted 1251 images into 311 folders.\n\n\nNow the flat directory, all_images has been copied to a new folder, sorted_images, that is organized by proposed ID, then encounter.\nsorted_images\n├── ID-0000\n│   ├── enc0\n│   │   ├── 0a49385ef8f1e74a.jpg\n│   │   ├── 4d69031e07ef3393.jpg\n│   │   ├── b4f1ca6229180f18.jpg\n│   │   └── e0016bed0be5bf9f.jpg\n│   ├── enc10\n│   │   ├── 0cdbe7c151420b0c.jpg\n│   │   ├── 19dd8e9db3a26066.jpg\n            ...\n│   └── enc3\n│       ├── 1ced4b1e3bd63781.jpg\n            ...\n│       └── bc17d3aa572320cc.jpg\n├── ID-0001\n│   ├── enc0\n│   │   ├── e7dbbe01ac71d6e8.jpg\n│   │   └── f86cdadd6ecb59aa.jpg\n│   ├── enc2\n│   │   ├── 2b66eacdced6c165.jpg\n│   │   ├── 7ce2b670757443de.jpg\n            ... \n├── ID-0206\n│   └── enc4\n│       └── 767bdedefe51aabb.jpg\n└── ID-0207\n    └── enc4\n        └── e345d4ddaae7db02.jpg"
  },
  {
    "objectID": "manuscript.html#conclusion",
    "href": "manuscript.html#conclusion",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nWe have demonstrated one possible workflow for using Pyseter. A potential user would manually grade images for quality and crop images of sufficient quality to the identifying mark. Then, they would use Pyseter to grade these images for distinctiveness and to sort the images above a distinctiveness threshold into folders, selecting the highest quality image of each proposed individual. Finally, the user would compare these images against the reference set, either manually or with an interface such as Happywhale.\nNevertheless, Pyseter offers users with cetacean datasets flexibility to conduct several kinds of analyses because it allows them to extract feature vectors from images with AnyDorsal. For example, users could compute the false negative rate for their dataset, which is useful for population assessments (Patton et al. 2025). Similarly, one could use Pyseter to evaluate the predictive performance of AnyDorsal on a species that was outside the training dataset. Further, after the initial sort, users could clean up the results by doing a second round of clustering and sorting. As Pyseter matures, we plan to add documentation on how to conduct such analyses with Pyseter. Finally, as photo-ID technology improves, e.g., automated quality grading, we plan on adding these algorithms to Pyseter."
  },
  {
    "objectID": "manuscript.html#supplements",
    "href": "manuscript.html#supplements",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "5 Supplements",
    "text": "5 Supplements\n\n5.1 Hierarchical Agglomerative Clustering\nBelow is a demonstration of how to use the HAC algorithm to cluster images, then sort them into folders.\n\nfrom pyseter.sort import HierarchicalCluster, format_ids\n\nhc = HierarchicalCluster(match_threshold=0.5)\nhac_idx = hc.cluster_images(feature_array)\n\n# format_ids converts the integer labels to something like 'ID-0001'\nhac_labels = format_ids(hac_idx)\nreport_cluster_results(hac_labels)\n\nhac_df = pd.DataFrame({'image': filenames, 'proposed_id': hac_labels})\nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nhac_df = hac_df.merge(encounter_info)\n\n# separate directory for the hac images\nhac_dir = working_dir + '/sorted_images_hac'\nos.makedirs(hac_dir, exist_ok=True)\nsort_images(hac_df, all_image_dir=image_dir, output_dir=hac_dir)\n\nFound 299 clusters.\nLargest cluster has 27 images.\nSorted 1251 images into 403 folders.\n\n\nHierarchicalCluster is useful for large datasets, yet will be more prone to false negative errors. In this example, it found 60 more clusters (proposed IDs) than the network matching, which may be dubious. Users will have to decide how to balance false positive versus false negative matches. For example, we recommend that users preprocess their images with Pyseter, then identify animals in the pre-processed images manually or a program such as Happywhale. This second round of identification should help clean up false negative matches. As such, users following this approach might be more averse to false positive errors in the first stage.\n\n\n5.2 Resizing images with Pillow\nHere is how is one example on how to do so with the Pillow library.\n\nfrom PIL import Image\n\nnew_size = 512, 512\n\n# files for resizing and where they'll be sent\nall_images = [i for i in os.listdir(image_dir) if i.endswith('jpg')]\nnew_dir = os.path.join(working_dir, 'thumbnails')\nos.makedirs(new_dir, exist_ok=True)\n\n# resize and save the files \nfor file in all_images:\n    old_path = os.path.join(image_dir, file)\n    with Image.open(old_path) as im:\n        im.thumbnail(new_size)\n        new_path = os.path.join(new_dir, file)\n        im.save(new_path)"
  },
  {
    "objectID": "api/sort.sort_images.html",
    "href": "api/sort.sort_images.html",
    "title": "sort.sort_images",
    "section": "",
    "text": "sort.sort_images(id_df, all_image_dir, output_dir)\nSort images into subfolders by proposed ID then encounter.\nCopy images from the flat all_image_dir into the output_dir, where the output_dir is now divided in subfolders by proposed ID then encounter.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid_df\npd.DataFrame\nPandas DataFrame with columns ['image', 'proposed_id', 'encounter'].\nrequired\n\n\nall_image_dir\nstr\nPath to flat directory with every image in the id_df.\nrequired\n\n\noutput_dir\nstr\nPath to new directory into which sort_images will copy files.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nCopies images to the output_dir.\n\n\n\n\n\n\nFor a complete working example with real images, see:\n\nTutorial"
  },
  {
    "objectID": "api/sort.sort_images.html#parameters",
    "href": "api/sort.sort_images.html#parameters",
    "title": "sort.sort_images",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nid_df\npd.DataFrame\nPandas DataFrame with columns ['image', 'proposed_id', 'encounter'].\nrequired\n\n\nall_image_dir\nstr\nPath to flat directory with every image in the id_df.\nrequired\n\n\noutput_dir\nstr\nPath to new directory into which sort_images will copy files.\nrequired"
  },
  {
    "objectID": "api/sort.sort_images.html#returns",
    "href": "api/sort.sort_images.html#returns",
    "title": "sort.sort_images",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nCopies images to the output_dir."
  },
  {
    "objectID": "api/sort.sort_images.html#examples",
    "href": "api/sort.sort_images.html#examples",
    "title": "sort.sort_images",
    "section": "",
    "text": "For a complete working example with real images, see:\n\nTutorial"
  },
  {
    "objectID": "api/sort.ClusterResults.html",
    "href": "api/sort.ClusterResults.html",
    "title": "sort.ClusterResults",
    "section": "",
    "text": "sort.ClusterResults\nsort.ClusterResults(cluster_labels)\nStoring NetworkCluster results."
  },
  {
    "objectID": "api/get_best_device.html",
    "href": "api/get_best_device.html",
    "title": "get_best_device",
    "section": "",
    "text": "get_best_device\nget_best_device()\nSelect torch device based on expected performance."
  },
  {
    "objectID": "api/extract.FeatureExtractor.html",
    "href": "api/extract.FeatureExtractor.html",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "extract.FeatureExtractor(batch_size, device=None, stochastic=False)\nExtract features from images.\nExtract feature vectors for individual identification from images. Currently, FeatureExtractor only includes the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbatch_size\nint\nThe number of images the GPU will process.\nrequired\n\n\ndevice\n(None, cuda, mps, cpu)\nDevice with which to extract the features. By default, the best device is chosen for the user (cuda, mps, or cpu)\nNone\n\n\nstochastic\nboolean\nCurrently unused.\nFalse\n\n\n\n\n\n\nFor a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.extract import FeatureExtractor\n\n# Initialize extractor\nextractor = FeatureExtractor(batch_size=16)\n\n# Extract features from all images\nfeatures = extractor.extract('path/to/images/')\n\n# Access individual image features\nimg_features = features['my_image.jpg']\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nextract\nExtracts features from images.\n\n\n\n\n\nextract.FeatureExtractor.extract(image_dir, bbox_csv=None)\nExtracts features from images.\nExtracts feature vectors for every image in a directory with the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/extract.FeatureExtractor.html#parameters",
    "href": "api/extract.FeatureExtractor.html#parameters",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbatch_size\nint\nThe number of images the GPU will process.\nrequired\n\n\ndevice\n(None, cuda, mps, cpu)\nDevice with which to extract the features. By default, the best device is chosen for the user (cuda, mps, or cpu)\nNone\n\n\nstochastic\nboolean\nCurrently unused.\nFalse"
  },
  {
    "objectID": "api/extract.FeatureExtractor.html#examples",
    "href": "api/extract.FeatureExtractor.html#examples",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "For a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.extract import FeatureExtractor\n\n# Initialize extractor\nextractor = FeatureExtractor(batch_size=16)\n\n# Extract features from all images\nfeatures = extractor.extract('path/to/images/')\n\n# Access individual image features\nimg_features = features['my_image.jpg']"
  },
  {
    "objectID": "api/extract.FeatureExtractor.html#methods",
    "href": "api/extract.FeatureExtractor.html#methods",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nextract\nExtracts features from images.\n\n\n\n\n\nextract.FeatureExtractor.extract(image_dir, bbox_csv=None)\nExtracts features from images.\nExtracts feature vectors for every image in a directory with the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html",
    "href": "api/extract.FeatureExtractor.extract.html",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "extract.FeatureExtractor.extract(image_dir, bbox_csv=None)\nExtracts features from images.\nExtracts feature vectors for every image in a directory with the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html#parameters",
    "href": "api/extract.FeatureExtractor.extract.html#parameters",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone"
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html#returns",
    "href": "api/extract.FeatureExtractor.extract.html#returns",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504."
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html#raises",
    "href": "api/extract.FeatureExtractor.extract.html#raises",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/sort.NetworkCluster.html",
    "href": "api/sort.NetworkCluster.html",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "sort.NetworkCluster(match_threshold=0.5)\nNetwork clustering of images\nCluster images with a simple network, where images are nodes and edges are images whose similarity score is above the match_threshold\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nSimilarity score threshold above which two images are considered to contain the same animal. Must lie between [0.0, 1.0]\n0.5\n\n\n\n\n\n\nNetwork clustering works best with smaller datasets, say, around 1000 images.\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import NetworkCluster\n&gt;&gt;&gt; from sklearn.metrics.pairwise import cosine_similarity\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; scores = cosine_similarity(feature_array)\n&gt;&gt;&gt; \n&gt;&gt;&gt; nc = NetworkCluster(match_threshold=0.5)\n&gt;&gt;&gt; results = nc.cluster_images(scores)\n&gt;&gt;&gt; len(np.unique(results.cluster_idx))\n2\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.NetworkCluster.cluster_images(similarity, message=True)\nCluster images\nCluster images based on their similarity scores with network clustering.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsimilarity\nnp.ndarray\nArray with shape (image_count, image_count) indicating the similarity between each pair of images.\nrequired\n\n\nmessage\nbool\nShould a message about potential false positives be printed to the console?\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nresults\nClusterResults\nObject of type pyster.ClusterResult. Integer labels for the cluster assignment of each image can be accessed with results.cluster_idx."
  },
  {
    "objectID": "api/sort.NetworkCluster.html#parameters",
    "href": "api/sort.NetworkCluster.html#parameters",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nSimilarity score threshold above which two images are considered to contain the same animal. Must lie between [0.0, 1.0]\n0.5"
  },
  {
    "objectID": "api/sort.NetworkCluster.html#notes",
    "href": "api/sort.NetworkCluster.html#notes",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "Network clustering works best with smaller datasets, say, around 1000 images."
  },
  {
    "objectID": "api/sort.NetworkCluster.html#examples",
    "href": "api/sort.NetworkCluster.html#examples",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import NetworkCluster\n&gt;&gt;&gt; from sklearn.metrics.pairwise import cosine_similarity\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; scores = cosine_similarity(feature_array)\n&gt;&gt;&gt; \n&gt;&gt;&gt; nc = NetworkCluster(match_threshold=0.5)\n&gt;&gt;&gt; results = nc.cluster_images(scores)\n&gt;&gt;&gt; len(np.unique(results.cluster_idx))\n2"
  },
  {
    "objectID": "api/sort.NetworkCluster.html#methods",
    "href": "api/sort.NetworkCluster.html#methods",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.NetworkCluster.cluster_images(similarity, message=True)\nCluster images\nCluster images based on their similarity scores with network clustering.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsimilarity\nnp.ndarray\nArray with shape (image_count, image_count) indicating the similarity between each pair of images.\nrequired\n\n\nmessage\nbool\nShould a message about potential false positives be printed to the console?\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nresults\nClusterResults\nObject of type pyster.ClusterResult. Integer labels for the cluster assignment of each image can be accessed with results.cluster_idx."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html",
    "href": "api/sort.HierarchicalCluster.html",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "sort.HierarchicalCluster(match_threshold=0.5)\nHierarchical clustering of images\nCluster images with the hierarchical agglomerative clustering (HAC) algorithm from scikit-learn.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nThreshold dictating how closely knit clusters should be. Must be between zero and one.\n0.5\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import HierarchicalCluster\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; \n&gt;&gt;&gt; hac = HierarchicalCluster(match_threshold=0.5)\n&gt;&gt;&gt; cluster_indices = hac.cluster_images(feature_array)\n&gt;&gt;&gt; len(np.unique(cluster_indices))\n2\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmatch_threshold\n\nThreshold indicating how closely knit clusters should be.\n\n\n\n\n\n\nHierarchicalCluster works best for larger datasets, say, over 1000 images. HierarchicalCluster may be prone to false negative errors.\nHierarchicalCluster uses the version of HAC with a distance threshold specified–i.e., an unknown number of clusters–complete linkage, and cosine as the distance metric.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.HierarchicalCluster.cluster_images(features)\nCluster images\nCluster feature vectors according to their cosine distance from one another.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nArray with shape (image_count, feature_count) containing the feature vector for each image.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nNumPy array containing integer labels for each cluster."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#parameters",
    "href": "api/sort.HierarchicalCluster.html#parameters",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nThreshold dictating how closely knit clusters should be. Must be between zero and one.\n0.5"
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#examples",
    "href": "api/sort.HierarchicalCluster.html#examples",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import HierarchicalCluster\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; \n&gt;&gt;&gt; hac = HierarchicalCluster(match_threshold=0.5)\n&gt;&gt;&gt; cluster_indices = hac.cluster_images(feature_array)\n&gt;&gt;&gt; len(np.unique(cluster_indices))\n2"
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#attributes",
    "href": "api/sort.HierarchicalCluster.html#attributes",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nmatch_threshold\n\nThreshold indicating how closely knit clusters should be."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#notes",
    "href": "api/sort.HierarchicalCluster.html#notes",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "HierarchicalCluster works best for larger datasets, say, over 1000 images. HierarchicalCluster may be prone to false negative errors.\nHierarchicalCluster uses the version of HAC with a distance threshold specified–i.e., an unknown number of clusters–complete linkage, and cosine as the distance metric."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#methods",
    "href": "api/sort.HierarchicalCluster.html#methods",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.HierarchicalCluster.cluster_images(features)\nCluster images\nCluster feature vectors according to their cosine distance from one another.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nArray with shape (image_count, feature_count) containing the feature vector for each image.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nNumPy array containing integer labels for each cluster."
  },
  {
    "objectID": "negative.html",
    "href": "negative.html",
    "title": "Computing false negative rates",
    "section": "",
    "text": "A subtle but important choice when deciding how to automate photo-ID is the number of proposed matches you are going to evaluate before deciding that an individual is new to the dataset. An algorithm optimist might argue that you just need to check the first proposed ID to make sure it’s not a false positive (i.e., claiming two distinct individuals are one). An algorithm skeptic might argue that you need to check every proposed ID, i.e., every individual in the reference set, before adding a new individual to the dataset.\nWe can reframe this debate in terms of false negative rates. A false negative occurs when you didn’t look far enough down the list of proposed IDs, and added a new individual that already existed in the dataset. Our algorithm optimist is arguing that the algorithm won’t produce false negatives, or they aren’t important. Our algorithm skeptic is arguing that the algorithm will produce false negatives and that false negatives are critical.\nThe skeptic is right in that false negatives are critically important. An 8% false negative rate might seem small, but it suggests that you will overestimate your population size by 20% (Patton et al. 2025). This overestimation can have grim consequences if, say, the estimate is used to compute potential biological removal.\nEither the skeptic or the optimist could be right about the prevalence of false negatives. But why trust them, when we can estimate them ourselves?",
    "crumbs": [
      "Examples",
      "False negative rates"
    ]
  },
  {
    "objectID": "negative.html#dataset",
    "href": "negative.html#dataset",
    "title": "Computing false negative rates",
    "section": "Dataset",
    "text": "Dataset\nTo demonstrate how to compute false negative rates, we’ll use the Happy Whale and Dolphin Kaggle competition dataset as an example. You can download the data by following that linked page (click the big “Download all” button). FYI, you’ll have to create an account first.\n\nfrom pyseter.sort import load_features\nfrom pyseter.identify import predict_ids\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef where(list, value):\n    \"\"\"Where in the list is the \"\"\"\n    try:\n        return (list.index(value) + 1)\n    except ValueError:\n        return np.nan\n\n# load in the feature vectors\ndata_dir = '/Users/PattonP/datasets/happywhale/'\nfeature_dir = data_dir + '/features'\n\nreference_path = feature_dir + '/train_features.npy'\nreference_files, reference_features = load_features(reference_path)\n\nquery_path = feature_dir + '/test_features.npy'\nquery_files, query_features = load_features(query_path)\n\n# get the IDs for every individual in the happywhale set\ndata_url = (\n    'https://raw.githubusercontent.com/philpatton/pyseter/main/' \n    'data/happywhale-ids.csv'\n)\nid_df = pd.read_csv(data_url)\n\nid_df = id_df.set_index('image')\nid_df.head(5)\n\n\n\n\n\n\n\n\nspecies\nindividual_id\n\n\nimage\n\n\n\n\n\n\n000110707af0ba.jpg\ngray_whale\nfbe2b15b5481\n\n\n00021adfb725ed.jpg\nmelon_headed_whale\ncadddb1636b9\n\n\n000562241d384d.jpg\nhumpback_whale\n1a71fbb72250\n\n\n0006287ec424cb.jpg\nfalse_killer_whale\n1424c7fec826\n\n\n0007c33415ce37.jpg\nfalse_killer_whale\n60008f293a2b\n\n\n\n\n\n\n\n\n\nCode\n# excel on mac corrupts the IDs (no need to do this on PC or linux)\nid_df['individual_id'] = id_df['individual_id'].apply(\n    lambda x: str(int(float(x))) if 'E+' in str(x) else x\n)",
    "crumbs": [
      "Examples",
      "False negative rates"
    ]
  },
  {
    "objectID": "negative.html#predicting-ids",
    "href": "negative.html#predicting-ids",
    "title": "Computing false negative rates",
    "section": "Predicting IDs",
    "text": "Predicting IDs\nWe’re also going to peek under the hood of identify.predict_ids. This is helpful because the number of proposed IDs will vary a lot with such a large and diverse dataset as the Happywhale dataset.\n\nfrom pyseter.identify import find_neighbors, insert_new_id, pool_predictions\nimport numpy as np\n\n# this is the true id of every id in the reference dataset\nids = id_df.loc[reference_files, 'individual_id'].to_numpy()\n\n# takes about 19 seconds\ndistance_matrix, index_matrix = find_neighbors(reference_features, query_features)\n\n# get the corresponding labels for each reference image\npredicted_ids = ids[index_matrix]\n\n# insert the prediction \"new_individual\" at the threshold\ndistances, ids = insert_new_id(distance_matrix, predicted_ids, threshold=0.5)\n\n# remove redundant predictions and take the minimum distance \npooled_distances, pooled_ids = pool_predictions(ids, distances)\n\nNow we want to find where in the pooled_ids is the true identity of the animal. If the algorithm’s first guess was right, then this value should be 1. As such, we are finding the rank of the correct guess for each query image.\n\nrecords = []\nfor i, image in enumerate(query_files):\n\n    # where is the true id in the list of predicted IDs?\n    true_id = id_df.loc[image]['individual_id']\n    rank = where(pooled_ids[i].tolist(), true_id)\n\n    # these will become the rows in our dataframe\n    records.append({'image': image, 'rank': rank})\n\ndf = pd.DataFrame.from_records(records).set_index('image').join(id_df)\ndf.head()\n\n\n\n\n\n\n\n\nrank\nspecies\nindividual_id\n\n\nimage\n\n\n\n\n\n\n\na704da09e32dc3.jpg\n5.0\nfrasiers_dolphin\n43dad7ffa3c7\n\n\nde1569496d42f4.jpg\n1.0\npilot_whale\ned237f7c2165\n\n\n4ab51dd663dd29.jpg\n1.0\nbeluga\nb9b24be2d5ae\n\n\nda27c3f9f96504.jpg\n1.0\nbottlenose_dolpin\nc02b7ad6faa0\n\n\n0df089463bfd6b.jpg\n3.0\ndusky_dolphin\nnew_individual\n\n\n\n\n\n\n\nSo in the case of the dusky dolphin image, 0df089463bfd6b.jpg, the algorithm’s first two guesses were that this individual was in the reference set, when in reality it was new to the reference set.",
    "crumbs": [
      "Examples",
      "False negative rates"
    ]
  },
  {
    "objectID": "negative.html#computing-false-negative-rates",
    "href": "negative.html#computing-false-negative-rates",
    "title": "Computing false negative rates",
    "section": "Computing false negative rates",
    "text": "Computing false negative rates\nNow we want to understand what our false negative rate would have been had we tried different strategies. These strategies, proposed_id_count, correspond to the arguments between the AI skeptic and the AI optimist. At one extreme, we only look at the first proposed ID. At the other, we look through the first 25 proposed IDs. Here, we assume that there are no false positive matches.\nFor fun, we’ll look at the average across species. Note that this is a naive approach, because the algorithm’s performance can vary widely across catalogs for the same species (Patton et al. 2023).\n\ndf_list = []\n\n# how many of the proposed ids did you check?\nfor proposed_id_count in range(1, 26):\n\n    # was the true id further down the list?\n    # i.e., had you kept looking would you have found it?\n    missed_match = df['rank'] &gt; proposed_id_count\n\n    # is this individual in the reference set? we're assuming no false positives\n    not_new = df['individual_id'] != 'new_individual'\n\n    # if both are true, then you committed a false negative error\n    df['error'] = missed_match & not_new\n\n    # compute the average for each species \n    fn_df = df.groupby('species')['error'].mean().rename('fn_rate').reset_index()\n    fn_df['proposed_id_count'] = proposed_id_count\n    \n    df_list.append(fn_df)\n\nWe can translate these false negative rates into expected relative bias in our estimate of the total population size. A relative bias of 10% means that we overestimate the population by 10%. For every one percentage point increase in the false negative rate, our relative bias increases by 2.56 percentage points (Patton et al. 2025). We’re going to exclude the Fraser’s dolphin catalog, which had extremely poor performance.\n\nfn_df = pd.concat(df_list)\nfn_df['rbias'] = fn_df['fn_rate'] * 2.56\nfn_df = fn_df.loc[fn_df['species'] != 'frasiers_dolphin'].reset_index()\n\nNow we can plot the results for each species. We’ve highlighted five randomly selected species to reduce over plotting.\n\n\nCode\nspecials = fn_df.sample(5, random_state=10).species.unique()\nspecial_df = fn_df.loc[fn_df.species.isin(specials)]\nnonspecial_df = fn_df.loc[~fn_df.species.isin(specials)]\n\nfig, ax = plt.subplots(figsize = (7, 5), tight_layout=True)\n\nfor name, df in nonspecial_df.groupby('species'):\n    ax.plot(df.proposed_id_count, df.rbias, alpha=0.3, c='tab:grey', zorder=-2)\n\nfor name, df in special_df.groupby('species'):\n    ax.plot(df.proposed_id_count, df.rbias, label=name.replace('_', ' '), linewidth=2.5)\n\nimport matplotlib.ticker as mtick\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n\nax.set_ylabel(r'Relative bias in $N$', fontsize=14)\nax.set_xlabel(r'Proposed IDs checked', fontsize=14)\nax.set_ylim((0, 0.6))\n\nax.spines[:].set_visible(False)\n\nax.xaxis.tick_bottom()\nax.yaxis.tick_left()\n\nax.grid(True, 'major', 'both', ls='--', lw=.5, c='k', alpha=.3)\n\nax.tick_params(axis='both', which='both', labelsize='large',\n               bottom=False, top=False, labelbottom=True,\n               left=False, right=False, labelleft=True)\n\nax.legend()\n\n\n\n\n\n\n\n\n\nWe can see that most species get below 10% once we’ve checked up to 10 proposed matches. In fact, 26 of the 39 datasets achieved a relative bias less than 10% at 10 proposed matches (Patton et al. 2025).\nOne important caveat that could be depressing performance is that, in this case, we’re matching against all species. As such, the 8th, 9th, 10th, proposed ID for a long-finned pilot whale may indeed by a short-finned pilot whale. In real life, biologists will know not to match against the wrong species. Correcting for this would decrease the false negative rate for all species.",
    "crumbs": [
      "Examples",
      "False negative rates"
    ]
  }
]