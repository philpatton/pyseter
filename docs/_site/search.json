[
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Introduction to pyseter",
    "section": "",
    "text": "Pyseter is an Python package for sorting images by an automatically generated ID. The main functions of Pyseter are:\nThis notebook will walk you through each major function. First, let’s make sure that Pyseter is properly installed, and that it can access Pytorch.\nimport pyseter\n\npyseter.verify_pytorch()\n\n✓ PyTorch 2.7.1+cu126 detected\n✓ CUDA GPU available: NVIDIA RTX A4000\nIf you’re on a Mac, you should see something like\nPlease note, however, that AnyDorsal consumes quite a bit of memory. As such, only Apple Silicon devices with 16 GB or more of memory will work. Ideally, future versions of Pyseter will use a smaller model.\nIf neither Apple Silicon or an NVIDIA GPU are available, you will see a message like this.\nPackages in Python tend to be subdivided into modules based on their functions. In Pyseter, the sort module contains functions for sorting files, including other forms of file management."
  },
  {
    "objectID": "tutorial.html#optional-folder-management",
    "href": "tutorial.html#optional-folder-management",
    "title": "Introduction to pyseter",
    "section": "Optional: Folder management",
    "text": "Optional: Folder management\nThe main purpose of Pyseter is organizing images into folders. To do keep things clean and tidy, we recommend establishing a working directory with a subfolder, e.g., called, all images, that contains every image you want to be sorted (see below for a different case). Optionally, you might want to have a .csv with encounter information in the working directory. This .csv would contain two columns: one for the image name, i.e., every image in all images, and another for the encounter. As such, the working directory would look like this.\nworking directory\n├── encounter_info.csv\n├── all images\n│   └──00cef32dc62b0f.jpg\n│   └──3ecc025ea6f9bf.jpg\n│   └──9f18762a48696b.jpg\n│   └──36f78517a512dd.jpg\n│   └──470d524b4d5303.jpg\n       ...\n│   └──4511c9e5cb7acb.jpg\nSometimes, you might have your images organized into subfolders by encounter.\nworking_dir\n└── original_images\n    ├── SL_HI_006_20220616 (CROPPED)\n    │   ├── 2022-06-16_CLD500_CL_006.JPG\n    │   ├── 2022-06-16_CLD500_CL_007.JPG\n    │   ├── 2022-06-16_CLD500_CL_008.JPG\n    │   ├── 2022-06-16_CLD500_CL_021.JPG\n    │   ├── 2022-06-16_CLD500_CL_042.JPG\n...\n    ├── SL_HI_007_20220616 (CROPPED)\n    │   ├── 2022-06-16_CLD500_CL_346.JPG\n    │   ├── 2022-06-16_CLD500_CL_347.JPG\n    │   ├── 2022-06-16_CLD500_CL_371.JPG\n    │   ├── 2022-06-16_CLD500_CL_372.JPG\nIn this case, you might want to accomplish two tasks: move all these images to one folder, e.g., all_images, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The prep_images() function does just that.\n\nfrom pyseter.sort import prep_images\n\n# various directories we'll be working with\nworking_dir = '/home/pattonp/koa_scratch/id_data/working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new directory containing every image\nimage_dir = working_dir + '/all_images'\n\n# copy images to a single folder, then \nprep_images(original_image_dir, all_image_dir=image_dir)\n\nCopied 1230 images to: /home/pattonp/koa_scratch/id_data/working_dir/all_images\nSaved encounter information to: /home/pattonp/koa_scratch/id_data/working_dir/encounter_info.csv\n\n\n\n\n\n\n\n\nR user tip\n\n\n\nIn python, you can concatenate strings with the + operator. This is equivalent to paste0(working_dir, '/original_images') in R.\n\n\n\n\n\n\n\n\nR user tip\n\n\n\nPackages in Python tend to be subdivided into modules based on their functions. In Pyseter, the sort module contains functions for sorting files, including other forms of file management."
  },
  {
    "objectID": "tutorial.html#extracting-features",
    "href": "tutorial.html#extracting-features",
    "title": "Introduction to pyseter",
    "section": "1. Extracting features",
    "text": "1. Extracting features\nPyseter identifies individuals by extracting feature vectors from images. Feature vectors summarize three-dimensional images into one-dimensional vectors that are useful for the task at hand, in this case, individual identification.\nPyseter extracts feature vectors with AnyDorsal, an algorithm for identifying whales and dolphins of many species. AnyDorsal is the same dorsal identification algorithm that’s included with Happywhale (although not to be confused with their humpback whale fluke ID algorithm).\nBefore we extract the feature vectors, let’s first create a subfolder within our working directory to save them in. This isn’t necessary, yet keeps things tidy.\n\nimport os\n\n# in case you want to save the features after extracting them \nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n\n\n\n\n\n\nR user tip\n\n\n\nThe module, os, is part of Python’s standard library. People often refer to R and its standard libraries as “base R.” Base R includes the stats library, which provides the function rnorm. The os module has many functions for tinkering with your operating system.\n\n\nWe will extract features with the FeatureExtractor class. To do so, we first need to initialize the class. This sets up important parameters, such as the batch_size, which is the number of images that will be processed in parallel. Larger batches should run faster, although your mileage may vary. If you specify too large of a batch, you may encounter an OutOfMemoryError (see below for an example). If you encounter this error, try specifying a larger batch size. If you encounter this error with a very small batch size (say, 2), you may need to resize your images. You can do this manually by reducing the file size in an image editing software, or with Python\nOutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 5.81 GiB of which 9.06 MiB is free. Including non-PyTorch memory, this process has 5.76 GiB memory in use. Of the allocated memory 5.64 GiB is allocated by PyTorch, and 50.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n\n\n\n\n\nR user tip\n\n\n\nPython error messages are comically long, putting CVS receipts to shame. This is because they show the entire traceback, i.e., this error caused this error caused this error, etc. To quickly diagnose the problem, scroll to the bottom of the message. Then, you can further dissect it by scrolling up.\n\n\n\nfrom pyseter.extract import FeatureExtractor\n\n# specify the configuration for the extractor \nfe = FeatureExtractor(batch_size=4,)\n\nUsing device: cuda (NVIDIA RTX A4000)\n\n\nOnce we’ve initialize the class, we can use its associated methods (functions). In this case, the only one we are interested in is extract(), which extracts a feature vector for every image in a specified directory. This can take several minutes, so we typically save the results afterwards.\n\n\n\n\n\n\nR user tip\n\n\n\nClasses and methods also exist in R, but operate more behind the scenes. For example, x &lt;- data.frame() initializes an object of class data.frame, and summary(x) calls the summary method for data.frames. Python makes this relationship more explicit. For example, the equivalent (although nonsensical) Python code would be x = data.frame() and x.summary().\n\n\n\nimport numpy as np\n\n# extract the features for the input directory then save them\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves them as an numpy array\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\nLoading model...\n\n\n/home/pattonp/.conda/envs/pyseter/lib/python3.13/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_l2_ns to current tf_efficientnet_l2.ns_jft_in1k.\n  model = create_fn(\n\n\nWarning: Missing keys when loading pretrained weights: ['head.weight']\nWarning: Unexpected keys when loading pretrained weights: ['head.fc.weight']\nExtracting features...\n\n\n100%|██████████| 308/308 [01:48&lt;00:00,  2.84it/s]\n\n\nThe object features is a dictionary, whose keys are the filenames and whose values are the feature vectors associated with each filename. This helps ensure that each image is associated with the correct feature vector. Nevertheless, it can be easier to work with actual numpy arrays. To do so, convert the keys to a list, then to a numpy array.\n\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n\n\n\n\n\n\n\nR user tip\n\n\n\nThe R objects that Python’s dictionary most resemble is the named vector or the list. Like a list, dictionaries can hold different data types. Unlike a list, dictionaries have no order, and therefore cannot be integer indexed.\n\n\nIf you’ve already extracted and saved features, you can load them with the code below.\n\n# import numpy as np\n# out_path = feature_dir + '/features.npy'\n# features = np.load(out_path, allow_pickle=True).item()\n# filenames = np.array(list(features.keys()))\n# feature_array = np.array(list(features.values()))"
  },
  {
    "objectID": "tutorial.html#clustering-images-by-proposed-id",
    "href": "tutorial.html#clustering-images-by-proposed-id",
    "title": "Introduction to pyseter",
    "section": "2. Clustering images by proposed ID",
    "text": "2. Clustering images by proposed ID\nPyseter comes with two algorithm’s for clustering images by proposed ID. Network clustering works better for small datasets. To use network clustering, we first need to compute the similarity scores between each pair of images.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarity_scores = cosine_similarity(feature_array)\n\nThis indicates how similar the individuals in each image are.\nNext, we can perform the clustering. Each cluster represents a proposed ID. To access the “cluster labels” (these are just integers representing the cluster), we can access the cluster_idx attribute, i.e, results.cluster_idx.\n\nfrom pyseter.sort import NetworkCluster, report_cluster_results\n\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n\nFollowing clusters may contain false positives:\n['ID_0013', 'ID_0040', 'ID_0061', 'ID_0079', 'ID_0086', 'ID_0110']\n\n\nNetwork clustering has one major hyperparameter, namely, the match_threshold, which indicates whether two images should be grouped within a cluster. That is, if the similarity score between two images is above a certain threshold, we cluster them into a proposed ID. High thresholds mean that few images will be clustered together, creating many clusters. Very low thresholds mean that many images will be clustered together, creating few clusters. report_cluster_results produces a quick and dirty summary of the number of clusters created, and the size of the largest cluster (i.e., the number of images associated to the most photographed individual). This is a quick sanity check.\n\nnetwork_idx = results.cluster_idx\nreport_cluster_results(network_idx)\n\nFound 245 clusters.\nLargest cluster has 30 images.\n\n\nYou’ll also note that nc.cluster_images() warns that some clusters may contain “false positives.” False positive matches occur when two separate individuals fall under the same proposed ID. We can diagnose possible false positives by evaluating the network. In this case, the network consists of nodes (images) and edges, which represent connections between images. Two images are connected when they have a similarity score above the threshold. A blob of connected nodes (i.e., connected components) represents a proposed ID.\nSometimes, the connected components look less like a blob and more like a barbell, where two sets of images have many connections amongst each other, yet these two blobs are only connected by one link. We suspect that such clusters contain false positives, i.e., two sets of images for two individuals connected by one spurious link. We can plot the networks of the suspicious clusters with results.plot_suspicious().\n\nresults.plot_suspicious()\n\n\n\n\n\n\n\n\nID_0033 clearly has a barbell shape, suggesting that this cluster consists of images for two individuals linked together by one spurious connection.ID_0013 also seems dubious, and ID_0060 might consist of four separate individuals.\nUsers can deal with suspicious clusters in several ways. First, raising the match threshold should reduce the number of false positive matches. That said, this will increase the false negative rate, whereby images of one individual are spread across multiple proposed IDs. Alternatively, users can manually inspect the images within the clusters and, if need be, divide up the images.\nAs the number of images being clustered grows, the overall false positive rate also grows (this is analogous the multiple comparison problem in statistics). At some point,the network matching becomes untenable; all but the highest match thresholds would produce too many false positives to be useful.\nFor these cases, there is HierarchicalCluster, which relies on the Hierarchical Agglomerative Clustering algorithm provided by the popular machine learning package, scikit-learn. Note that HierarchicalCluster will run noticeably slower than the NetworkCluster.\n\nfrom pyseter.sort import HierarchicalCluster, format_ids\n\n# initialize the object then cluster away! \nhc = HierarchicalCluster(match_threshold=0.5)\nhac_idx = hc.cluster_images(feature_array)\n\nThe HierarchicalCluster results object is much simpler, in that it just returns the cluster indices for each image. We can make these labels a little prettier with the format_ids function.\n\nhac_labels = format_ids(hac_idx)\nprint(hac_labels[:5])\n\n# quick summary of the clustering results\nreport_cluster_results(hac_labels)\n\n['ID-0092', 'ID-0159', 'ID-0075', 'ID-0000', 'ID-0034']\nFound 306 clusters.\nLargest cluster has 19 images.\n\n\nHierarchicalCluster is useful for large datasets, yet will be more prone to false negative errors. In this example, it found 60 more clusters (proposed IDs) than the network matching, which may be dubious. Users will have to decide for themselves how to balance false positive versus false negative matches. For example, we recommend that users preprocess their images with Pyseter, then identify animals in the pre-processed images manually or with Happywhale. This second round of identification should help clean up false negative matches. As such, users following this approach might be more averse to false positive errors in the first stage."
  },
  {
    "objectID": "tutorial.html#sorting-images-by-proposed-id",
    "href": "tutorial.html#sorting-images-by-proposed-id",
    "title": "Introduction to pyseter",
    "section": "3. Sorting images by proposed ID",
    "text": "3. Sorting images by proposed ID\nOnce we’ve identified individuals, we can sort them into folder by proposed ID and encounter. To do so, we need to create a pandas DataFrame that indicates the proposed ID and encounter for each filename. Recall that we created the encounter_info.csv with the prep_images() function above. For this sort, we’ll use the NetworkCluster results\n\nimport pandas as pd\n\n# create a dataframe proposed id and encounter for each image\nnetwork_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})\nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nnetwork_df = network_df.merge(encounter_info)\n\nnetwork_df.head()\n\n\n\n\n\n\n\n\nimage\nproposed_id\nencounter\n\n\n\n\n0\n2022-06-25_CLD500_CL_735.jpg\nID-0000\nSL_HI_016_20220625 (CROPPED)\n\n\n1\n2021-11-17_Hilo_AP_2_UNK_0330.jpg\nID-0001\nSL_Hilo_13_20211117 (CROPPED)\n\n\n2\n2022-06-20_CLD500_CL_1166.jpg\nID-0002\nSL_HI_010_20220620 (CROPPED)\n\n\n3\n2022-06-16_D750_ANNM_062.JPG\nID-0003\nSL_HI_006_20220616 (CROPPED)\n\n\n4\n2022-06-20_CLD500_CL_453.jpg\nID-0004\nSL_HI_010_20220620 (CROPPED)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFormatting the ID DataFrame\n\n\n\nThe ID DataFrame must have columns named image, proposed_id, and encounter. Otherwise sort_images will not work. Luckily, changing column names in pandas is straightforward.\nnetwork_df.columns = ['image', 'proposed_id', 'encounter']\n\n\n\n\n\n\n\n\nR user tip\n\n\n\nPandas is a package for managing dataframes, and is a straight knock-off of R’s data.frame. In my experience, most R users lose patience not with Python, but with pandas. Pandas is just similar enough to R that one should be able to directly port R ideas over. However, key differences between pandas and R prevent this, causing immense frustration (especially for yours truly!)\n\n\nTo sort the images, we need to specify an output directory, then run the sort_images function.\n\nfrom pyseter.sort import sort_images\n\n# make an output directory \nsorted_dir = working_dir + '/sorted_images'\nos.makedirs(sorted_dir, exist_ok=True)\n\n# sort the images into folders based on proposed id\nsort_images(network_df, all_image_dir=image_dir, output_dir=sorted_dir)\n\nSorted 1230 images into 335 folders.\n\n\nWe can check to see that this worked by plotting a grid of images with matplotlib.\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# get the files associated with the first ID\nfirst_id_dir = sorted_dir + '/ID-0000'\nthis_encounter = os.listdir(first_id_dir)[0]\nencounter_dir = first_id_dir + '/' + this_encounter\nencounter_files = os.listdir(encounter_dir)\n\n# plot a grid of images \nfig, axes = plt.subplots(3, 3, tight_layout=True)\nfor i, filename in enumerate(encounter_files[:9]):\n    path = encounter_dir + '/' + filename\n    image = Image.open(path)\n    axes.flat[i].imshow(image)\n    axes.flat[i].axis('off')"
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html",
    "href": "api/sort.HierarchicalCluster.html",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "sort.HierarchicalCluster(match_threshold=0.5)\nHierarchical clustering of images\nCluster images with the hierarchical agglomerative clustering (HAC) algorithm from scikit-learn.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nThreshold dictating how closely knit clusters should be. Must be between zero and one.\n0.5\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import HierarchicalCluster\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; \n&gt;&gt;&gt; hac = HierarchicalCluster(match_threshold=0.5)\n&gt;&gt;&gt; cluster_indices = hac.cluster_images(feature_array)\n&gt;&gt;&gt; len(np.unique(cluster_indices))\n2\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmatch_threshold\n\nThreshold indicating how closely knit clusters should be.\n\n\n\n\n\n\nHierarchicalCluster works best for larger datasets, say, over 1000 images. HierarchicalCluster may be prone to false negative errors.\nHierarchicalCluster uses the version of HAC with a distance threshold specified–i.e., an unknown number of clusters–complete linkage, and cosine as the distance metric.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.HierarchicalCluster.cluster_images(features)\nCluster images\nCluster feature vectors according to their cosine distance from one another.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nArray with shape (image_count, feature_count) containing the feature vector for each image.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nNumPy array containing integer labels for each cluster."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#parameters",
    "href": "api/sort.HierarchicalCluster.html#parameters",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nThreshold dictating how closely knit clusters should be. Must be between zero and one.\n0.5"
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#examples",
    "href": "api/sort.HierarchicalCluster.html#examples",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import HierarchicalCluster\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; \n&gt;&gt;&gt; hac = HierarchicalCluster(match_threshold=0.5)\n&gt;&gt;&gt; cluster_indices = hac.cluster_images(feature_array)\n&gt;&gt;&gt; len(np.unique(cluster_indices))\n2"
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#attributes",
    "href": "api/sort.HierarchicalCluster.html#attributes",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nmatch_threshold\n\nThreshold indicating how closely knit clusters should be."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#notes",
    "href": "api/sort.HierarchicalCluster.html#notes",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "HierarchicalCluster works best for larger datasets, say, over 1000 images. HierarchicalCluster may be prone to false negative errors.\nHierarchicalCluster uses the version of HAC with a distance threshold specified–i.e., an unknown number of clusters–complete linkage, and cosine as the distance metric."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#methods",
    "href": "api/sort.HierarchicalCluster.html#methods",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.HierarchicalCluster.cluster_images(features)\nCluster images\nCluster feature vectors according to their cosine distance from one another.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nArray with shape (image_count, feature_count) containing the feature vector for each image.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nNumPy array containing integer labels for each cluster."
  },
  {
    "objectID": "api/sort.NetworkCluster.html",
    "href": "api/sort.NetworkCluster.html",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "sort.NetworkCluster(match_threshold=0.5)\nNetwork clustering of images\nCluster images with a simple network, where images are nodes and edges are images whose similarity score is above the match_threshold\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nSimilarity score threshold above which two images are considered to contain the same animal. Must lie between [0.0, 1.0]\n0.5\n\n\n\n\n\n\nNetwork clustering works best with smaller datasets, say, around 1000 images.\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import NetworkCluster\n&gt;&gt;&gt; from sklearn.metrics.pairwise import cosine_similarity\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; scores = cosine_similarity(feature_array)\n&gt;&gt;&gt; \n&gt;&gt;&gt; nc = NetworkCluster(match_threshold=0.5)\n&gt;&gt;&gt; results = nc.cluster_images(scores)\n&gt;&gt;&gt; len(np.unique(results.cluster_idx))\n2\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.NetworkCluster.cluster_images(similarity, message=True)\nCluster images\nCluster images based on their similarity scores with network clustering.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsimilarity\nnp.ndarray\nArray with shape (image_count, image_count) indicating the similarity between each pair of images.\nrequired\n\n\nmessage\nbool\nShould a message about potential false positives be printed to the console?\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nresults\nClusterResults\nObject of type pyster.ClusterResult. Integer labels for the cluster assignment of each image can be accessed with results.cluster_idx."
  },
  {
    "objectID": "api/sort.NetworkCluster.html#parameters",
    "href": "api/sort.NetworkCluster.html#parameters",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nSimilarity score threshold above which two images are considered to contain the same animal. Must lie between [0.0, 1.0]\n0.5"
  },
  {
    "objectID": "api/sort.NetworkCluster.html#notes",
    "href": "api/sort.NetworkCluster.html#notes",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "Network clustering works best with smaller datasets, say, around 1000 images."
  },
  {
    "objectID": "api/sort.NetworkCluster.html#examples",
    "href": "api/sort.NetworkCluster.html#examples",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import NetworkCluster\n&gt;&gt;&gt; from sklearn.metrics.pairwise import cosine_similarity\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; scores = cosine_similarity(feature_array)\n&gt;&gt;&gt; \n&gt;&gt;&gt; nc = NetworkCluster(match_threshold=0.5)\n&gt;&gt;&gt; results = nc.cluster_images(scores)\n&gt;&gt;&gt; len(np.unique(results.cluster_idx))\n2"
  },
  {
    "objectID": "api/sort.NetworkCluster.html#methods",
    "href": "api/sort.NetworkCluster.html#methods",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.NetworkCluster.cluster_images(similarity, message=True)\nCluster images\nCluster images based on their similarity scores with network clustering.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsimilarity\nnp.ndarray\nArray with shape (image_count, image_count) indicating the similarity between each pair of images.\nrequired\n\n\nmessage\nbool\nShould a message about potential false positives be printed to the console?\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nresults\nClusterResults\nObject of type pyster.ClusterResult. Integer labels for the cluster assignment of each image can be accessed with results.cluster_idx."
  },
  {
    "objectID": "api/sort.process_images.html",
    "href": "api/sort.process_images.html",
    "title": "sort.process_images",
    "section": "",
    "text": "sort.process_images\nsort.process_images(image_root, all_image_dir)\nCopy all images to a temporary directory and return encounter information"
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Extracting feature vectors from images\n\n\n\nextract.FeatureExtractor\nExtract features from images.\n\n\n\n\n\n\nClustering images by proposed IDs\n\n\n\nsort.HierarchicalCluster\nHierarchical clustering of images\n\n\nsort.NetworkCluster\nNetwork clustering of images\n\n\nsort.prep_images\nCopy all images to a flat directory and save a csv with encounter info.\n\n\nsort.sort_images\nSort images into subfolders by proposed ID then encounter.\n\n\nsort.load_features\nLoad previously extracted features.\n\n\n\n\n\n\nGrade images by distinctiveness\n\n\n\ngrade.rate_distinctiveness\nGrade images by their distinctiveness.\n\n\n\n\n\n\nFunctions to make sure PyTorch is working\n\n\n\nget_best_device\nSelect torch device based on expected performance.\n\n\nverify_pytorch\nVerify PyTorch installation and show device options."
  },
  {
    "objectID": "api/index.html#feature-extraction",
    "href": "api/index.html#feature-extraction",
    "title": "API Reference",
    "section": "",
    "text": "Extracting feature vectors from images\n\n\n\nextract.FeatureExtractor\nExtract features from images."
  },
  {
    "objectID": "api/index.html#sorting-and-clustering",
    "href": "api/index.html#sorting-and-clustering",
    "title": "API Reference",
    "section": "",
    "text": "Clustering images by proposed IDs\n\n\n\nsort.HierarchicalCluster\nHierarchical clustering of images\n\n\nsort.NetworkCluster\nNetwork clustering of images\n\n\nsort.prep_images\nCopy all images to a flat directory and save a csv with encounter info.\n\n\nsort.sort_images\nSort images into subfolders by proposed ID then encounter.\n\n\nsort.load_features\nLoad previously extracted features."
  },
  {
    "objectID": "api/index.html#grading",
    "href": "api/index.html#grading",
    "title": "API Reference",
    "section": "",
    "text": "Grade images by distinctiveness\n\n\n\ngrade.rate_distinctiveness\nGrade images by their distinctiveness."
  },
  {
    "objectID": "api/index.html#verifying-installation",
    "href": "api/index.html#verifying-installation",
    "title": "API Reference",
    "section": "",
    "text": "Functions to make sure PyTorch is working\n\n\n\nget_best_device\nSelect torch device based on expected performance.\n\n\nverify_pytorch\nVerify PyTorch installation and show device options."
  },
  {
    "objectID": "api/verify_pytorch.html",
    "href": "api/verify_pytorch.html",
    "title": "verify_pytorch",
    "section": "",
    "text": "verify_pytorch\nverify_pytorch()\nVerify PyTorch installation and show device options."
  },
  {
    "objectID": "api/sort.sort_images.html",
    "href": "api/sort.sort_images.html",
    "title": "sort.sort_images",
    "section": "",
    "text": "sort.sort_images(id_df, all_image_dir, output_dir)\nSort images into subfolders by proposed ID then encounter.\nCopy images from the flat all_image_dir into the output_dir, where the output_dir is now divided in subfolders by proposed ID then encounter.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid_df\npd.DataFrame\nPandas DataFrame with columns ['image', 'proposed_id', 'encounter'].\nrequired\n\n\nall_image_dir\nstr\nPath to flat directory with every image in the id_df.\nrequired\n\n\noutput_dir\nstr\nPath to new directory into which sort_images will copy files.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nCopies images to the output_dir.\n\n\n\n\n\n\nFor a complete working example with real images, see:\n\nTutorial"
  },
  {
    "objectID": "api/sort.sort_images.html#parameters",
    "href": "api/sort.sort_images.html#parameters",
    "title": "sort.sort_images",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nid_df\npd.DataFrame\nPandas DataFrame with columns ['image', 'proposed_id', 'encounter'].\nrequired\n\n\nall_image_dir\nstr\nPath to flat directory with every image in the id_df.\nrequired\n\n\noutput_dir\nstr\nPath to new directory into which sort_images will copy files.\nrequired"
  },
  {
    "objectID": "api/sort.sort_images.html#returns",
    "href": "api/sort.sort_images.html#returns",
    "title": "sort.sort_images",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nCopies images to the output_dir."
  },
  {
    "objectID": "api/sort.sort_images.html#examples",
    "href": "api/sort.sort_images.html#examples",
    "title": "sort.sort_images",
    "section": "",
    "text": "For a complete working example with real images, see:\n\nTutorial"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pyseter",
    "section": "",
    "text": "Pyseter is a Python package for processing images before photo-identification. Pyseter includes functions for:"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Pyseter",
    "section": "Introduction",
    "text": "Introduction\nPlease see the Tutorial for a longer form demonstration on working with Pyseter, including tips for R users."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Pyseter",
    "section": "Installation",
    "text": "Installation\nR users will also find a seperate page for installation instructions."
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Pyseter",
    "section": "Citation",
    "text": "Citation"
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "Python users",
    "section": "",
    "text": "This page has information on how to install for users who have some familiarity with Python. R users should see this page, which has more thorough instructions and introductions to Python concepts.\nUsers can use whatever package management utility they like, such as conda, venv, pixi, or uv. Both PyTorch and Pyseter can be installed with pip.",
    "crumbs": [
      "Install",
      "Python users"
    ]
  },
  {
    "objectID": "install.html#install-pytorch",
    "href": "install.html#install-pytorch",
    "title": "Python users",
    "section": "Install PyTorch",
    "text": "Install PyTorch\nInstalling PyTorch will allow users to extract features from images, i.e., identify individuals in images. Extracting features should be fast for users with an NVIDIA GPU, and reasonable for users with a Mac with Apple Silicon.\n\n\n\n\n\n\nWarning\n\n\n\nFor all other users, extracting features from images will be extremely slow.\n\n\nPyTorch installation can be a little finicky. Essentially, it depends on what operating system you’re using, and what version of CUDA you’re using. To install PyTorch, I recommend following these instructions.\n\nWindows users\nBelow is an example for Windows users. This relies on CUDA 12.8, which should work for most people.\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128\nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\nMac Users\nBelow is an example for Mac users. The Mac version of PyTorch relies on MPS acceleration, which is good but not at the level of CUDA.\npip3 install torch torchvision \nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\n\n\n\n\nWarning\n\n\n\nMac users will need to have an Apple Silicon processor, e.g., an M1 chip. Additionally, they will need at least 16 GB of memory (RAM) to use AnyDorsal.",
    "crumbs": [
      "Install",
      "Python users"
    ]
  },
  {
    "objectID": "install.html#install-pyseter",
    "href": "install.html#install-pyseter",
    "title": "Python users",
    "section": "Install Pyseter",
    "text": "Install Pyseter\nNow, install Pyseter from PyPI.\npip3 install pyseter\nNow you’re ready to go!",
    "crumbs": [
      "Install",
      "Python users"
    ]
  },
  {
    "objectID": "install.html#verify-the-installation",
    "href": "install.html#verify-the-installation",
    "title": "Python users",
    "section": "Verify the installation",
    "text": "Verify the installation\nVerify the Pyseter installation by running the following commands.\nimport pyseter\npyseter.verify_pytorch()\nIf you have access to an NVIDIA GPU, you should see something like\n✓ PyTorch 2.7.1+cu126 detected\n✓ CUDA GPU available: NVIDIA A30 MIG 2g.12gb",
    "crumbs": [
      "Install",
      "Python users"
    ]
  },
  {
    "objectID": "api/sort.load_features.html",
    "href": "api/sort.load_features.html",
    "title": "sort.load_features",
    "section": "",
    "text": "sort.load_features(feature_path)\nLoad previously extracted features.\nLoad features produced by the FeatureExtractor.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeature_path\nstr\nPath to .npy file\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nfilenames, feature_array: np.array, np.array\nThe filenames array correspond to each row of the feature_array, which has shape (image_count, feature_count)"
  },
  {
    "objectID": "api/sort.load_features.html#parameters",
    "href": "api/sort.load_features.html#parameters",
    "title": "sort.load_features",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfeature_path\nstr\nPath to .npy file\nrequired"
  },
  {
    "objectID": "api/sort.load_features.html#returns",
    "href": "api/sort.load_features.html#returns",
    "title": "sort.load_features",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nfilenames, feature_array: np.array, np.array\nThe filenames array correspond to each row of the feature_array, which has shape (image_count, feature_count)"
  },
  {
    "objectID": "api/grade.rate_distinctiveness.html",
    "href": "api/grade.rate_distinctiveness.html",
    "title": "grade.rate_distinctiveness",
    "section": "",
    "text": "grade.rate_distinctiveness(features, match_threshold=0.6)\nGrade images by their distinctiveness.\nCompute the embedding recognizability score (ERS) for each image in the feature array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nNumPy array of shape (image_count, feature_count) containing feature vectors for each image\nrequired\n\n\nmatch_threshold\nfloat\nThe threshold above which two images are considered a match. Must be between (0, 1)\n0.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nEmbedding recognizability score (ERS), a measure of distinctiveness, for every image in the dataset."
  },
  {
    "objectID": "api/grade.rate_distinctiveness.html#parameters",
    "href": "api/grade.rate_distinctiveness.html#parameters",
    "title": "grade.rate_distinctiveness",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nNumPy array of shape (image_count, feature_count) containing feature vectors for each image\nrequired\n\n\nmatch_threshold\nfloat\nThe threshold above which two images are considered a match. Must be between (0, 1)\n0.6"
  },
  {
    "objectID": "api/grade.rate_distinctiveness.html#returns",
    "href": "api/grade.rate_distinctiveness.html#returns",
    "title": "grade.rate_distinctiveness",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.array\nEmbedding recognizability score (ERS), a measure of distinctiveness, for every image in the dataset."
  },
  {
    "objectID": "api/get_best_device.html",
    "href": "api/get_best_device.html",
    "title": "get_best_device",
    "section": "",
    "text": "get_best_device\nget_best_device()\nSelect torch device based on expected performance."
  },
  {
    "objectID": "api/extract.FeatureExtractor.html",
    "href": "api/extract.FeatureExtractor.html",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "extract.FeatureExtractor(batch_size, device=None, stochastic=False)\nExtract features from images.\nExtract feature vectors for individual identification from images. Currently, FeatureExtractor only includes the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbatch_size\nint\nThe number of images the GPU will process.\nrequired\n\n\ndevice\n(None, cuda, mps, cpu)\nDevice with which to extract the features. By default, the best device is chosen for the user (cuda, mps, or cpu)\nNone\n\n\nstochastic\nboolean\nCurrently unused.\nFalse\n\n\n\n\n\n\nFor a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.extract import FeatureExtractor\n\n# Initialize extractor\nextractor = FeatureExtractor(batch_size=16)\n\n# Extract features from all images\nfeatures = extractor.extract('path/to/images/')\n\n# Access individual image features\nimg_features = features['my_image.jpg']\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nextract\nExtracts features from images.\n\n\n\n\n\nextract.FeatureExtractor.extract(image_dir, bbox_csv=None)\nExtracts features from images.\nExtracts feature vectors for every image in a directory with the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/extract.FeatureExtractor.html#parameters",
    "href": "api/extract.FeatureExtractor.html#parameters",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbatch_size\nint\nThe number of images the GPU will process.\nrequired\n\n\ndevice\n(None, cuda, mps, cpu)\nDevice with which to extract the features. By default, the best device is chosen for the user (cuda, mps, or cpu)\nNone\n\n\nstochastic\nboolean\nCurrently unused.\nFalse"
  },
  {
    "objectID": "api/extract.FeatureExtractor.html#examples",
    "href": "api/extract.FeatureExtractor.html#examples",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "For a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.extract import FeatureExtractor\n\n# Initialize extractor\nextractor = FeatureExtractor(batch_size=16)\n\n# Extract features from all images\nfeatures = extractor.extract('path/to/images/')\n\n# Access individual image features\nimg_features = features['my_image.jpg']"
  },
  {
    "objectID": "api/extract.FeatureExtractor.html#methods",
    "href": "api/extract.FeatureExtractor.html#methods",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nextract\nExtracts features from images.\n\n\n\n\n\nextract.FeatureExtractor.extract(image_dir, bbox_csv=None)\nExtracts features from images.\nExtracts feature vectors for every image in a directory with the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html",
    "href": "api/extract.FeatureExtractor.extract.html",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "extract.FeatureExtractor.extract(image_dir, bbox_csv=None)\nExtracts features from images.\nExtracts feature vectors for every image in a directory with the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html#parameters",
    "href": "api/extract.FeatureExtractor.extract.html#parameters",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone"
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html#returns",
    "href": "api/extract.FeatureExtractor.extract.html#returns",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504."
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html#raises",
    "href": "api/extract.FeatureExtractor.extract.html#raises",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/sort.prep_images.html",
    "href": "api/sort.prep_images.html",
    "title": "sort.prep_images",
    "section": "",
    "text": "sort.prep_images(image_dir, all_image_dir)\nCopy all images to a flat directory and save a csv with encounter info.\nSome users may have their image directory structured such that each image is in a subfolder by encounter, e.g., original_images/enc1/img1.jpg. The FeatureExtractor in pyseter.extract prefers flat directories. prep_images flattens the original_images directory by copying every image to all_image_dir, then saves a csv with encounter information to the working directory.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nPath to directory containing images.\nrequired\n\n\nall_image_dir\nstr\nPath to new directory where user wants to copy all their images.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nSaves images to the all_image_dir and the encounter information to the csv in the working_dir.\n\n\n\n\n\n\nFor a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.sort import prep_images\nprep_images('working_dir/original_images/')"
  },
  {
    "objectID": "api/sort.prep_images.html#parameters",
    "href": "api/sort.prep_images.html#parameters",
    "title": "sort.prep_images",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nPath to directory containing images.\nrequired\n\n\nall_image_dir\nstr\nPath to new directory where user wants to copy all their images.\nrequired"
  },
  {
    "objectID": "api/sort.prep_images.html#returns",
    "href": "api/sort.prep_images.html#returns",
    "title": "sort.prep_images",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nSaves images to the all_image_dir and the encounter information to the csv in the working_dir."
  },
  {
    "objectID": "api/sort.prep_images.html#examples",
    "href": "api/sort.prep_images.html#examples",
    "title": "sort.prep_images",
    "section": "",
    "text": "For a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.sort import prep_images\nprep_images('working_dir/original_images/')"
  },
  {
    "objectID": "install-r.html",
    "href": "install-r.html",
    "title": "R users who are new to Python",
    "section": "",
    "text": "We expect that most people using Pyseter will be familiar with R, and completely new to Python. This raises the question: why release Pyseter as a Python package? The answer is that Python is much more suited to deep learning than R, and most of the new developments in automated photo-identification rely on deep learning.\nAs such, Pyseter users will have to familiarize themselves with a few Python concepts, such as conda and Jupyter, before getting started.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-conda",
    "href": "install-r.html#install-conda",
    "title": "R users who are new to Python",
    "section": "Install conda",
    "text": "Install conda\nConda is an important tool for managing packages in Python. While R, for the most part, handles packages for you behind the scenes, Python requires a more hands on approach.\nTo get started, we first need to install a package manager called conda. There are many forms of conda, with Anaconda being the most popular. For several reasons, we prefer another form of conda called Miniforge.\n\nDownload and install Miniforge (a form of conda)\n\nAfter installing, you can verify your installation by opening the command line interface (CLI), which will depend on your operating system. Are you on Windows? Open the “miniforge prompt” in your start menu. Are you on Mac? Open the Terminal application. Then, type the following command into the CLI and hit return.\nconda --version\nYou should see something like conda 25.5.1. Of course, Anaconda, miniconda, mamba, or any other form of conda will work too.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#create-a-new-environment",
    "href": "install-r.html#create-a-new-environment",
    "title": "R users who are new to Python",
    "section": "Create a new environment",
    "text": "Create a new environment\nThen, you’ll create an environment for the package will live in. Environments are walled off areas where we can install packages. This allows you to have multiple versions of the same package installed on your machine, which can help prevent conflicts.\nEnter the following two commands into the CLI:\nconda create -n pyseter_env\nconda activate pyseter_env\nHere, I name (hence the -n) the environment pyseter_env, but you can call it anything you like!\nNow your environment is ready to go! Try installing your first package, pip. Pip is another way of installing Python packages, and will be helpful for installing PyTorch and Pyseter (see below). To do so, enter the following command into the CLI.\nconda install pip -y\nIn this case, we include the -y argument so we don’t have to immediately answer yes to the next question. Once this is working, you’re ready to proceed to the next section.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-pytorch",
    "href": "install-r.html#install-pytorch",
    "title": "R users who are new to Python",
    "section": "Install PyTorch",
    "text": "Install PyTorch\nInstalling PyTorch will allow users to extract features from images, i.e., identify individuals in images. Extracting features should be fast for users with an NVIDIA GPU, and reasonable for users with a Mac with Apple Silicon.\n\n\n\n\n\n\nWarning\n\n\n\nFor all other users, extracting features from images will be extremely slow.\n\n\nPyTorch installation can be a little finicky. Essentially, it depends on what operating system you’re using, and what version of CUDA you’re using. CUDA is the technology that turns your NVIDIA GPU into a deep learning machine. To install PyTorch, I recommend following these instructions.\n\nWindows users\nBelow is an example for Windows users. This relies on CUDA 12.8, which should work for most people. If you haven’t already, open the CLI (e.g., the miniforge prompt). Then activate your environment before installing.\nconda activate pyseter_env\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128\nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\nMac users\nBelow is an example for Mac users. The Mac version of PyTorch relies on MPS acceleration, which is good but not at the level of CUDA. If you haven’t already, open you’re command line interface (e.g., the miniforge prompt). Then activate your environment before installing.\nconda activate pyseter_env\npip3 install torch torchvision \nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\n\n\n\n\nWarning\n\n\n\nMac users will need to have an Apple Silicon processor, e.g., an M1 chip. Additionally, they will need at least 16 GB of memory (RAM) to use AnyDorsal.\n\n\n\n\nLinux users\nYou run Linux, but you’ve never used Python?",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-pyseter",
    "href": "install-r.html#install-pyseter",
    "title": "R users who are new to Python",
    "section": "Install Pyseter",
    "text": "Install Pyseter\nNow, install Pyseter. If you haven’t already, activate your environment before installing.\nconda activate pyseter_env\npip3 install pyseter\nNow you’re ready to go!",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-vs-code-or-positron",
    "href": "install-r.html#install-vs-code-or-positron",
    "title": "R users who are new to Python",
    "section": "Install VS Code or Positron",
    "text": "Install VS Code or Positron\nMost users will interact with Pyseter via a Jupyter Notebook. There are many methods for opening, editing, running, and saving Jupyter Notebooks. We are personally biased towards VS Code. Alternatively, R users might also try out Positron. The team at Posit (formerly, R Studio) developed Positron from the open source version of VS Code, but with R users in mind. As such, it might be a nice hybrid option. That said, we have found Positron to throw confusing errors and have found VS Code to be more stable.\n\nDownload and install VS Code\n\nOpen VS Code, then click “File -&gt; Open Folder”. Navigate to wherever you’d like to work, then click “New Folder.” You can call this folder something like “learn-pyseter” or “pyseter-jobs”. Open the new folder. Click “File -&gt; New File” then select Jupyter Notebook. Click “Select Kernel” in the top right corner, select “Python environments” and then “pyseter_env”, or whatever you named your environment. For more information, check out this great overview of using Jupyter Notebooks in VS Code.\nNow you’re ready to proceed to the next section.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#verify-the-installation",
    "href": "install-r.html#verify-the-installation",
    "title": "R users who are new to Python",
    "section": "Verify the installation",
    "text": "Verify the installation\nVerify the Pyseter installation by running the following cell in your notebook.\nimport pyseter\npyseter.verify_pytorch()\nIf you’re on a windows computer with an NVIDIA GPU, you should see something like\n✓ PyTorch 2.7.1+cu126 detected\n✓ CUDA GPU available: NVIDIA A30 MIG 2g.12gb\nOnce this is working, you’re ready to check out the “General Overview” notebook in the examples folder of this repository!",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#anydorsal-weights",
    "href": "install-r.html#anydorsal-weights",
    "title": "R users who are new to Python",
    "section": "AnyDorsal weights",
    "text": "AnyDorsal weights\nPyseter relies on the AnyDorsal algorithm to extract features from images. The first time you use the FeatureExtractor, Pyseter will download the AnyDorsal weights from Hugging Face. The weights take up roughly 4.5 GB. As such, to use the FeatureExtractor, users must have enough storage space to accommodate the weights.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  }
]