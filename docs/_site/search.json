[
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Introduction to pyseter",
    "section": "",
    "text": "Pyseter is an Python package for sorting images by an automatically generated ID. The main functions of Pyseter are:\nThis notebook will walk you through each major function. First, let’s make sure that Pyseter is properly installed, and that it can access Pytorch.\nimport pyseter\n\npyseter.verify_pytorch()\n\n✓ PyTorch 2.7.1+cu126 detected\n✓ CUDA GPU available: NVIDIA RTX A4000\nIf you’re on a Mac, you should see something like\nPlease note, however, that AnyDorsal consumes quite a bit of memory. As such, only Apple Silicon devices with 16 GB or more of memory will work. Ideally, future versions of Pyseter will use a smaller model.\nIf neither Apple Silicon or an NVIDIA GPU are available, you will see a message like this.\nPackages in Python tend to be subdivided into modules based on their functions. In Pyseter, the sort module contains functions for sorting files, including other forms of file management."
  },
  {
    "objectID": "tutorial.html#optional-folder-management",
    "href": "tutorial.html#optional-folder-management",
    "title": "Introduction to pyseter",
    "section": "Optional: Folder management",
    "text": "Optional: Folder management\nThe main purpose of Pyseter is organizing images into folders. To do keep things clean and tidy, we recommend establishing a working directory with a subfolder, e.g., called, all images, that contains every image you want to be sorted (see below for a different case). Optionally, you might want to have a .csv with encounter information in the working directory. This .csv would contain two columns: one for the image name, i.e., every image in all images, and another for the encounter. As such, the working directory would look like this.\nworking directory\n├── encounter_info.csv\n├── all images\n│   └──00cef32dc62b0f.jpg\n│   └──3ecc025ea6f9bf.jpg\n│   └──9f18762a48696b.jpg\n│   └──36f78517a512dd.jpg\n│   └──470d524b4d5303.jpg\n       ...\n│   └──4511c9e5cb7acb.jpg\nSometimes, you might have your images organized into subfolders by encounter.\nworking_dir\n└── original_images\n    ├── SL_HI_006_20220616 (CROPPED)\n    │   ├── 2022-06-16_CLD500_CL_006.JPG\n    │   ├── 2022-06-16_CLD500_CL_007.JPG\n    │   ├── 2022-06-16_CLD500_CL_008.JPG\n    │   ├── 2022-06-16_CLD500_CL_021.JPG\n    │   ├── 2022-06-16_CLD500_CL_042.JPG\n...\n    ├── SL_HI_007_20220616 (CROPPED)\n    │   ├── 2022-06-16_CLD500_CL_346.JPG\n    │   ├── 2022-06-16_CLD500_CL_347.JPG\n    │   ├── 2022-06-16_CLD500_CL_371.JPG\n    │   ├── 2022-06-16_CLD500_CL_372.JPG\nIn this case, you might want to accomplish two tasks: move all these images to one folder, e.g., all_images, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The prep_images() function does just that.\n\nfrom pyseter.sort import prep_images\n\n# various directories we'll be working with\nworking_dir = '/home/pattonp/koa_scratch/id_data/working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new directory containing every image\nimage_dir = working_dir + '/all_images'\n\n# copy images to a single folder, then \nprep_images(original_image_dir, all_image_dir=image_dir)\n\nCopied 1230 images to: /home/pattonp/koa_scratch/id_data/working_dir/all_images\nSaved encounter information to: /home/pattonp/koa_scratch/id_data/working_dir/encounter_info.csv\n\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nIn python, you can concatenate strings with the + operator. This is equivalent to paste0(working_dir, '/original_images') in R.\n\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nPackages in Python tend to be subdivided into modules based on their functions. In Pyseter, the sort module contains functions for sorting files, including other forms of file management."
  },
  {
    "objectID": "tutorial.html#extracting-features",
    "href": "tutorial.html#extracting-features",
    "title": "Introduction to pyseter",
    "section": "1. Extracting features",
    "text": "1. Extracting features\nPyseter identifies individuals by extracting feature vectors from images. Feature vectors summarize three-dimensional images into one-dimensional vectors that are useful for the task at hand, in this case, individual identification.\nPyseter extracts feature vectors with AnyDorsal, an algorithm for identifying whales and dolphins of many species. AnyDorsal is the same dorsal identification algorithm that’s included with Happywhale (although not to be confused with their humpback whale fluke ID algorithm).\nBefore we extract the feature vectors, let’s first create a subfolder within our working directory to save them in. This isn’t necessary, yet keeps things tidy.\n\nimport os\n\n# in case you want to save the features after extracting them \nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nThe module, os, is part of Python’s standard library. People often refer to R and its standard libraries as “base R.” Base R includes the stats library, which provides the function rnorm. The os module has many functions for tinkering with your operating system.\n\n\nWe will extract features with the FeatureExtractor class. To do so, we first need to initialize the class. This sets up important parameters, such as the batch_size, which is the number of images that will be processed in parallel. Larger batches should run faster, although your mileage may vary. If you specify too large of a batch, you may encounter an OutOfMemoryError (see below for an example). If you encounter this error, try specifying a larger batch size. If you encounter this error with a very small batch size (say, 2), you may need to resize your images. You can do this manually by reducing the file size in an image editing software, or with Python\nOutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 5.81 GiB of which 9.06 MiB is free. Including non-PyTorch memory, this process has 5.76 GiB memory in use. Of the allocated memory 5.64 GiB is allocated by PyTorch, and 50.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nPython error messages are comically long, putting CVS receipts to shame. This is because they show the entire traceback, i.e., this error caused this error caused this error, etc. To quickly diagnose the problem, scroll to the bottom of the message. Then, you can further dissect it by scrolling up.\n\n\n\nfrom pyseter.extract import FeatureExtractor\n\n# specify the configuration for the extractor \nfe = FeatureExtractor(batch_size=4,)\n\nUsing device: cuda (NVIDIA RTX A4000)\n\n\nOnce we’ve initialize the class, we can use its associated methods (functions). In this case, the only one we are interested in is extract(), which extracts a feature vector for every image in a specified directory. This can take several minutes, so we typically save the results afterwards.\n\n\n\n\n\n\nTipR user tip\n\n\n\nClasses and methods also exist in R, but operate more behind the scenes. For example, x &lt;- data.frame() initializes an object of class data.frame, and summary(x) calls the summary method for data.frames. Python makes this relationship more explicit. For example, the equivalent (although nonsensical) Python code would be x = data.frame() and x.summary().\n\n\n\nimport numpy as np\n\n# extract the features for the input directory then save them\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves them as an numpy array\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\nLoading model...\n\n\n/home/pattonp/.conda/envs/pyseter/lib/python3.13/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_l2_ns to current tf_efficientnet_l2.ns_jft_in1k.\n  model = create_fn(\n\n\nWarning: Missing keys when loading pretrained weights: ['head.weight']\nWarning: Unexpected keys when loading pretrained weights: ['head.fc.weight']\nExtracting features...\n\n\n100%|██████████| 308/308 [01:48&lt;00:00,  2.84it/s]\n\n\nThe object features is a dictionary, whose keys are the filenames and whose values are the feature vectors associated with each filename. This helps ensure that each image is associated with the correct feature vector. Nevertheless, it can be easier to work with actual numpy arrays. To do so, convert the keys to a list, then to a numpy array.\n\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nThe R objects that Python’s dictionary most resemble is the named vector or the list. Like a list, dictionaries can hold different data types. Unlike a list, dictionaries have no order, and therefore cannot be integer indexed.\n\n\nIf you’ve already extracted and saved features, you can load them with the code below.\n\n# import numpy as np\n# out_path = feature_dir + '/features.npy'\n# features = np.load(out_path, allow_pickle=True).item()\n# filenames = np.array(list(features.keys()))\n# feature_array = np.array(list(features.values()))"
  },
  {
    "objectID": "tutorial.html#clustering-images-by-proposed-id",
    "href": "tutorial.html#clustering-images-by-proposed-id",
    "title": "Introduction to pyseter",
    "section": "2. Clustering images by proposed ID",
    "text": "2. Clustering images by proposed ID\nPyseter comes with two algorithm’s for clustering images by proposed ID. Network clustering works better for small datasets. To use network clustering, we first need to compute the similarity scores between each pair of images.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarity_scores = cosine_similarity(feature_array)\n\nThis indicates how similar the individuals in each image are.\nNext, we can perform the clustering. Each cluster represents a proposed ID. To access the “cluster labels” (these are just integers representing the cluster), we can access the cluster_idx attribute, i.e, results.cluster_idx.\n\nfrom pyseter.sort import NetworkCluster, report_cluster_results\n\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n\nFollowing clusters may contain false positives:\n['ID_0013', 'ID_0040', 'ID_0061', 'ID_0079', 'ID_0086', 'ID_0110']\n\n\nNetwork clustering has one major hyperparameter, namely, the match_threshold, which indicates whether two images should be grouped within a cluster. That is, if the similarity score between two images is above a certain threshold, we cluster them into a proposed ID. High thresholds mean that few images will be clustered together, creating many clusters. Very low thresholds mean that many images will be clustered together, creating few clusters. report_cluster_results produces a quick and dirty summary of the number of clusters created, and the size of the largest cluster (i.e., the number of images associated to the most photographed individual). This is a quick sanity check.\n\nnetwork_idx = results.cluster_idx\nreport_cluster_results(network_idx)\n\nFound 245 clusters.\nLargest cluster has 30 images.\n\n\nYou’ll also note that nc.cluster_images() warns that some clusters may contain “false positives.” False positive matches occur when two separate individuals fall under the same proposed ID. We can diagnose possible false positives by evaluating the network. In this case, the network consists of nodes (images) and edges, which represent connections between images. Two images are connected when they have a similarity score above the threshold. A blob of connected nodes (i.e., connected components) represents a proposed ID.\nSometimes, the connected components look less like a blob and more like a barbell, where two sets of images have many connections amongst each other, yet these two blobs are only connected by one link. We suspect that such clusters contain false positives, i.e., two sets of images for two individuals connected by one spurious link. We can plot the networks of the suspicious clusters with results.plot_suspicious().\n\nresults.plot_suspicious()\n\n\n\n\n\n\n\n\nID_0033 clearly has a barbell shape, suggesting that this cluster consists of images for two individuals linked together by one spurious connection.ID_0013 also seems dubious, and ID_0060 might consist of four separate individuals.\nUsers can deal with suspicious clusters in several ways. First, raising the match threshold should reduce the number of false positive matches. That said, this will increase the false negative rate, whereby images of one individual are spread across multiple proposed IDs. Alternatively, users can manually inspect the images within the clusters and, if need be, divide up the images.\nAs the number of images being clustered grows, the overall false positive rate also grows (this is analogous the multiple comparison problem in statistics). At some point,the network matching becomes untenable; all but the highest match thresholds would produce too many false positives to be useful.\nFor these cases, there is HierarchicalCluster, which relies on the Hierarchical Agglomerative Clustering algorithm provided by the popular machine learning package, scikit-learn. Note that HierarchicalCluster will run noticeably slower than the NetworkCluster.\n\nfrom pyseter.sort import HierarchicalCluster, format_ids\n\n# initialize the object then cluster away! \nhc = HierarchicalCluster(match_threshold=0.5)\nhac_idx = hc.cluster_images(feature_array)\n\nThe HierarchicalCluster results object is much simpler, in that it just returns the cluster indices for each image. We can make these labels a little prettier with the format_ids function.\n\nhac_labels = format_ids(hac_idx)\nprint(hac_labels[:5])\n\n# quick summary of the clustering results\nreport_cluster_results(hac_labels)\n\n['ID-0092', 'ID-0159', 'ID-0075', 'ID-0000', 'ID-0034']\nFound 306 clusters.\nLargest cluster has 19 images.\n\n\nHierarchicalCluster is useful for large datasets, yet will be more prone to false negative errors. In this example, it found 60 more clusters (proposed IDs) than the network matching, which may be dubious. Users will have to decide for themselves how to balance false positive versus false negative matches. For example, we recommend that users preprocess their images with Pyseter, then identify animals in the pre-processed images manually or with Happywhale. This second round of identification should help clean up false negative matches. As such, users following this approach might be more averse to false positive errors in the first stage."
  },
  {
    "objectID": "tutorial.html#sorting-images-by-proposed-id",
    "href": "tutorial.html#sorting-images-by-proposed-id",
    "title": "Introduction to pyseter",
    "section": "3. Sorting images by proposed ID",
    "text": "3. Sorting images by proposed ID\nOnce we’ve identified individuals, we can sort them into folder by proposed ID and encounter. To do so, we need to create a pandas DataFrame that indicates the proposed ID and encounter for each filename. Recall that we created the encounter_info.csv with the prep_images() function above. For this sort, we’ll use the NetworkCluster results\n\nimport pandas as pd\n\n# create a dataframe proposed id and encounter for each image\nnetwork_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})\nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nnetwork_df = network_df.merge(encounter_info)\n\nnetwork_df.head()\n\n\n\n\n\n\n\n\nimage\nproposed_id\nencounter\n\n\n\n\n0\n2022-06-25_CLD500_CL_735.jpg\nID-0000\nSL_HI_016_20220625 (CROPPED)\n\n\n1\n2021-11-17_Hilo_AP_2_UNK_0330.jpg\nID-0001\nSL_Hilo_13_20211117 (CROPPED)\n\n\n2\n2022-06-20_CLD500_CL_1166.jpg\nID-0002\nSL_HI_010_20220620 (CROPPED)\n\n\n3\n2022-06-16_D750_ANNM_062.JPG\nID-0003\nSL_HI_006_20220616 (CROPPED)\n\n\n4\n2022-06-20_CLD500_CL_453.jpg\nID-0004\nSL_HI_010_20220620 (CROPPED)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningFormatting the ID DataFrame\n\n\n\nThe ID DataFrame must have columns named image, proposed_id, and encounter. Otherwise sort_images will not work. Luckily, changing column names in pandas is straightforward.\nnetwork_df.columns = ['image', 'proposed_id', 'encounter']\n\n\n\n\n\n\n\n\nTipR user tip\n\n\n\nPandas is a package for managing dataframes, and is a straight knock-off of R’s data.frame. In my experience, most R users lose patience not with Python, but with pandas. Pandas is just similar enough to R that one should be able to directly port R ideas over. However, key differences between pandas and R prevent this, causing immense frustration (especially for yours truly!)\n\n\nTo sort the images, we need to specify an output directory, then run the sort_images function.\n\nfrom pyseter.sort import sort_images\n\n# make an output directory \nsorted_dir = working_dir + '/sorted_images'\nos.makedirs(sorted_dir, exist_ok=True)\n\n# sort the images into folders based on proposed id\nsort_images(network_df, all_image_dir=image_dir, output_dir=sorted_dir)\n\nSorted 1230 images into 335 folders.\n\n\nWe can check to see that this worked by plotting a grid of images with matplotlib.\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# get the files associated with the first ID\nfirst_id_dir = sorted_dir + '/ID-0000'\nthis_encounter = os.listdir(first_id_dir)[0]\nencounter_dir = first_id_dir + '/' + this_encounter\nencounter_files = os.listdir(encounter_dir)\n\n# plot a grid of images \nfig, axes = plt.subplots(3, 3, tight_layout=True)\nfor i, filename in enumerate(encounter_files[:9]):\n    path = encounter_dir + '/' + filename\n    image = Image.open(path)\n    axes.flat[i].imshow(image)\n    axes.flat[i].axis('off')"
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html",
    "href": "api/sort.HierarchicalCluster.html",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "sort.HierarchicalCluster(match_threshold=0.5)\nHierarchical clustering of images\nCluster images with the hierarchical agglomerative clustering (HAC) algorithm from scikit-learn.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nThreshold dictating how closely knit clusters should be. Must be between zero and one.\n0.5\n\n\n\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import HierarchicalCluster\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; \n&gt;&gt;&gt; hac = HierarchicalCluster(match_threshold=0.5)\n&gt;&gt;&gt; cluster_indices = hac.cluster_images(feature_array)\n&gt;&gt;&gt; len(np.unique(cluster_indices))\n2\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmatch_threshold\n\nThreshold indicating how closely knit clusters should be.\n\n\n\n\n\n\nHierarchicalCluster works best for larger datasets, say, over 1000 images. HierarchicalCluster may be prone to false negative errors.\nHierarchicalCluster uses the version of HAC with a distance threshold specified–i.e., an unknown number of clusters–complete linkage, and cosine as the distance metric.\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.HierarchicalCluster.cluster_images(features)\nCluster images\nCluster feature vectors according to their cosine distance from one another.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nArray with shape (image_count, feature_count) containing the feature vector for each image.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nNumPy array containing integer labels for each cluster."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#parameters",
    "href": "api/sort.HierarchicalCluster.html#parameters",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nThreshold dictating how closely knit clusters should be. Must be between zero and one.\n0.5"
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#examples",
    "href": "api/sort.HierarchicalCluster.html#examples",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import HierarchicalCluster\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; \n&gt;&gt;&gt; hac = HierarchicalCluster(match_threshold=0.5)\n&gt;&gt;&gt; cluster_indices = hac.cluster_images(feature_array)\n&gt;&gt;&gt; len(np.unique(cluster_indices))\n2"
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#attributes",
    "href": "api/sort.HierarchicalCluster.html#attributes",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nmatch_threshold\n\nThreshold indicating how closely knit clusters should be."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#notes",
    "href": "api/sort.HierarchicalCluster.html#notes",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "HierarchicalCluster works best for larger datasets, say, over 1000 images. HierarchicalCluster may be prone to false negative errors.\nHierarchicalCluster uses the version of HAC with a distance threshold specified–i.e., an unknown number of clusters–complete linkage, and cosine as the distance metric."
  },
  {
    "objectID": "api/sort.HierarchicalCluster.html#methods",
    "href": "api/sort.HierarchicalCluster.html#methods",
    "title": "sort.HierarchicalCluster",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.HierarchicalCluster.cluster_images(features)\nCluster images\nCluster feature vectors according to their cosine distance from one another.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nArray with shape (image_count, feature_count) containing the feature vector for each image.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nNumPy array containing integer labels for each cluster."
  },
  {
    "objectID": "api/sort.NetworkCluster.html",
    "href": "api/sort.NetworkCluster.html",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "sort.NetworkCluster(match_threshold=0.5)\nNetwork clustering of images\nCluster images with a simple network, where images are nodes and edges are images whose similarity score is above the match_threshold\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nSimilarity score threshold above which two images are considered to contain the same animal. Must lie between [0.0, 1.0]\n0.5\n\n\n\n\n\n\nNetwork clustering works best with smaller datasets, say, around 1000 images.\n\n\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import NetworkCluster\n&gt;&gt;&gt; from sklearn.metrics.pairwise import cosine_similarity\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; scores = cosine_similarity(feature_array)\n&gt;&gt;&gt; \n&gt;&gt;&gt; nc = NetworkCluster(match_threshold=0.5)\n&gt;&gt;&gt; results = nc.cluster_images(scores)\n&gt;&gt;&gt; len(np.unique(results.cluster_idx))\n2\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.NetworkCluster.cluster_images(similarity, message=True)\nCluster images\nCluster images based on their similarity scores with network clustering.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsimilarity\nnp.ndarray\nArray with shape (image_count, image_count) indicating the similarity between each pair of images.\nrequired\n\n\nmessage\nbool\nShould a message about potential false positives be printed to the console?\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nresults\nClusterResults\nObject of type pyster.ClusterResult. Integer labels for the cluster assignment of each image can be accessed with results.cluster_idx."
  },
  {
    "objectID": "api/sort.NetworkCluster.html#parameters",
    "href": "api/sort.NetworkCluster.html#parameters",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmatch_threshold\nfloat\nSimilarity score threshold above which two images are considered to contain the same animal. Must lie between [0.0, 1.0]\n0.5"
  },
  {
    "objectID": "api/sort.NetworkCluster.html#notes",
    "href": "api/sort.NetworkCluster.html#notes",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "Network clustering works best with smaller datasets, say, around 1000 images."
  },
  {
    "objectID": "api/sort.NetworkCluster.html#examples",
    "href": "api/sort.NetworkCluster.html#examples",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyseter.sort import NetworkCluster\n&gt;&gt;&gt; from sklearn.metrics.pairwise import cosine_similarity\n&gt;&gt;&gt; from numpy.random import normal\n&gt;&gt;&gt; \n&gt;&gt;&gt; cluster1 = normal(-200, 1, size=(15, 5504))\n&gt;&gt;&gt; cluster2 = normal(200, 1, size=(5, 5504))\n&gt;&gt;&gt; feature_array = np.vstack([cluster1, cluster2])\n&gt;&gt;&gt; scores = cosine_similarity(feature_array)\n&gt;&gt;&gt; \n&gt;&gt;&gt; nc = NetworkCluster(match_threshold=0.5)\n&gt;&gt;&gt; results = nc.cluster_images(scores)\n&gt;&gt;&gt; len(np.unique(results.cluster_idx))\n2"
  },
  {
    "objectID": "api/sort.NetworkCluster.html#methods",
    "href": "api/sort.NetworkCluster.html#methods",
    "title": "sort.NetworkCluster",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncluster_images\nCluster images\n\n\n\n\n\nsort.NetworkCluster.cluster_images(similarity, message=True)\nCluster images\nCluster images based on their similarity scores with network clustering.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsimilarity\nnp.ndarray\nArray with shape (image_count, image_count) indicating the similarity between each pair of images.\nrequired\n\n\nmessage\nbool\nShould a message about potential false positives be printed to the console?\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nresults\nClusterResults\nObject of type pyster.ClusterResult. Integer labels for the cluster assignment of each image can be accessed with results.cluster_idx."
  },
  {
    "objectID": "api/sort.process_images.html",
    "href": "api/sort.process_images.html",
    "title": "sort.process_images",
    "section": "",
    "text": "sort.process_images\nsort.process_images(image_root, all_image_dir)\nCopy all images to a temporary directory and return encounter information"
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "Extracting feature vectors from images\n\n\n\nextract.FeatureExtractor\nExtract features from images.\n\n\n\n\n\n\nClustering images by proposed IDs\n\n\n\nsort.HierarchicalCluster\nHierarchical clustering of images\n\n\nsort.NetworkCluster\nNetwork clustering of images\n\n\nsort.ClusterResults\nStoring NetworkCluster results.\n\n\nsort.prep_images\nCopy all images to a flat directory and save a csv with encounter info.\n\n\nsort.sort_images\nSort images into subfolders by proposed ID then encounter.\n\n\nsort.load_features\nLoad previously extracted features.\n\n\n\n\n\n\nGrade images by distinctiveness\n\n\n\ngrade.rate_distinctiveness\nGrade images by their distinctiveness.\n\n\n\n\n\n\nFunctions to make sure PyTorch is working\n\n\n\nget_best_device\nSelect torch device based on expected performance.\n\n\nverify_pytorch\nVerify PyTorch installation and show device options."
  },
  {
    "objectID": "api/index.html#feature-extraction",
    "href": "api/index.html#feature-extraction",
    "title": "API Reference",
    "section": "",
    "text": "Extracting feature vectors from images\n\n\n\nextract.FeatureExtractor\nExtract features from images."
  },
  {
    "objectID": "api/index.html#sorting-and-clustering",
    "href": "api/index.html#sorting-and-clustering",
    "title": "API Reference",
    "section": "",
    "text": "Clustering images by proposed IDs\n\n\n\nsort.HierarchicalCluster\nHierarchical clustering of images\n\n\nsort.NetworkCluster\nNetwork clustering of images\n\n\nsort.ClusterResults\nStoring NetworkCluster results.\n\n\nsort.prep_images\nCopy all images to a flat directory and save a csv with encounter info.\n\n\nsort.sort_images\nSort images into subfolders by proposed ID then encounter.\n\n\nsort.load_features\nLoad previously extracted features."
  },
  {
    "objectID": "api/index.html#grading",
    "href": "api/index.html#grading",
    "title": "API Reference",
    "section": "",
    "text": "Grade images by distinctiveness\n\n\n\ngrade.rate_distinctiveness\nGrade images by their distinctiveness."
  },
  {
    "objectID": "api/index.html#verifying-installation",
    "href": "api/index.html#verifying-installation",
    "title": "API Reference",
    "section": "",
    "text": "Functions to make sure PyTorch is working\n\n\n\nget_best_device\nSelect torch device based on expected performance.\n\n\nverify_pytorch\nVerify PyTorch installation and show device options."
  },
  {
    "objectID": "api/verify_pytorch.html",
    "href": "api/verify_pytorch.html",
    "title": "verify_pytorch",
    "section": "",
    "text": "verify_pytorch\nverify_pytorch()\nVerify PyTorch installation and show device options."
  },
  {
    "objectID": "api/grade.rate_distinctiveness.html",
    "href": "api/grade.rate_distinctiveness.html",
    "title": "grade.rate_distinctiveness",
    "section": "",
    "text": "grade.rate_distinctiveness(features, match_threshold=0.6)\nGrade images by their distinctiveness.\nCompute the embedding recognizability score (ERS) for each image in the feature array.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nNumPy array of shape (image_count, feature_count) containing feature vectors for each image\nrequired\n\n\nmatch_threshold\nfloat\nThe threshold above which two images are considered a match. Must be between (0, 1)\n0.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.array\nEmbedding recognizability score (ERS), a measure of distinctiveness, for every image in the dataset."
  },
  {
    "objectID": "api/grade.rate_distinctiveness.html#parameters",
    "href": "api/grade.rate_distinctiveness.html#parameters",
    "title": "grade.rate_distinctiveness",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfeatures\nnp.ndarray\nNumPy array of shape (image_count, feature_count) containing feature vectors for each image\nrequired\n\n\nmatch_threshold\nfloat\nThe threshold above which two images are considered a match. Must be between (0, 1)\n0.6"
  },
  {
    "objectID": "api/grade.rate_distinctiveness.html#returns",
    "href": "api/grade.rate_distinctiveness.html#returns",
    "title": "grade.rate_distinctiveness",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nnp.array\nEmbedding recognizability score (ERS), a measure of distinctiveness, for every image in the dataset."
  },
  {
    "objectID": "api/sort.load_features.html",
    "href": "api/sort.load_features.html",
    "title": "sort.load_features",
    "section": "",
    "text": "sort.load_features(feature_path)\nLoad previously extracted features.\nLoad features produced by the FeatureExtractor.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfeature_path\nstr\nPath to .npy file\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nfilenames, feature_array: np.array, np.array\nThe filenames array correspond to each row of the feature_array, which has shape (image_count, feature_count)"
  },
  {
    "objectID": "api/sort.load_features.html#parameters",
    "href": "api/sort.load_features.html#parameters",
    "title": "sort.load_features",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfeature_path\nstr\nPath to .npy file\nrequired"
  },
  {
    "objectID": "api/sort.load_features.html#returns",
    "href": "api/sort.load_features.html#returns",
    "title": "sort.load_features",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nfilenames, feature_array: np.array, np.array\nThe filenames array correspond to each row of the feature_array, which has shape (image_count, feature_count)"
  },
  {
    "objectID": "manuscript.html",
    "href": "manuscript.html",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "",
    "text": "Photographic identification (photo-ID) is an effective and non-invasive method for studying many aspects of ecology, including movement (Gardiner et al. 2014), social behavior (Bejder et al. 1998), abundance (McPherson et al. 2024), survival (Morrison et al. 2011), distribution (McGuire et al. 2020), recruitment (Setyawan et al. 2022), migration (Hill et al. 2020), and resource-selection (Patton 2025). Photo-ID is a multi-step process that culminates in comparing a query set—images of animals whose identity we wish to know—against a reference set—images of known individuals. This last step can be accomplished by hand (Karanth 1995), with a database (Adams et al. 2006), or with individual identification software that comes with database management, such as Happywhale (Cheeseman et al. 2021).\nCurating a query set from a batch of field images involves several steps that are often manual and labor-intensive. These include selecting images with animals (Beery et al. 2019), selecting images of sufficient quality (Urian et al. 2015), and selecting images of animals with distinctive markings (Rosel et al. 2011), which is necessary in partially marked populations. Additionally, users may want to limit the query set to only the highest quality images of each individual from a single encounter (e.g., a burst of images from a camera trap). This especially helpful for manual photo-ID because it can dramatically reduce the number of comparisons between the query and the reference set. Further, limiting the query set to only the best images from each encounter may reduce the chance of a missed match, i.e., a false negative error (Urian et al. 2015). One way to curate the query set this way is to “sort” the images by apparent individual and encounter (Figure 1).Sorting, however, is a labor-intensive process in itself, in that it requires \\(\\binom{m}{2}=m(m-1)/2\\) comparisons, where \\(m\\) is the number of images in the query set. As such, a query set of 500 images requires 124,750 comparisons to complete the sort.\n\n\n\n\n\n\nFigure 1: Example of sorted images. Before sorting, the entire query set is in one directory. After sorting, the query set is divided into many directories. In this example, the second level of directories are the temporary IDs and the third level of directories are the encounters\n\n\n\nWe developed a Python package, Pyseter, to help automate several of these steps. For example, Pyseter’s extract module extracts feature vectors from images, which are useful for estimating similarity scores and identifying individuals (Miele et al. 2021). The grade module includes functions for evaluating the distinctiveness of an animal’s markings. Finally, the sort module contains two clustering algorithms for classifying individuals into proposed identities. Additionally, sort includes functions for sorting images into proposed identities and encounters Figure 1. Pyseter currently lacks functions for detecting animals in images (Beery et al. 2019) or grading the quality of images (Rosel et al. 2011). We plan on adding these functions as the technology advances."
  },
  {
    "objectID": "manuscript.html#introduction",
    "href": "manuscript.html#introduction",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "",
    "text": "Photographic identification (photo-ID) is an effective and non-invasive method for studying many aspects of ecology, including movement (Gardiner et al. 2014), social behavior (Bejder et al. 1998), abundance (McPherson et al. 2024), survival (Morrison et al. 2011), distribution (McGuire et al. 2020), recruitment (Setyawan et al. 2022), migration (Hill et al. 2020), and resource-selection (Patton 2025). Photo-ID is a multi-step process that culminates in comparing a query set—images of animals whose identity we wish to know—against a reference set—images of known individuals. This last step can be accomplished by hand (Karanth 1995), with a database (Adams et al. 2006), or with individual identification software that comes with database management, such as Happywhale (Cheeseman et al. 2021).\nCurating a query set from a batch of field images involves several steps that are often manual and labor-intensive. These include selecting images with animals (Beery et al. 2019), selecting images of sufficient quality (Urian et al. 2015), and selecting images of animals with distinctive markings (Rosel et al. 2011), which is necessary in partially marked populations. Additionally, users may want to limit the query set to only the highest quality images of each individual from a single encounter (e.g., a burst of images from a camera trap). This especially helpful for manual photo-ID because it can dramatically reduce the number of comparisons between the query and the reference set. Further, limiting the query set to only the best images from each encounter may reduce the chance of a missed match, i.e., a false negative error (Urian et al. 2015). One way to curate the query set this way is to “sort” the images by apparent individual and encounter (Figure 1).Sorting, however, is a labor-intensive process in itself, in that it requires \\(\\binom{m}{2}=m(m-1)/2\\) comparisons, where \\(m\\) is the number of images in the query set. As such, a query set of 500 images requires 124,750 comparisons to complete the sort.\n\n\n\n\n\n\nFigure 1: Example of sorted images. Before sorting, the entire query set is in one directory. After sorting, the query set is divided into many directories. In this example, the second level of directories are the temporary IDs and the third level of directories are the encounters\n\n\n\nWe developed a Python package, Pyseter, to help automate several of these steps. For example, Pyseter’s extract module extracts feature vectors from images, which are useful for estimating similarity scores and identifying individuals (Miele et al. 2021). The grade module includes functions for evaluating the distinctiveness of an animal’s markings. Finally, the sort module contains two clustering algorithms for classifying individuals into proposed identities. Additionally, sort includes functions for sorting images into proposed identities and encounters Figure 1. Pyseter currently lacks functions for detecting animals in images (Beery et al. 2019) or grading the quality of images (Rosel et al. 2011). We plan on adding these functions as the technology advances."
  },
  {
    "objectID": "manuscript.html#installation",
    "href": "manuscript.html#installation",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "2 Installation",
    "text": "2 Installation\nHere, we assume some familiarity with Python, conda, and pip. Users coming from R, who may be less familiar with these concepts, should reference the “General Overview” Notebook that’s included in this manuscript’s attendant Zenodo repository. [Reviewers will find it in the Anonymous GitHub repository.]\nIf using conda, we recommend creating a fresh conda environment. Additionally, before installing Pyseter, users must install Pytorch. Follow the directions on the Pytorch website, which will vary based on your operating system and how you plan to use GPU acceleration. Users who plan on extracting features should have an NVIDIA GPU that is CUDA compatible, or a Mac with at least 16 GB of RAM. Below, we demonstrate the bash commands necessary to install Pytorch. These can be executed in a Jupyter Notebook (as below) or a command line interface (e.g., the miniforge prompt in Windows or the Terminal application in a Unix-like OS). After installing Pytorch, users can install Pyseter from PyPI.\n\n%%bash\nconda create -n pyseter_env  \nconda activate pyseter_env\nconda install pip3\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128\npip3 install pyseter\n\nUsers can verify the installation by running the following Python commands in, say, a Jupyter Notebook,\n\nimport pyseter\npyseter.verify_pytorch()\n\n✓ PyTorch 2.9.1 detected\n✓ Apple Silicon (MPS) GPU available\n\n\nwhich will tell the user which form of GPU acceleration is available, if any."
  },
  {
    "objectID": "manuscript.html#spinner-dolphin-example",
    "href": "manuscript.html#spinner-dolphin-example",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "3 Spinner dolphin example",
    "text": "3 Spinner dolphin example\nWe demonstrate the core modules and functions of Pyseter with a example using spinner dolphins (Stenella longirostris). Theoretically, Pyseter is taxon agnostic. Functions in the grade and sort modules work with similarity scores, which can be generated from any individual identification algorithm (Miele et al. 2021). The extract module, however, only includes one individual identification algorithm, namely, AnyDorsal, which is only suitable for cetacean dorsal images (see below) (Patton et al. 2023). As such, we chose a cetacean example to demonstrate the package’s full capabilities. The images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi (Lacey et al. 2025). Every image collected during the study was graded for quality and distinctiveness Lacey et al. (2025). This example dataset only includes images of sufficient quality that have been cropped to the identifying mark—the dorsal fin (Rosel et al. 2011) (Figure 2). This example includes 208 images of animals without distinctive markings, and 1043 images of animals with distinctive markings.\n\n\nCode\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# grab the first nine images in the dataset\nnrow = ncol = 3\ndemo_dir = 'working_dir/all_images'\ndemo_images = os.listdir(demo_dir)[:(nrow * ncol)]\n\n# plot a grid of images \nfig, axes = plt.subplots(nrow, ncol, figsize=(6, 6), tight_layout=True)\nfor i, filename in enumerate(demo_images):\n    path = os.path.join(demo_dir, filename)\n    image = Image.open(path)\n    axes.flat[i].imshow(image)\n    axes.flat[i].axis('off')\n\nfig.savefig('images/fig-demo.png', transparent=False, bbox_inches='tight', dpi=600)\n\n\n\n\n\n\n\n\nFigure 2: Nine images from the spinner dolphin example (Lacey et al. 2025)\n\n\n\n\n\n\n3.1 Folder management\nTo do keep things clean and tidy, we recommend establishing a working_directory with a subfolder, e.g., called, all_images, that contains every image that needs to be sorted (see below for a different case). The working directory should also contain a .csv with encounter information. This .csv would contain two columns: one for the image name, i.e., every image in all_images, and another for the encounter. As such, the working directory would look like this.\nworking_dir\n├── all_images\n│   ├── 0a49385ef8f1e74a.jpg\n│   ├── 0aca671c4afbd9b9.jpg\n        ...\n│   └── ffa8759a92174857.jpg\n└── encounter_info.csv\nAlternatively, you might have your images organized into subfolders by encounter.\nworking_dir\n└── original_images\n    ├── enc0\n    │   ├── 0a49385ef8f1e74a.jpg\n        ├── 1e105f9659c12a66.jpg\n            ...\n    │   └── f5093b3089b44e67.jpg\n    └── enc12\n        ├── 0b5c44f167d89d6c.jpg\n        ├── 1e0c186da31a53c4.jpg\n            ...\n        └── f9bb41e7ce0d672d.jpg\nIn this case, you might want to accomplish two tasks: move all these images to one folder, e.g., all_images, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The prep_images() function does just that.\n\nfrom pyseter.sort import prep_images\n\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)\n\nCopied 1251 images to: working_dir/all_images\nSaved encounter information to: /Users/PattonP/source/repos/pyseter/docs/working_dir/encounter_info.csv\n\n\n\n\n3.2 Extracting features\nPyseter’s extract module includes the AnyDorsal algorithm, which was trained to identify cetaceans of 24 species (Patton et al. 2023). AnyDorsal’s identifying performance varies by species. Species primarily identified by nicks and notches along the dorsal fin will perform best (Patton et al. 2023).\nPyseter extracts features with the FeatureExtractor class, which needs to be initialized by setting the batch_size. The batch_size dictates how many images will be processed by the GPU at once. Larger batch sizes might run faster yet might also produce an OutOfMemoryError. For lower memory GPUs, we recommend smaller batch sizes. If you encounter an OutOfMemoryError with batch_size=1, then you will have to reduce the size of your images. See the Supplement for how to do so with the Pillow library.\n\nfrom pyseter.extract import FeatureExtractor\n\n# feature extraction can take time so it's useful to save the result \nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\nfe = FeatureExtractor(batch_size=4)\n\nUsing device: mps (Apple Silicon GPU)\n\n\nThe extract() method of the FeatureExtractor class only takes one argument, image_dir, the flattened directory containing every image to be processed. Feature extraction can take several minutes, depending on the number of files and the GPU, so we recommend saving the results afterwards. The extract() function returns a Python dictionary where the filenames are the keys, and the features are the values. It can be useful to convert these to NumPy arrays\n\nimport numpy as np\n\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\n# convert keys and values to numpy arrays\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n\nAlternatively, we can load previously saved results with the load_features() function. In either case, feature_array will be a two-dimensional matrix of shape (n, 5504), where n is the number of images and 5504 is the number of features returned by AnyDorsal.\n\n# alternatively, load in the feature dictionary from file \nimport numpy as np \nfrom pyseter.sort import load_features\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)\n\n\n\n3.3 Grading individuals by distinctiveness\nHere, we introduce one of Pyseter’s clustering algorithms, NetworkCluster, because doing so helps understand the distinctiveness grading algorithm (see Section 3.4 for a more thorough description of NetworkCluster). Network clustering works with similarity scores, which represent the similarity between two individuals in a pair of images. We can define a threshold score, the match_threshold, above which we consider two individuals to be the same. That is, if the similarity score between two images is above a certain threshold, we cluster them into a proposed ID. As such, network clustering works by treating the query set as a network, where the nodes are images and the edges are similarity scores above a threshold. Each set of connected components, i.e., images whose similarity scores are above the match threshold, represents a proposed ID.\nWe might expect the indistinct individuals to cluster together. In the context of facial recognition, Deng et al. (2023) observed that “unrecognizable identities”, e.g., extremely blurry or masked faces, tend to cluster together. As such, for partially marked populations, the largest cluster in the query set may represent every indistinct individual. Following Deng et al. (2023), we can compute the average feature vector for this cluster. The distance between this average feature vector and the feature vector for each image is the distinctiveness score for that image. As such, the score applies to the image, not the animal. To get a score for an animal, users could average the distinctiveness scores across images for that animal.\n\nfrom pyseter.grade import rate_distinctiveness\ndistinctiveness = rate_distinctiveness(feature_array, match_threshold=0.5)\n\nUnrecognizable identity cluster consists of 196 images.\n\n\n/Users/PattonP/miniforge3/envs/pyseter_env/lib/python3.14/site-packages/pyseter/grade.py:35: UserWarning: Distinctiveness grades are experimental and should be verified.\n  warn(UserWarning('Distinctiveness grades are experimental and should be verified.'))\n\n\nWe can evaluate the effectiveness of these scores with AUC (i.e., the area under the receiver operating curve). AUC is a metric for evaluating a classifier’s ability to balance true positive and false positive rates. In this example, a classifier built from the distinctiveness grades achieves an AUC of 0.925 Figure 3.\n\n\nCode\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\nimport pandas as pd\n\n# merge the ers scores with the true values to compare for each image\nmapping = pd.read_csv('/Users/PattonP/datasets/pyseter-data/file_mapping.csv').iloc[:, 1:]\ners_df = pd.DataFrame({'image_new': filenames, 'ers': 1 - distinctiveness})\ners_df = ers_df.merge(mapping)\n\n# compute the curve first, which will get displayed\ny_score = ers_df['ers']\ny_test, _ = ers_df.distinctiveness.factorize()\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nfig, ax = plt.subplots(figsize=(4, 3))\n\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nroc_auc = roc_auc_score(y_test, y_score)\nax.text(0.95, 0.6, f'AUC={roc_auc:0.3f}', ha='right', va='top')\n\nax.set_title('ROC Curve for \\nDistinctiveness Classifier')\n\nfig.savefig('images/fig-auc.png', transparent=False, bbox_inches='tight', dpi=300)\n\n\n\n\n\n\n\n\nFigure 3: Receiving operator characteristic (ROC) curve for a classifier based on the ERS. The area under the curve (AUC), a measure of overall performance, is listed in the middle.\n\n\n\n\n\nThese distinctiveness scores are experimental in that have not been robustly tested across a variety of scenarios. Nevertheless, users might use them as a guide, potentially making distinctiveness grading somewhat easier.\n\n\n3.4 Clustering images into proposed IDs\nTo use NetworkCluster, users must first compute the similarity scores between each pair of images. After computing the scores, we cluster the images, where each cluster represents a proposed identity.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pyseter.sort import NetworkCluster, report_cluster_results\n\nsimilarity_scores = cosine_similarity(feature_array)\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n\nFollowing clusters may contain false positives:\n['ID_0001', 'ID_0006', 'ID_0008', 'ID_0021', 'ID_0110']\n\n\nAs mentioned above, network clustering has one major hyperparameter, match_threshold, which indicates whether two images should be grouped within a cluster. High thresholds mean that few images will be clustered together, creating many clusters. Very low thresholds mean that many images will be clustered together, creating few clusters. The report_cluster_results() function produces a quick and dirty summary of the number of clusters created, and the size of the largest cluster (i.e., the number of images associated to the most photographed individual). This is a quick sanity check. The results object has several useful attributes and methods (see below). For example, results.cluster_idx contains the proposed ID for each image.\n\nnetwork_idx = results.cluster_idx\nreport_cluster_results(network_idx)\n\nFound 208 clusters.\nLargest cluster has 128 images.\n\n\nIn this example, the nc.cluster_images() function warned that some clusters may contain “false positives.” False positive matches occur when two separate individuals fall under the same proposed ID. We can diagnose possible false positives by evaluating the network. Recall that, in the network, a blob of connected nodes (i.e., connected components) represents a proposed ID. These connected components, however, can sometimes look less like a blob and more like a barbell, where two sets of images have many connections amongst each other, yet the two blobs are only connected by a single link. We suspect that such clusters represent false positives, i.e., two sets of images for two individuals connected by one spurious link. We can plot the networks of the suspicious clusters with the results.plot_suspicious() function.\n\nresults.plot_suspicious()\n\n\n\n\n\n\n\nFigure 4: Proposed IDs that may contain false positive errors. Each image (colored) circle is linked to another image (gray line) if similarity between them exceeds a threshold. The colors of each circle represent potential IDs within the proposed ID.\n\n\n\n\n\nBoth ID_0011 and ID_0150 appear to have spurious links combining two separate IDs. To deal with this, we could increase manually separate these clusters, or increase the match threshold. Cluster ID_0002 represents the “unrecognizable individual” cluster (see Section 3.3).\nAs the number of images being clustered grows, the overall false positive rate also grows (this is analogous the multiple comparison problem in statistics). At some point, network matching becomes untenable; all but the highest match thresholds would produce too many false positives to be useful. For these cases, there is HierarchicalCluster, which relies on the Hierarchical Agglomerative Clustering algorithm provided by the popular machine learning package, scikit-learn (Pedregosa et al. 2011). Note that HierarchicalCluster will run noticeably slower than NetworkCluster. See the supplement for an example with HierarchicalCluster.\n\n\n3.5 Sorting images by proposed ID then encounter\nWith these cluster results, we can sort images by proposed ID then encounter. To do so, we need to create a Pandas DataFrame that indicates the proposed ID and encounter for each filename (McKinney 2010). Recall that we created the encounter_info.csv with the prep_images() function above.\n\nimport pandas as pd\n\nid_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})\n\n# join with the encounter information using \"encounter\" as a key \nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nid_df = id_df.merge(encounter_info)\nid_df.head()\n\n\n\n\n\n\n\n\nimage\nproposed_id\nencounter\n\n\n\n\n0\n2c8750b066372ab5.jpg\nID-0000\nenc8\n\n\n1\n568fc1d376b616a6.jpg\nID-0001\nenc8\n\n\n2\nddeb347716d7861c.jpg\nID-0002\nenc6\n\n\n3\nb65f9334b05f48f4.jpg\nID-0003\nenc0\n\n\n4\nd84aefa4d99d6f9a.jpg\nID-0004\nenc4\n\n\n\n\n\n\n\nFinally, to sort the images, we need to specify an output directory, then run the sort_images() function. Note that the ID DataFrame must have columns named image, proposed_id, and encounter. Otherwise sort_images() will not work.\n\nfrom pyseter.sort import sort_images\n\n# make an output directory \nsorted_dir = working_dir + '/sorted_images'\nos.makedirs(sorted_dir, exist_ok=True)\n\nsort_images(id_df, all_image_dir=image_dir, output_dir=sorted_dir)\n\nSorted 1251 images into 311 folders.\n\n\nNow the flat directory, all_images has been copied to a new folder, sorted_images, that is organized by proposed ID, then encounter.\nsorted_images\n├── ID-0000\n│   ├── enc0\n│   │   ├── 0a49385ef8f1e74a.jpg\n│   │   ├── 4d69031e07ef3393.jpg\n│   │   ├── b4f1ca6229180f18.jpg\n│   │   └── e0016bed0be5bf9f.jpg\n│   ├── enc10\n│   │   ├── 0cdbe7c151420b0c.jpg\n│   │   ├── 19dd8e9db3a26066.jpg\n            ...\n│   └── enc3\n│       ├── 1ced4b1e3bd63781.jpg\n            ...\n│       └── bc17d3aa572320cc.jpg\n├── ID-0001\n│   ├── enc0\n│   │   ├── e7dbbe01ac71d6e8.jpg\n│   │   └── f86cdadd6ecb59aa.jpg\n│   ├── enc2\n│   │   ├── 2b66eacdced6c165.jpg\n│   │   ├── 7ce2b670757443de.jpg\n            ... \n├── ID-0206\n│   └── enc4\n│       └── 767bdedefe51aabb.jpg\n└── ID-0207\n    └── enc4\n        └── e345d4ddaae7db02.jpg"
  },
  {
    "objectID": "manuscript.html#conclusion",
    "href": "manuscript.html#conclusion",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nWe have demonstrated one possible workflow for using Pyseter. A potential user would manually grade images for quality and crop images of sufficient quality to the identifying mark. Then, they would use Pyseter to grade these images for distinctiveness and to sort the images above a distinctiveness threshold into folders, selecting the highest quality image of each proposed individual. Finally, the user would compare these images against the reference set, either manually or with an interface such as Happywhale.\nNevertheless, Pyseter offers users with cetacean datasets flexibility to conduct several kinds of analyses because it allows them to extract feature vectors from images with AnyDorsal. For example, users could compute the false negative rate for their dataset, which is useful for population assessments (Patton et al. 2025). Similarly, one could use Pyseter to evaluate the predictive performance of AnyDorsal on a species that was outside the training dataset. Further, after the initial sort, users could clean up the results by doing a second round of clustering and sorting. As Pyseter matures, we plan to add documentation on how to conduct such analyses with Pyseter. Finally, as photo-ID technology improves, e.g., automated quality grading, we plan on adding these algorithms to Pyseter."
  },
  {
    "objectID": "manuscript.html#supplements",
    "href": "manuscript.html#supplements",
    "title": "Pyseter: A Python package for processing images before photo-identification",
    "section": "5 Supplements",
    "text": "5 Supplements\n\n5.1 Hierarchical Agglomerative Clustering\nBelow is a demonstration of how to use the HAC algorithm to cluster images, then sort them into folders.\n\nfrom pyseter.sort import HierarchicalCluster, format_ids\n\nhc = HierarchicalCluster(match_threshold=0.5)\nhac_idx = hc.cluster_images(feature_array)\n\n# format_ids converts the integer labels to something like 'ID-0001'\nhac_labels = format_ids(hac_idx)\nreport_cluster_results(hac_labels)\n\nhac_df = pd.DataFrame({'image': filenames, 'proposed_id': hac_labels})\nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nhac_df = hac_df.merge(encounter_info)\n\n# separate directory for the hac images\nhac_dir = working_dir + '/sorted_images_hac'\nos.makedirs(hac_dir, exist_ok=True)\nsort_images(hac_df, all_image_dir=image_dir, output_dir=hac_dir)\n\nFound 299 clusters.\nLargest cluster has 27 images.\nSorted 1251 images into 403 folders.\n\n\nHierarchicalCluster is useful for large datasets, yet will be more prone to false negative errors. In this example, it found 60 more clusters (proposed IDs) than the network matching, which may be dubious. Users will have to decide how to balance false positive versus false negative matches. For example, we recommend that users preprocess their images with Pyseter, then identify animals in the pre-processed images manually or a program such as Happywhale. This second round of identification should help clean up false negative matches. As such, users following this approach might be more averse to false positive errors in the first stage.\n\n\n5.2 Resizing images with Pillow\nHere is how is one example on how to do so with the Pillow library.\n\nfrom PIL import Image\n\nnew_size = 512, 512\n\n# files for resizing and where they'll be sent\nall_images = [i for i in os.listdir(image_dir) if i.endswith('jpg')]\nnew_dir = os.path.join(working_dir, 'thumbnails')\nos.makedirs(new_dir, exist_ok=True)\n\n# resize and save the files \nfor file in all_images:\n    old_path = os.path.join(image_dir, file)\n    with Image.open(old_path) as im:\n        im.thumbnail(new_size)\n        new_path = os.path.join(new_dir, file)\n        im.save(new_path)"
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "Python users",
    "section": "",
    "text": "This page has information on how to install for users who have some familiarity with Python. R users should see this page, which has more thorough instructions and introductions to Python concepts.\nUsers can use whatever package management utility they like, such as conda, venv, pixi, or uv. Both PyTorch and Pyseter can be installed with pip.",
    "crumbs": [
      "Install",
      "Python users"
    ]
  },
  {
    "objectID": "install.html#install-pytorch",
    "href": "install.html#install-pytorch",
    "title": "Python users",
    "section": "Install PyTorch",
    "text": "Install PyTorch\nInstalling PyTorch will allow users to extract features from images, i.e., identify individuals in images. Extracting features should be fast for users with an NVIDIA GPU, and reasonable for users with a Mac with Apple Silicon.\n\n\n\n\n\n\nWarning\n\n\n\nFor all other users, extracting features from images will be extremely slow.\n\n\nPyTorch installation can be a little finicky. Essentially, it depends on what operating system you’re using, and what version of CUDA you’re using. To install PyTorch, I recommend following these instructions.\n\nWindows users\nBelow is an example for Windows users. This relies on CUDA 12.8, which should work for most people.\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128\nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\nMac Users\nBelow is an example for Mac users. The Mac version of PyTorch relies on MPS acceleration, which is good but not at the level of CUDA.\npip3 install torch torchvision \nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\n\n\n\n\nWarning\n\n\n\nMac users will need to have an Apple Silicon processor, e.g., an M1 chip. Additionally, they will need at least 16 GB of memory (RAM) to use AnyDorsal.",
    "crumbs": [
      "Install",
      "Python users"
    ]
  },
  {
    "objectID": "install.html#install-pyseter",
    "href": "install.html#install-pyseter",
    "title": "Python users",
    "section": "Install Pyseter",
    "text": "Install Pyseter\nNow, install Pyseter from PyPI.\npip3 install pyseter\nNow you’re ready to go!",
    "crumbs": [
      "Install",
      "Python users"
    ]
  },
  {
    "objectID": "install.html#verify-the-installation",
    "href": "install.html#verify-the-installation",
    "title": "Python users",
    "section": "Verify the installation",
    "text": "Verify the installation\nVerify the Pyseter installation by running the following commands.\nimport pyseter\npyseter.verify_pytorch()\nIf you have access to an NVIDIA GPU, you should see something like\n✓ PyTorch 2.7.1+cu126 detected\n✓ CUDA GPU available: NVIDIA A30 MIG 2g.12gb",
    "crumbs": [
      "Install",
      "Python users"
    ]
  },
  {
    "objectID": "sort.html",
    "href": "sort.html",
    "title": "Pyseter",
    "section": "",
    "text": "from sklearn.metrics.pairwise import cosine_similarity\nfrom pyseter.sort import load_features, NetworkCluster\nfrom pyseter.experimental import launch_quality_review\nimport pandas as pd\n\nfeature_dir = 'working_dir/features'\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)\n\nsimilarity_scores = cosine_similarity(feature_array)\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n\nid_df = pd.DataFrame({'image': filenames, 'proposed_id': results.cluster_idx})\n\n# join with the encounter information using \"encounter\" as a key \nencounter_info = pd.read_csv('working_dir/encounter_info.csv')\nid_df = id_df.merge(encounter_info)\nid_df.head()\n\nFollowing clusters may contain false positives:\n['ID_0001', 'ID_0006', 'ID_0008', 'ID_0021', 'ID_0110']\n\n\n\n\n\n\n\n\n\nimage\nproposed_id\nencounter\n\n\n\n\n0\n2c8750b066372ab5.jpg\nID-0000\nenc8\n\n\n1\n568fc1d376b616a6.jpg\nID-0001\nenc8\n\n\n2\nddeb347716d7861c.jpg\nID-0002\nenc6\n\n\n3\nb65f9334b05f48f4.jpg\nID-0003\nenc0\n\n\n4\nd84aefa4d99d6f9a.jpg\nID-0004\nenc4\n\n\n\n\n\n\n\n\nlaunch_quality_review(\n    id_df, \n    'working_dir/all_images'\n)\n\n* Running on local URL:  http://127.0.0.1:7862\n* To create a public link, set `share=True` in `launch()`."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pyseter",
    "section": "",
    "text": "Pyseter is a Python package for processing images before photo-identification. Pyseter includes functions for:"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Pyseter",
    "section": "Introduction",
    "text": "Introduction\nPlease see the Getting Started page for a demonstration of the basic functionality of Pyseter using an example of spinner dolphins."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Pyseter",
    "section": "Installation",
    "text": "Installation\nUsers can install Pyseter with pip, uv, or pixi. For example,\npip install pyster\nWe imagine that most Pyseter users will be more familiar with R than Python. If this is you, please see the R users who are new to Python page for detailed instructions on the steps needed to work with Pyseter."
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting started with Pyseter",
    "section": "",
    "text": "Here we present a quick overview of the main functions of Pyseter with a demo dataset."
  },
  {
    "objectID": "getting-started.html#installation",
    "href": "getting-started.html#installation",
    "title": "Getting started with Pyseter",
    "section": "Installation",
    "text": "Installation\nIf you haven’t already, install some form of conda. Anaconda is probably the easiest to install, but we are personally partial to miniforge. Of course, you are also welcome to use any other package management tool, such as uv, pixi, or good old fashioned venv.\nOnce you’ve installed conda, open the command line interface (e.g., the miniforge prompt or the terminal). Then you’ll need to do several things:\n\nCreate the conda environment you’ll be using\nActive the environment and install pip\nInstall packages\n\nPytorch, which will depend on your operating system and GPU availability\ngdown (for downloading the the dataset)\nipykernel (optional, useful if using Jupyter Lab or Jupyter Notebook)\npyseter\n\n\nBelow is an example of the commands necessary to install on a Windows machine equipped with an NVIDIA GPU.\nconda create -n pyseter_env -y  # create new environment \nsource activate pyseter_env     # activate it\nconda install pip -y            # pip is necessary to install torch and pyseter\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu128 \npip install pyseter gdown ipykernel==6.30.1 ipywidgets\npython -m ipykernel install --user --name pyseter_env --display-name \"Python (Pyseter)\""
  },
  {
    "objectID": "getting-started.html#dataset",
    "href": "getting-started.html#dataset",
    "title": "Getting started with Pyseter",
    "section": "Dataset",
    "text": "Dataset\nThe images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi. Every image collected during the study was graded for quality and distinctiveness. This example dataset only includes images of sufficient quality that have been cropped to the identifying mark—the dorsal fin. This example includes 208 images of animals without distinctive markings, and 1043 images of animals with distinctive markings.\nOpen a Jupyter notebook and download the data with gdown, which pulls data from Google Drive.\n\nimport gdown\n\n# download the demo data\nfile_id = '1puM7YBTVFbIAT3xNBQV1g09K0bMGLk1y' \nfile_url = f'https://drive.google.com/uc?id={file_id}'\n\ngdown.download(file_url, quiet=False, use_cookies=False)\n\nThen, unzip the folder into our working_directory.\n\nimport zipfile\nimport shutil\n\n# extract the files to the working directory\nwith zipfile.ZipFile('original_images.zip', 'r') as zip_ref:\n    zip_ref.extractall('working_dir')\n\n# this is an artifact of zipping on mac \nshutil.rmtree('working_dir/__MACOSX')"
  },
  {
    "objectID": "getting-started.html#working-with-pyseter",
    "href": "getting-started.html#working-with-pyseter",
    "title": "Getting started with Pyseter",
    "section": "Working with Pyseter",
    "text": "Working with Pyseter\nNow we’re ready to work in Pyseter. First, verify that your PyTorch installation. This will also let you know how fast (or slow) you can expect Pyseter to be.\n\nimport pyseter\npyseter.verify_pytorch()\n\n✓ PyTorch 2.10.0 detected\n✓ Apple Silicon (MPS) GPU available\n\n\nThe demo dataset is organized into subfolders by encounter.\nworking_dir\n└── original_images\n    ├── enc0\n    │   ├── 0a49385ef8f1e74a.jpg\n        ├── 1e105f9659c12a66.jpg\n            ...\n    │   └── f5093b3089b44e67.jpg\n    └── enc12\n        ├── 0b5c44f167d89d6c.jpg\n        ├── 1e0c186da31a53c4.jpg\n            ...\n        └── f9bb41e7ce0d672d.jpg\nOur lives will be a little easier if we do two things: move all these images to a flat folder, i.e., with no subfolders, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The prep_images() function does just that.\n\nfrom pyseter.sort import prep_images\n\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)\n\nCopied 1251 images to: working_dir/all_images\nSaved encounter information to: /Users/PattonP/source/repos/pyseter/docs/working_dir/encounter_info.csv\n\n\nWe can look at a few random images with matplotlib.\n\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# select a random id to plot\nrng = np.random.default_rng(seed=17)\nimages = os.listdir(image_dir)\n\n# create a figure where each subplot is an image \ncolumn_count = 3\nrow_count = 7\nfig, axes = plt.subplots(row_count, column_count, tight_layout=True, \n                         figsize=(9, row_count * 3))\n\n# plot each image\nfor i in range(column_count * row_count):\n    ax = axes.flat[i]\n    file_name = images[i]\n    path = f'working_dir/all_images/{file_name}'\n    image = Image.open(path)\n    ax.imshow(image)\n    ax.axis('off')\n\n\n\n\n\n\n\n\n\nFeature extraction\nIdentifying animals in images requires extracting feature vectors. The distance between two feature vectors tells us similarity between two images. Similar images likely contain the same individual, if the model is working.\nTo do so, we’ll initialize the FeatureExtractor. The only argument is the batch size, which we recommend setting to something low, like 4.\n\nimport os\nfrom pyseter.extract import FeatureExtractor\n\n# we'll save the results in the feature_dir\nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n# initialize the extractor \nfe = FeatureExtractor(batch_size=4)\n\nUsing device: mps (Apple Silicon GPU)\n\n\nNow we can extract features. This will take a while, depending on hardware. On a machine with an NVIDIA GPU, this will take a minute or two. On a Mac, this will take around 10 minutes. Unfortunately, we have not been able to test on a PC without a GPU, but expect it to be slower than 10 minutes.\nThe first time you extract features with Pyseter, it will download AnyDorsal to your machine. AnyDorsal is huge (4.5GB), so be prepared!\n\nimport numpy as np\n\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\n# convert keys and values to numpy arrays\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n\nYou can also load in features from a previously saved session.\n\n# alternatively, load in the feature dictionary from file \nimport numpy as np \nfrom pyseter.sort import load_features\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)"
  },
  {
    "objectID": "getting-started.html#clustering-individuals",
    "href": "getting-started.html#clustering-individuals",
    "title": "Getting started with Pyseter",
    "section": "Clustering individuals",
    "text": "Clustering individuals\nNext, we can cluster individuals into proposed IDs. One simple method for doing so is to say that any two images with similarity scores above a match_threshold belong to the same individual (note that the similarity score is one minus the distance between two feature vectors). This creates a network where each node is an image and the connected nodes represent one proposed ID. Hence, we call this NetworkCluster\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pyseter.sort import NetworkCluster, report_cluster_results\n\nsimilarity_scores = cosine_similarity(feature_array)\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n\nFollowing clusters may contain false positives:\n['ID_0001', 'ID_0006', 'ID_0008', 'ID_0021', 'ID_0110']\n\n\nYou can look at the results with report_cluster_results.\n\nnetwork_idx = results.cluster_idx\nreport_cluster_results(network_idx)\n\nFound 208 clusters.\nLargest cluster has 128 images.\n\n\nIn an ideal world, every image of the same individual would be connected in the network, and not connected to images of any other individuals. As such, the ideal network would look like a bunch of blobs. As such, we might be suspicious of clusters that look like barbells, i.e., two blobs connected by one link. That one link might be a false positive, linking two distinct identities. The plot_suspicious function plots things that might look like barbells.\n\nresults.plot_suspicious()"
  },
  {
    "objectID": "getting-started.html#sorting-individuals-into-folders",
    "href": "getting-started.html#sorting-individuals-into-folders",
    "title": "Getting started with Pyseter",
    "section": "Sorting individuals into folders",
    "text": "Sorting individuals into folders\nNow, we might want to inspect these proposed ID. One method is to chuck the IDs into a table, i.e., a DataFrame.\n\nimport pandas as pd\n\nid_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})\n\n# join with the encounter information using \"encounter\" as a key \nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nid_df = id_df.merge(encounter_info)\nid_df.head()\n\n\n\n\n\n\n\n\nimage\nproposed_id\nencounter\n\n\n\n\n0\n2c8750b066372ab5.jpg\nID-0000\nenc8\n\n\n1\n568fc1d376b616a6.jpg\nID-0001\nenc8\n\n\n2\nddeb347716d7861c.jpg\nID-0002\nenc6\n\n\n3\nb65f9334b05f48f4.jpg\nID-0003\nenc0\n\n\n4\nd84aefa4d99d6f9a.jpg\nID-0004\nenc4\n\n\n\n\n\n\n\nBut this isn’t particularly satisfying, since we can’t actually see the IDs. Probably the easiest way to to sort the images into folders then explore them with the file explore (e.g., Finder on Mac), or with a program like ACDSee. We can do so with sort_images.\n\nfrom pyseter.sort import sort_images\n\n# make an output directory \nsorted_dir = working_dir + '/sorted_images'\nos.makedirs(sorted_dir, exist_ok=True)\n\nsort_images(id_df, all_image_dir=image_dir, output_dir=sorted_dir)\n\nSorted 1251 images into 311 folders.\n\n\nWe can also plot some images with matplotlib, if that’s your thing.\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# select a random id to plot\nrng = np.random.default_rng(seed=17)\nrandom_id = rng.choice(id_df.proposed_id)\n\n# collect all the images for that id\nid_info = id_df.loc[id_df.proposed_id == random_id].sort_values('encounter').reset_index(drop=True)\nimage_count = len(id_info)\n\n# create a figure where each subplot is an image \ncolumn_count = 3\nrow_count = (image_count + column_count - 1) // column_count\nfig, axes = plt.subplots(row_count, column_count, tight_layout=True, \n                         figsize=(9, row_count * 3))\n\n# plot each image\nfor i, row in id_info.iterrows():\n    ax = axes.flat[i]\n    path = f'working_dir/all_images/{row.image}'\n    image = Image.open(path)\n    ax.imshow(image)\n    ax.set_title(row.encounter)\n    ax.axis('off')\n\nfig.text(0.5, 1.0, f'Proposed ID: {random_id}', ha='center', va='bottom',\n         fontsize=16)\n\n# delete any unnecessary subplots\nfor i in range(image_count, row_count * column_count):\n    axes.flat[i].remove()"
  },
  {
    "objectID": "identify.html",
    "href": "identify.html",
    "title": "Pyseter",
    "section": "",
    "text": "from pyseter.sort import load_features\nfrom pyseter.identify import identify\nfrom pyseter.experimental import launch_review\nimport pandas as pd\n\nid_df = pd.read_csv('/Users/PattonP/datasets/happywhale/train.csv')\n\nreference_path = '/Users/PattonP/datasets/happywhale/features/train_features.npy'\nreference_files, reference_features = load_features(reference_path)\n\nquery_path = '/Users/PattonP/datasets/happywhale/features/test_features.npy'\nquery_files, query_features = load_features(query_path)\n\nquery_dict = dict(zip(query_files, query_features))\nreference_dict = dict(zip(reference_files, reference_features))\n\nid_df.head()\n\n\n\n\n\n\n\n\nimage\nspecies\nindividual_id\n\n\n\n\n0\n00021adfb725ed.jpg\nmelon_headed_whale\ncadddb1636b9\n\n\n1\n000562241d384d.jpg\nhumpback_whale\n1a71fbb72250\n\n\n2\n0007c33415ce37.jpg\nfalse_killer_whale\n60008f293a2b\n\n\n3\n0007d9bca26a99.jpg\nbottlenose_dolphin\n4b00fe572063\n\n\n4\n00087baf5cef7a.jpg\nhumpback_whale\n8e5253662392\n\n\n\n\n\n\n\n\nprediction_df = identify(reference_dict, query_dict, id_df)\n\n\nlaunch_review(\n    prediction_df,\n    id_df, \n    '/Users/PattonP/datasets/happywhale/test_images',\n    '/Users/PattonP/datasets/happywhale/train_images'\n)\n\n* Running on local URL:  http://127.0.0.1:7861\n* To create a public link, set `share=True` in `launch()`."
  },
  {
    "objectID": "api/sort.sort_images.html",
    "href": "api/sort.sort_images.html",
    "title": "sort.sort_images",
    "section": "",
    "text": "sort.sort_images(id_df, all_image_dir, output_dir)\nSort images into subfolders by proposed ID then encounter.\nCopy images from the flat all_image_dir into the output_dir, where the output_dir is now divided in subfolders by proposed ID then encounter.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid_df\npd.DataFrame\nPandas DataFrame with columns ['image', 'proposed_id', 'encounter'].\nrequired\n\n\nall_image_dir\nstr\nPath to flat directory with every image in the id_df.\nrequired\n\n\noutput_dir\nstr\nPath to new directory into which sort_images will copy files.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nCopies images to the output_dir.\n\n\n\n\n\n\nFor a complete working example with real images, see:\n\nTutorial"
  },
  {
    "objectID": "api/sort.sort_images.html#parameters",
    "href": "api/sort.sort_images.html#parameters",
    "title": "sort.sort_images",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nid_df\npd.DataFrame\nPandas DataFrame with columns ['image', 'proposed_id', 'encounter'].\nrequired\n\n\nall_image_dir\nstr\nPath to flat directory with every image in the id_df.\nrequired\n\n\noutput_dir\nstr\nPath to new directory into which sort_images will copy files.\nrequired"
  },
  {
    "objectID": "api/sort.sort_images.html#returns",
    "href": "api/sort.sort_images.html#returns",
    "title": "sort.sort_images",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nCopies images to the output_dir."
  },
  {
    "objectID": "api/sort.sort_images.html#examples",
    "href": "api/sort.sort_images.html#examples",
    "title": "sort.sort_images",
    "section": "",
    "text": "For a complete working example with real images, see:\n\nTutorial"
  },
  {
    "objectID": "api/sort.ClusterResults.html",
    "href": "api/sort.ClusterResults.html",
    "title": "sort.ClusterResults",
    "section": "",
    "text": "sort.ClusterResults\nsort.ClusterResults(cluster_labels)\nStoring NetworkCluster results."
  },
  {
    "objectID": "api/get_best_device.html",
    "href": "api/get_best_device.html",
    "title": "get_best_device",
    "section": "",
    "text": "get_best_device\nget_best_device()\nSelect torch device based on expected performance."
  },
  {
    "objectID": "api/extract.FeatureExtractor.html",
    "href": "api/extract.FeatureExtractor.html",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "extract.FeatureExtractor(batch_size, device=None, stochastic=False)\nExtract features from images.\nExtract feature vectors for individual identification from images. Currently, FeatureExtractor only includes the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbatch_size\nint\nThe number of images the GPU will process.\nrequired\n\n\ndevice\n(None, cuda, mps, cpu)\nDevice with which to extract the features. By default, the best device is chosen for the user (cuda, mps, or cpu)\nNone\n\n\nstochastic\nboolean\nCurrently unused.\nFalse\n\n\n\n\n\n\nFor a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.extract import FeatureExtractor\n\n# Initialize extractor\nextractor = FeatureExtractor(batch_size=16)\n\n# Extract features from all images\nfeatures = extractor.extract('path/to/images/')\n\n# Access individual image features\nimg_features = features['my_image.jpg']\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nextract\nExtracts features from images.\n\n\n\n\n\nextract.FeatureExtractor.extract(image_dir, bbox_csv=None)\nExtracts features from images.\nExtracts feature vectors for every image in a directory with the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/extract.FeatureExtractor.html#parameters",
    "href": "api/extract.FeatureExtractor.html#parameters",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nbatch_size\nint\nThe number of images the GPU will process.\nrequired\n\n\ndevice\n(None, cuda, mps, cpu)\nDevice with which to extract the features. By default, the best device is chosen for the user (cuda, mps, or cpu)\nNone\n\n\nstochastic\nboolean\nCurrently unused.\nFalse"
  },
  {
    "objectID": "api/extract.FeatureExtractor.html#examples",
    "href": "api/extract.FeatureExtractor.html#examples",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "For a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.extract import FeatureExtractor\n\n# Initialize extractor\nextractor = FeatureExtractor(batch_size=16)\n\n# Extract features from all images\nfeatures = extractor.extract('path/to/images/')\n\n# Access individual image features\nimg_features = features['my_image.jpg']"
  },
  {
    "objectID": "api/extract.FeatureExtractor.html#methods",
    "href": "api/extract.FeatureExtractor.html#methods",
    "title": "extract.FeatureExtractor",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nextract\nExtracts features from images.\n\n\n\n\n\nextract.FeatureExtractor.extract(image_dir, bbox_csv=None)\nExtracts features from images.\nExtracts feature vectors for every image in a directory with the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html",
    "href": "api/extract.FeatureExtractor.extract.html",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "extract.FeatureExtractor.extract(image_dir, bbox_csv=None)\nExtracts features from images.\nExtracts feature vectors for every image in a directory with the AnyDorsal algorithm.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html#parameters",
    "href": "api/extract.FeatureExtractor.extract.html#parameters",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nDirectory of images to from which to extract features. Directory should be flat, in that there should not be subdirectories with images.\nrequired\n\n\nbbox_csv\nstr\nOptional path to csv file with bounding boxes for each image in the image_dir.\nNone"
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html#returns",
    "href": "api/extract.FeatureExtractor.extract.html#returns",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\ndict\nA mapping image file names to the corresponding feature vector. The file names are represented as strings, while the feature vector. is a NumPy array. For example: {'img1.jpg': np.array([0.1, 0.1, 0.2, ..., 0.9]),  'img2.jpg': np.array([0.2, 0.3, 0.4, ..., 0.1])} The numpy array should have length 5504."
  },
  {
    "objectID": "api/extract.FeatureExtractor.extract.html#raises",
    "href": "api/extract.FeatureExtractor.extract.html#raises",
    "title": "extract.FeatureExtractor.extract",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nOutOfMemoryError\nThe GPU has run out of memory. Try reducing your batch size, or reducing the file size of the images in the directory."
  },
  {
    "objectID": "api/sort.prep_images.html",
    "href": "api/sort.prep_images.html",
    "title": "sort.prep_images",
    "section": "",
    "text": "sort.prep_images(image_dir, all_image_dir)\nCopy all images to a flat directory and save a csv with encounter info.\nSome users may have their image directory structured such that each image is in a subfolder by encounter, e.g., original_images/enc1/img1.jpg. The FeatureExtractor in pyseter.extract prefers flat directories. prep_images flattens the original_images directory by copying every image to all_image_dir, then saves a csv with encounter information to the working directory.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nPath to directory containing images.\nrequired\n\n\nall_image_dir\nstr\nPath to new directory where user wants to copy all their images.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\nSaves images to the all_image_dir and the encounter information to the csv in the working_dir.\n\n\n\n\n\n\nFor a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.sort import prep_images\n\n# old directory, structured by encounter\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)"
  },
  {
    "objectID": "api/sort.prep_images.html#parameters",
    "href": "api/sort.prep_images.html#parameters",
    "title": "sort.prep_images",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nimage_dir\nstr\nPath to directory containing images.\nrequired\n\n\nall_image_dir\nstr\nPath to new directory where user wants to copy all their images.\nrequired"
  },
  {
    "objectID": "api/sort.prep_images.html#returns",
    "href": "api/sort.prep_images.html#returns",
    "title": "sort.prep_images",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nNone\nSaves images to the all_image_dir and the encounter information to the csv in the working_dir."
  },
  {
    "objectID": "api/sort.prep_images.html#examples",
    "href": "api/sort.prep_images.html#examples",
    "title": "sort.prep_images",
    "section": "",
    "text": "For a complete working example with real images, see:\n\nTutorial\n\nBasic usage pattern::\nfrom pyseter.sort import prep_images\n\n# old directory, structured by encounter\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)"
  },
  {
    "objectID": "install-r.html",
    "href": "install-r.html",
    "title": "R users who are new to Python",
    "section": "",
    "text": "We expect that most people using Pyseter will be familiar with R, and completely new to Python. This raises the question: why release Pyseter as a Python package? The answer is that Python is much more suited to deep learning than R, and most of the new developments in automated photo-identification rely on deep learning.\nAs such, Pyseter users will have to familiarize themselves with a few Python concepts, such as conda and Jupyter, before getting started.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-conda",
    "href": "install-r.html#install-conda",
    "title": "R users who are new to Python",
    "section": "Install conda",
    "text": "Install conda\nConda is an important tool for managing packages in Python. While R, for the most part, handles packages for you behind the scenes, Python requires a more hands on approach.\nTo get started, we first need to install a package manager called conda. There are many forms of conda, with Anaconda being the most popular. For several reasons, we prefer another form of conda called Miniforge.\n\nDownload and install Miniforge (a form of conda)\n\nAfter installing, you can verify your installation by opening the command line interface (CLI), which will depend on your operating system. Are you on Windows? Open the “miniforge prompt” in your start menu. Are you on Mac? Open the Terminal application. Then, type the following command into the CLI and hit return.\nconda --version\nYou should see something like conda 25.5.1. Of course, Anaconda, miniconda, mamba, or any other form of conda will work too.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#create-a-new-environment",
    "href": "install-r.html#create-a-new-environment",
    "title": "R users who are new to Python",
    "section": "Create a new environment",
    "text": "Create a new environment\nThen, you’ll create an environment for the package will live in. Environments are walled off areas where we can install packages. This allows you to have multiple versions of the same package installed on your machine, which can help prevent conflicts.\nEnter the following two commands into the CLI:\nconda create -n pyseter_env\nconda activate pyseter_env\nHere, I name (hence the -n) the environment pyseter_env, but you can call it anything you like!\nNow your environment is ready to go! Try installing your first package, pip. Pip is another way of installing Python packages, and will be helpful for installing PyTorch and Pyseter (see below). To do so, enter the following command into the CLI.\nconda install pip -y\nIn this case, we include the -y argument so we don’t have to immediately answer yes to the next question. Once this is working, you’re ready to proceed to the next section.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-pytorch",
    "href": "install-r.html#install-pytorch",
    "title": "R users who are new to Python",
    "section": "Install PyTorch",
    "text": "Install PyTorch\nInstalling PyTorch will allow users to extract features from images, i.e., identify individuals in images. Extracting features should be fast for users with an NVIDIA GPU, and reasonable for users with a Mac with Apple Silicon.\n\n\n\n\n\n\nWarning\n\n\n\nFor all other users, extracting features from images will be extremely slow.\n\n\nPyTorch installation can be a little finicky. Essentially, it depends on what operating system you’re using, and what version of CUDA you’re using. CUDA is the technology that turns your NVIDIA GPU into a deep learning machine. To install PyTorch, I recommend following these instructions.\n\nWindows users\nBelow is an example for Windows users. This relies on CUDA 12.8, which should work for most people. If you haven’t already, open the CLI (e.g., the miniforge prompt). Then activate your environment before installing.\nconda activate pyseter_env\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128\nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\nMac users\nBelow is an example for Mac users. The Mac version of PyTorch relies on MPS acceleration, which is good but not at the level of CUDA. If you haven’t already, open you’re command line interface (e.g., the miniforge prompt). Then activate your environment before installing.\nconda activate pyseter_env\npip3 install torch torchvision \nPyTorch is pretty big (over a gigabyte), so this may take a few minutes.\n\n\n\n\n\n\nWarning\n\n\n\nMac users will need to have an Apple Silicon processor, e.g., an M1 chip. Additionally, they will need at least 16 GB of memory (RAM) to use AnyDorsal.\n\n\n\n\nLinux users\nYou run Linux, but you’ve never used Python?",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-pyseter",
    "href": "install-r.html#install-pyseter",
    "title": "R users who are new to Python",
    "section": "Install Pyseter",
    "text": "Install Pyseter\nNow, install Pyseter. If you haven’t already, activate your environment before installing.\nconda activate pyseter_env\npip3 install pyseter\nNow you’re ready to go!",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#install-vs-code-or-positron",
    "href": "install-r.html#install-vs-code-or-positron",
    "title": "R users who are new to Python",
    "section": "Install VS Code or Positron",
    "text": "Install VS Code or Positron\nMost users will interact with Pyseter via a Jupyter Notebook. There are many methods for opening, editing, running, and saving Jupyter Notebooks. We are personally biased towards VS Code. Alternatively, R users might also try out Positron. The team at Posit (formerly, R Studio) developed Positron from the open source version of VS Code, but with R users in mind. As such, it might be a nice hybrid option. That said, we have found Positron to throw confusing errors and have found VS Code to be more stable.\n\nDownload and install VS Code\n\nOpen VS Code, then click “File -&gt; Open Folder”. Navigate to wherever you’d like to work, then click “New Folder.” You can call this folder something like “learn-pyseter” or “pyseter-jobs”. Open the new folder. Click “File -&gt; New File” then select Jupyter Notebook. Click “Select Kernel” in the top right corner, select “Python environments” and then “pyseter_env”, or whatever you named your environment. For more information, check out this great overview of using Jupyter Notebooks in VS Code.\nNow you’re ready to proceed to the next section.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#verify-the-installation",
    "href": "install-r.html#verify-the-installation",
    "title": "R users who are new to Python",
    "section": "Verify the installation",
    "text": "Verify the installation\nVerify the Pyseter installation by running the following cell in your notebook.\nimport pyseter\npyseter.verify_pytorch()\nIf you’re on a windows computer with an NVIDIA GPU, you should see something like\n✓ PyTorch 2.7.1+cu126 detected\n✓ CUDA GPU available: NVIDIA A30 MIG 2g.12gb\nOnce this is working, you’re ready to check out the “General Overview” notebook in the examples folder of this repository!",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  },
  {
    "objectID": "install-r.html#anydorsal-weights",
    "href": "install-r.html#anydorsal-weights",
    "title": "R users who are new to Python",
    "section": "AnyDorsal weights",
    "text": "AnyDorsal weights\nPyseter relies on the AnyDorsal algorithm to extract features from images. The first time you use the FeatureExtractor, Pyseter will download the AnyDorsal weights from Hugging Face. The weights take up roughly 4.5 GB. As such, to use the FeatureExtractor, users must have enough storage space to accommodate the weights.",
    "crumbs": [
      "Install",
      "R users who are new to Python"
    ]
  }
]