{
  "hash": "dc0aab579fcafa6e921048340b684e77",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Identifying animals with a reference set\njupyter:\n  kernelspec:\n    display_name: Python (Pyseter)\n    language: python\n    name: pyseter_env\nexecute:\n  cache: true\n---\n\nIn this notebook, we'll demonstrate how do identify animals in a query set using a catalog of known individuals (i.e., a reference set). We'll use the [Happy Whale and Dolphin Kaggle competition dataset](https://www.kaggle.com/competitions/happy-whale-and-dolphin/data) as an example. You can download the data by following that linked page (click the big \"Download all\" button). FYI, you'll have to create an account first.\n\nThere are three components of the Happywhale dataset that we'll focus on:\n\n  - **train.csv** .csv containing the IDs for every image in the reference set \n  - **train_images** Directory containing every image in the reference set \n  - **test_images** Directory containing every image in the query set\n\nIn this case, we're treating the training dataset as the reference set, since we know the true identities. \n\n## Set up\n\nFeel free to place the data anywhere you like, e.g., within a `pyseter_jobs` folder or something. I frequently come back to the Happywhale dataset, so I have it saved locally. \n\n::: {#bad632db .cell execution_count=1}\n``` {.python .cell-code}\n%config InlineBackend.figure_format = 'retina'\nimport os\n\nfrom pyseter.extract import FeatureExtractor\nfrom pyseter.sort import load_features\nfrom pyseter.identify import predict_ids, update_reference_features\nimport numpy as np\nimport pandas as pd\n\ndata_dir = '/Users/PattonP/datasets/happywhale/'\n```\n:::\n\n\n## Extracting features using bounding boxes\n\nNow that we've downloaded the data, we'll get ready to extract the feature vectors by initializing the `FeatureExtractor`. Some of the images in the Happywhale dataset are pretty big, so we'll set the `batch_size` to a low value, `4`.\n\n::: {#3f696f3f .cell execution_count=2}\n``` {.python .cell-code}\n# we'll save the results in the feature_dir\nfeature_dir = data_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n# initialize the extractor \nfe = FeatureExtractor(batch_size=4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: mps (Apple Silicon GPU)\n```\n:::\n:::\n\n\n:::{.callout-warning}\n\nThere are about 75,000 images in the Happywhale dataset. In my testing, on an NVIDIA GPU, it takes about 45 minutes to extract the features for the 50,000 reference images, and 25 minutes to extract the features for the 25,000 query images. On my Apple M4 MacBook, it takes about 3 hours and 30 minutes for the reference images and about 2 hours for the query images.\n\n:::\n\nAdditionally, we'll need to supply *bounding boxes* to the feature extractor. Many of the Happywhale images are taken from far away, so we need to crop the image to just the animal. To do so, we'll supply the path to the bounding box .csv as to the argument, `bbox_csv`. The .csv needs to have columns named: `['image', 'xmin', 'xmax', 'ymin', ymax']` that contain the image name and the coordinates for the corners of the box.  \n\n::: {#aa861527 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](identify_files/figure-html/cell-4-output-1.png){width=1910 height=341}\n:::\n:::\n\n\n::: {#894ad2a9 .cell execution_count=4}\n``` {.python .cell-code}\nbbox_url = 'https://raw.githubusercontent.com/philpatton/pyseter/main/data/happywhale-charm-boxes.csv'\n\ntrain_dir = data_dir + '/train_images'\ntrain_features = fe.extract(image_dir=train_dir, bbox_csv=bbox_url)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/train_features.npy'\nnp.save(out_path, train_features)\n\n# now do the test images\ntest_dir = data_dir + '/test_images'\ntest_features = fe.extract(image_dir=test_dir, bbox_csv=bbox_url)\n\nout_path = feature_dir + '/test_features.npy'\nnp.save(out_path, test_features)\n```\n:::\n\n\nIf you've already extracted the features, you can load them back into your session.\n\n::: {#1a722cd1 .cell execution_count=5}\n``` {.python .cell-code}\nreference_path = feature_dir + '/train_features.npy'\nreference_files, reference_features = load_features(reference_path)\n\nquery_path = feature_dir + '/test_features.npy'\nquery_files, query_features = load_features(query_path)\n```\n:::\n\n\n## Identifying animals\n\nFirst, we'll create two dictionaries. Dictionaries are similar to a named list in R, where we can access the value in the dictionary by providing it's key. In this case, the key will be the image name and the value will be the feature vector for that image.\n\nWe'll also need a `DataFrame` that tells us the identity of every individual in the reference set. This comes with the Kaggle dataset, in the `train.csv` file.\n\n::: {#9660d97f .cell execution_count=6}\n``` {.python .cell-code}\nquery_dict = dict(zip(query_files, query_features))\nreference_dict = dict(zip(reference_files, reference_features))\n\nid_df = pd.read_csv(data_dir + '/train.csv')\nid_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>species</th>\n      <th>individual_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00021adfb725ed.jpg</td>\n      <td>melon_headed_whale</td>\n      <td>cadddb1636b9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000562241d384d.jpg</td>\n      <td>humpback_whale</td>\n      <td>1a71fbb72250</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0007c33415ce37.jpg</td>\n      <td>false_killer_whale</td>\n      <td>60008f293a2b</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0007d9bca26a99.jpg</td>\n      <td>bottlenose_dolphin</td>\n      <td>4b00fe572063</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00087baf5cef7a.jpg</td>\n      <td>humpback_whale</td>\n      <td>8e5253662392</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAnd now we're ready to make predictions! By default, `predict_ids` returns 10 proposed IDs. Here we'll show just 2 so for the sake of variety. \n\n::: {#3d8cd9f6 .cell execution_count=7}\n``` {.python .cell-code}\nprediction_df = predict_ids(reference_dict, query_dict, id_df, proposed_id_count=2)\nprediction_df.head(20)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>rank</th>\n      <th>predicted_id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a704da09e32dc3.jpg</td>\n      <td>1</td>\n      <td>5f2296c18e26</td>\n      <td>0.500233</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a704da09e32dc3.jpg</td>\n      <td>2</td>\n      <td>new_individual</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>de1569496d42f4.jpg</td>\n      <td>1</td>\n      <td>ed237f7c2165</td>\n      <td>0.826259</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>de1569496d42f4.jpg</td>\n      <td>2</td>\n      <td>new_individual</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4ab51dd663dd29.jpg</td>\n      <td>1</td>\n      <td>b9b24be2d5ae</td>\n      <td>0.680653</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4ab51dd663dd29.jpg</td>\n      <td>2</td>\n      <td>31f748b822f4</td>\n      <td>0.503390</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>da27c3f9f96504.jpg</td>\n      <td>1</td>\n      <td>c02b7ad6faa0</td>\n      <td>0.937102</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>da27c3f9f96504.jpg</td>\n      <td>2</td>\n      <td>new_individual</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0df089463bfd6b.jpg</td>\n      <td>1</td>\n      <td>f7b322faeeb5</td>\n      <td>0.538287</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0df089463bfd6b.jpg</td>\n      <td>2</td>\n      <td>ae9cca8f13ca</td>\n      <td>0.504653</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>813892efb592e0.jpg</td>\n      <td>1</td>\n      <td>c22d65f2d2f0</td>\n      <td>0.808234</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>813892efb592e0.jpg</td>\n      <td>2</td>\n      <td>new_individual</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0c9304ddd0ba35.jpg</td>\n      <td>1</td>\n      <td>2df99dc71d85</td>\n      <td>0.852067</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0c9304ddd0ba35.jpg</td>\n      <td>2</td>\n      <td>new_individual</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14718a369776c5.jpg</td>\n      <td>1</td>\n      <td>e8d3c0ff0951</td>\n      <td>0.800375</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>14718a369776c5.jpg</td>\n      <td>2</td>\n      <td>new_individual</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>65653992318202.jpg</td>\n      <td>1</td>\n      <td>c4e546efa5ca</td>\n      <td>0.842364</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>65653992318202.jpg</td>\n      <td>2</td>\n      <td>new_individual</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>9857340b9e8c8e.jpg</td>\n      <td>1</td>\n      <td>1a20c92ffe68</td>\n      <td>0.813362</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>9857340b9e8c8e.jpg</td>\n      <td>2</td>\n      <td>new_individual</td>\n      <td>0.500000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBy default, `predict_ids` inserts a dummy prediction \"new_individual\" at 0.5. This makes it easy to evaluate the algorithm with metrics like MAP@5, or calculate the false negative rate. \n\nYou can save the results with `to_csv` from pandas. \n\n::: {#4970a16e .cell execution_count=8}\n``` {.python .cell-code}\nprediction_df.to_csv('predicted_ids.csv', index=False)\n```\n:::\n\n\n## Updating the reference set\n\nLet's say you've gone through and confirmed all the matches in your query set, e.g., with the [*AnyDorsal* ID app](app.qmd). Now you would like to update your reference set with the new IDs.\n\nHere, we'll take the naive approach that the algorithm's first choice was always correct and update our reference set accordingly.\n\n::: {#bf3cba3a .cell execution_count=9}\n``` {.python .cell-code}\n# select the first match as the correct one\nconfirmed_matches = prediction_df.loc[prediction_df['rank'] == 1]\n```\n:::\n\n\nNow we'll want to update two things: `id_df`, which contains the true ids for all our reference images, and `reference_features`, which contains the feature vectors for every reference image. `update_reference_features` allows us to update the reference features, such that we can easily import them later with `load_features()`.\n\n::: {#e9106522 .cell execution_count=10}\n``` {.python .cell-code}\n# a dataframe for every image with a confirmed id\nconfirmed_match_df = confirmed_matches[['image', 'predicted_id']]\nconfirmed_match_df.columns = ['image', 'individual_id']\n\n# create a new reference dict \nupdated_reference_dict = update_reference_features(\n    reference_dict, query_dict, confirmed_match_df\n)\n\n# save the output so we can load them later \nout_path = feature_dir + '/updated_features.npy'\nnp.save(out_path, updated_reference_dict)\n```\n:::\n\n\nWe can also update our `id_df`. To do so, we need to \"union\" it (SQL jargon) with the confirmed matches. We do this with `pd.concat()`, which is similar to R's `rbind`. Note that that we don't have the species classification for the individuals in the query set. \n\n::: {#e3f3dca2 .cell execution_count=11}\n``` {.python .cell-code}\n# union with the id_df \nupdated_id_df = pd.concat((id_df, confirmed_match_df)).reset_index(drop=True)\nupdated_id_df.to_csv(data_dir + 'updated_ids.csv', index=False)\nupdated_id_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>species</th>\n      <th>individual_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00021adfb725ed.jpg</td>\n      <td>melon_headed_whale</td>\n      <td>cadddb1636b9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000562241d384d.jpg</td>\n      <td>humpback_whale</td>\n      <td>1a71fbb72250</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0007c33415ce37.jpg</td>\n      <td>false_killer_whale</td>\n      <td>60008f293a2b</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0007d9bca26a99.jpg</td>\n      <td>bottlenose_dolphin</td>\n      <td>4b00fe572063</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00087baf5cef7a.jpg</td>\n      <td>humpback_whale</td>\n      <td>8e5253662392</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "identify_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}