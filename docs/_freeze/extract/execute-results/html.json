{
  "hash": "33d9ea795bfb946fe1751ace00a22cd6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Extracting features\njupyter:\n  kernelspec:\n    display_name: Python (Pyseter)\n    language: python\n    name: pyseter_env\n---\n\nHere we present a quick overview on how to extract feature vectors with Pyseter. Feature vectors are summaries of images that are useful for identifying individuals. If *AnyDorsal* is working, then the similarity between two images should indicate that the individuals within the images look similar. \n\n::: {#bbd18aa0 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\n\nfrom pyseter.extract import FeatureExtractor\nfrom pyseter.sort import load_features, prep_images\nimport gdown\nimport numpy as np\nimport pyseter\nimport zipfile\n```\n:::\n\n\n## Dataset\n\nThe images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawai ªi. We can download the data with `gdown`, which pulls data from Google Drive. We'll unzip the file to the `working_dir`. \n\n::: {#6be7e03b .cell execution_count=2}\n``` {.python .cell-code}\n# download the demo data\nfile_id = '1puM7YBTVFbIAT3xNBQV1g09K0bMGLk1y' \nfile_url = f'https://drive.google.com/uc?id={file_id}'\n\ngdown.download(file_url, quiet=False, use_cookies=False)\n\n# extract the files to the working directory\nwith zipfile.ZipFile('original_images.zip', 'r') as zip_ref:\n    zip_ref.extractall('working_dir')\n```\n:::\n\n\nThe demo dataset is organized into subfolders by encounter. Our lives will be a little easier if we move all these images to a flat folder. The `prep_images()` function does just that.\n\n::: {#505dd0bc .cell execution_count=3}\n``` {.python .cell-code}\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCopied 1251 images to: working_dir/all_images\nSaved encounter information to: /Users/PattonP/source/repos/pyseter/docs/working_dir/encounter_info.csv\n```\n:::\n:::\n\n\n## Extracting features\n\nExtracting features, like all modern AI, depends on GPUs. Users with an NVIDIA GPU can expect fast feature extraction. Extracting features for the ~1200 images in the demo dataset, for example, will take about two minutes on an NVIDIA GPU. Most university or governmental high performance computing clusters (HPCs) will have GPUs available. \n\nFeature extraction also works reasonably quickly on Apple Silicon (i.e., M1-M4). Extracting features for in the demo dataset, for example, will take about 10 minutes on Apple Silicon, depending on the model.\n\nWe can verify the Pyseter installation and the acceleration with `verify_pytorch`.\n\n::: {#ae893ef6 .cell execution_count=4}\n``` {.python .cell-code}\npyseter.verify_pytorch()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n:) PyTorch 2.10.0 detected\n:) Apple Silicon (MPS) GPU available\n```\n:::\n:::\n\n\nWe also need to initialize the `FeatureExtractor`. The only argument is the `batch_size`, which we recommend setting to something low, like 4. \n\n::: {#632306df .cell execution_count=5}\n``` {.python .cell-code}\n# we'll save the results in the feature_dir\nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n# initialize the extractor \nfe = FeatureExtractor(batch_size=4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: mps (Apple Silicon GPU)\n```\n:::\n:::\n\n\nThe first time you extract features with Pyseter, it will download [AnyDorsal](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.14167) to your machine. AnyDorsal is huge (4.5GB), so be prepared! \n\nThis will take a minute, so we recommend saving the results afterwards. \n\n::: {#c428bd43 .cell execution_count=6}\n``` {.python .cell-code}\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\n# convert keys and values to numpy arrays\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n```\n:::\n\n\nYou can load previously saved features with `load_features`\n\n::: {#0e2e2fb0 .cell execution_count=7}\n``` {.python .cell-code}\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)\n```\n:::\n\n\n## Command line interface\n\nUsers can also extract features using the command line interface (e.g., terminal). This can be especially helpful when running on your university's or your agency's high performance computing cluster (HPC). \n\nTo do so, open the terminal and activate your Pyseter environment.\n\n::: {#ebc1eb5b .cell execution_count=8}\n``` {.python .cell-code}\n$ cd happywhale\n$ conda activate pyseter_env \n$ python -m pyseter.extract --dir test_images --bbox_csv happywhale-charm-boxes.csv\n```\n:::\n\n\nThere are two arguments:\n\n  - `dir` specifies the flat directory containing the images from which to extract features\n  - `bbox_csv` optionally indicates where to find the .csv with bounding boxes. \n\nThe features will save to the file, `features/features.npy`, in the **parent** directory of the `dir` you specify. In the example above, the test images live in the `happywhale/test_images` folder, and the features are saved to the `happywhale/features` folder. \n\n",
    "supporting": [
      "extract_files"
    ],
    "filters": [],
    "includes": {}
  }
}