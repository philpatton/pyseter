{
  "hash": "41d39dbfbcf552c704d7d0a7d59c8a9d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Pyseter: A Python package for processing images before photo-identification\"\nauthor:\n  - name: Philip T. Patton\n    orcid: 0000-0003-2059-4355\n    corresponding: true\n    email: PattonP@si.edu\n    affiliations:\n      - Migratory Bird Center, Smithsonian's National Zoo and Conservation Biology Institute\n      - Marine Mammal Research Program, University of Hawaiʻi at Mānoa\n  - name: Claire Lacey\n    orcid: 0000-0003-0541-8193\n    corresponding: false\n    affiliations:\n      - Marine Mammal Research Program, University of Hawaiʻi at Mānoa\n  - name: Lars Bejder\n    orcid: 0000-0001-8138-8606\n    corresponding: false\n    affiliations:\n      - Marine Mammal Research Program, University of Hawaiʻi at Mānoa\n      - Zoophysiology, Department of Bioscience, Aarhus University\nkeywords: computer vision, deep learning, clustering, whale, dolphin\nabstract: Photographic identification (photo-ID) is an effective and non-invasive method for studying many aspects of animal ecology. Photo-ID, however, is a labor-intensive process that involves many steps including, but not limited to, grading individuals by the distinctiveness of their markings (useful for partially marked populations), and curating the best image of each individual from each encounter. This latter step is helpful for reducing false negative errors and reducing the labor effort involved in manual or semi-automated identification. We introduce Pyseter, a Python package for automating several of these steps. Pyseter’s extract module allows users to extract feature vectors from images with AnyDorsal, an individual identification algorithm that has been trained and tested on 24 species of cetacean. The grade module has experimental features for evaluating distinctiveness with information in the feature vectors. The sort module includes two clustering algorithms for grouping images into proposed IDs, as well as methods for investigating cluster performance. Finally, sort also allows users to sort images into subdirectories by this proposed ID then encounter. We demonstrate a typical Pyseter workflow with an example of 1200 spinner dolphin images from Hawaiʻi. Both the grade and sort modules are taxon agnostic, in that they will work with similarity scores or feature vectors from any identification algorithm. Feature vectors are useful for several types of analyses, including estimating individual similarity, comparing a query set with a reference set, and evaluating algorithmic performance in terms of population assessments. As such, the extract module facilitates several useful analyses for users with cetacean datasets. Finally, Pyseter can readily accommodate additional algorithms, e.g., animal detection and quality grading, as the technology becomes available. \ndate: last-modified\nbibliography: references.bib\ncsl: ecology.csl\nnumber-sections: true\njupyter: \"python3\"\n  \nexecute:\n  echo: true\n  include: true\nengine: jupyter\n---\n\n## Introduction\n\nPhotographic identification (photo-ID) is an effective and non-invasive method for studying many aspects of ecology, including movement [@gardiner-2014-dragon], social behavior [@bejder-1998-social], abundance [@mcpherson-2024-dolphin], survival [@morrison-2011-survival], distribution [@mcguire-2020-belu0], recruitment [@setyawan-2022-population], migration [@hill-2020-humpback], and resource-selection [@patton-2025-dissertation]. Photo-ID is a multi-step process that culminates in comparing a query set—images of animals whose identity we wish to know—against a reference set—images of known individuals. This last step can be accomplished by hand [@karanth-1995-camera], with a database [@adams-2006-finbase], or with individual identification software that comes with database management, such as Happywhale [@cheeseman-2021-happywhale].\n\nCurating a query set from a batch of field images involves several steps that are often manual and labor-intensive. These include selecting images with animals [@beery-2019-efficient], selecting images of sufficient quality [@urian-2015-cmr], and selecting images of animals with distinctive markings [@rosel-2011-grading], which is necessary in partially marked populations. Additionally, users may want to limit the query set to only the highest quality images of each individual from a single encounter (e.g., a burst of images from a camera trap). This especially helpful for manual photo-ID because it can dramatically reduce the number of comparisons between the query and the reference set. Further, limiting the query set to only the best images from each encounter may reduce the chance of a missed match, i.e., a false negative error [@urian-2015-cmr]. One way to curate the query set this way is to “sort” the images by apparent individual and encounter (Figure 1).Sorting, however, is a labor-intensive process in itself, in that it requires $\\binom{m}{2}=m(m-1)/2$ comparisons, where $m$ is the number of images in the query set. As such, a query set of 500 images requires 124,750 comparisons to complete the sort. \n\n![Example of sorted images. Before sorting, the entire query set is in one directory. After sorting, the query set is divided into many directories. In this example, the second level of directories are the temporary IDs and the third level of directories are the encounters](images/sort-figure.png){#fig-sort}\n\nWe developed a Python package, Pyseter, to help automate several of these steps. For example, Pyseter’s `extract` module extracts feature vectors from images, which are useful for estimating similarity scores and identifying individuals [@miele-2021-metric]. The `grade` module includes functions for evaluating the distinctiveness of an animal’s markings. Finally, the `sort` module contains two clustering algorithms for classifying individuals into proposed identities. Additionally, `sort` includes functions for sorting images into proposed identities and encounters [@fig-sort]. Pyseter currently lacks functions for detecting animals in images [@beery-2019-efficient] or grading the quality of images [@rosel-2011-grading]. We plan on adding these functions as the technology advances.\n\n## Installation\n\nHere, we assume some familiarity with Python, conda, and pip. Users coming from R, who may be less familiar with these concepts, should reference the “General Overview” Notebook that’s included in this manuscript’s attendant Zenodo repository. [Reviewers will find it in the Anonymous GitHub repository.]\n\nIf using conda, we recommend creating a fresh conda environment. Additionally, before installing Pyseter, users must install Pytorch. Follow the directions on the Pytorch website, which will vary based on your operating system and how you plan to use GPU acceleration. Users who plan on extracting features should have an NVIDIA GPU that is CUDA compatible, or a Mac with at least 16 GB of RAM. Below, we demonstrate the bash commands necessary to install Pytorch. These can be executed in a Jupyter Notebook (as below) or a command line interface (e.g., the miniforge prompt in Windows or the Terminal application in a Unix-like OS). After installing Pytorch, users can install Pyseter from PyPI.\n\n::: {#36c54962 .cell execution_count=1}\n``` {.python .cell-code}\n%%bash\nconda create -n pyseter_env  \nconda activate pyseter_env\nconda install pip3\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128\npip3 install pyseter\n```\n:::\n\n\nUsers can verify the installation by running the following Python commands in, say, a Jupyter Notebook,\n\n::: {#29a56e7b .cell execution_count=2}\n``` {.python .cell-code}\nimport pyseter\npyseter.verify_pytorch()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n✓ PyTorch 2.7.0 detected\n✓ Apple Silicon (MPS) GPU available\n```\n:::\n:::\n\n\nwhich will tell the user which form of GPU acceleration is available, if any.\n \n \n## Spinner dolphin example \n\nWe demonstrate the core modules and functions of Pyseter with a example using spinner dolphins (*Stenella longirostris*). Theoretically, Pyseter is taxon agnostic. Functions in the grade and sort modules work with similarity scores, which can be generated from any individual identification algorithm [@miele-2021-metric]. The extract module, however, only includes one individual identification algorithm, namely, AnyDorsal, which is only suitable for cetacean dorsal images (see below) [@patton-2023-deep]. As such, we chose a cetacean example to demonstrate the package’s full capabilities. The images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi [@lacey-2025-spinner]. Every image collected during the study was graded for quality and distinctiveness [@urian-2015-cmr, @lacey-2025-spinner]. This example dataset only includes images of sufficient quality that have been cropped to the identifying mark—the dorsal fin [@rosel-2011-grading] (@fig-demo). This example includes 208 images of animals without distinctive markings, and 1043 images of animals with distinctive markings.  \n\n::: {#cell-fig-demo .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# grab the first nine images in the dataset\nnrow = ncol = 3\ndemo_dir = '/Users/philtpatton/datasets/pyseter-data/working_dir/all_images'\ndemo_images = os.listdir(demo_dir)[:(nrow * ncol)]\n\n# plot a grid of images \nfig, axes = plt.subplots(nrow, ncol, figsize=(6, 6), tight_layout=True)\nfor i, filename in enumerate(demo_images):\n    path = os.path.join(demo_dir, filename)\n    image = Image.open(path)\n    axes.flat[i].imshow(image)\n    axes.flat[i].axis('off')\n\nfig.savefig('images/fig-demo.png', transparent=False, bbox_inches='tight', dpi=600)\n```\n\n::: {.cell-output .cell-output-display}\n![Nine images from the spinner dolphin example [@lacey-2025-spinner]](tutorial_files/figure-html/fig-demo-output-1.png){#fig-demo width=566 height=535}\n:::\n:::\n\n\n### Folder management\n\nTo do keep things clean and tidy, we recommend establishing a `working_directory` with a subfolder, e.g., called, `all_images`, that contains every image that needs to be sorted (see below for a different case). The working directory should also contain a .csv with encounter information. This .csv would contain two columns: one for the image name, i.e., every image in `all_images`, and another for the encounter. As such, the working directory would look like this.\n\n```\nworking_dir\n├── all_images\n│   ├── 0a49385ef8f1e74a.jpg\n│   ├── 0aca671c4afbd9b9.jpg\n        ...\n│   └── ffa8759a92174857.jpg\n└── encounter_info.csv\n```\n\nAlternatively, you might have your images organized into subfolders by encounter. \n\n```\nworking_dir\n└── original_images\n    ├── enc0\n    │   ├── 0a49385ef8f1e74a.jpg\n        ├── 1e105f9659c12a66.jpg\n            ...\n    │   └── f5093b3089b44e67.jpg\n    └── enc12\n        ├── 0b5c44f167d89d6c.jpg\n        ├── 1e0c186da31a53c4.jpg\n            ...\n        └── f9bb41e7ce0d672d.jpg\n```\nIn this case, you might want to accomplish two tasks: move all these images to one folder, e.g., `all_images`, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The `prep_images()` function does just that.\n\n::: {#1eb9269c .cell execution_count=4}\n``` {.python .cell-code}\nfrom pyseter.sort import prep_images\n\nworking_dir = '/Users/philtpatton/datasets/pyseter-data/working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCopied 1251 images to: /Users/philtpatton/datasets/pyseter-data/working_dir/all_images\nSaved encounter information to: /Users/philtpatton/datasets/pyseter-data/working_dir/encounter_info.csv\n```\n:::\n:::\n\n\n### Extracting features\n\nPyseter’s `extract` module includes the AnyDorsal algorithm, which was trained to identify cetaceans of 24 species [@patton-2023-deep]. AnyDorsal’s identifying performance varies by species. Species primarily identified by nicks and notches along the dorsal fin will perform best [@patton-2023-deep].\n\nPyseter extracts features with the `FeatureExtractor` class, which needs to be initialized by setting the `batch_size`. The `batch_size` dictates how many images will be processed by the GPU at once. Larger batch sizes might run faster yet might also produce an `OutOfMemoryError`. For lower memory GPUs, we recommend smaller batch sizes. If you encounter an `OutOfMemoryError` with `batch_size=1`, then you will have to reduce the size of your images. See the Supplement for how to do so with the Pillow library.\n\n::: {#569ac60a .cell execution_count=5}\n``` {.python .cell-code}\nfrom pyseter.extract import FeatureExtractor\n\n# feature extraction can take time so it's useful to save the result \nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\nfe = FeatureExtractor(batch_size=4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: mps (Apple Silicon GPU)\n```\n:::\n:::\n\n\nThe `extract()` method of the `FeatureExtractor` class only takes one argument, `image_dir`, the flattened directory containing every image to be processed. Feature extraction can take several minutes, depending on the number of files and the GPU, so we recommend saving the results afterwards. The `extract()` function returns a Python dictionary where the filenames are the keys, and the features are the values. It can be useful to convert these to NumPy arrays \n\n::: {#5178f5d6 .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\n\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\n# convert keys and values to numpy arrays\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n```\n:::\n\n\nAlternatively, we can load previously saved results with the `load_features()` function. In either case, `feature_array` will be a two-dimensional matrix of shape `(n, 5504)`, where `n` is the number of images and `5504` is the number of features returned by AnyDorsal.\n\n::: {#6cbf54a4 .cell execution_count=7}\n``` {.python .cell-code}\n# alternatively, load in the feature dictionary from file \nimport numpy as np \nfrom pyseter.sort import load_features\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)\n```\n:::\n\n\n### Grading individuals by distinctiveness\n\nHere, we introduce one of Pyseter’s clustering algorithms, `NetworkCluster`, because doing so helps understand the distinctiveness grading algorithm (see Section 3.4 for a more thorough description of `NetworkCluster`). Network clustering works with similarity scores, which represent the similarity between two individuals in a pair of images. We can define a threshold score, the `match_threshold`, above which we consider two individuals to be the same. That is, if the similarity score between two images is above a certain threshold, we cluster them into a proposed ID. As such, network clustering works by treating the query set as a network, where the nodes are images and the edges are similarity scores above a threshold. Each set of connected components, i.e., images whose similarity scores are above the match threshold, represents a proposed ID.\n\nWe might expect the indistinct individuals to cluster together. In the context of facial recognition, @deng-2023-ui observed that “unrecognizable identities”, e.g., extremely blurry or masked faces, tend to cluster together. As such, for partially marked populations, the largest cluster in the query set may represent every indistinct individual. Following @deng-2023-ui, we can compute the average feature vector for this cluster. The distance between this average feature vector and the feature vector for each image is the distinctiveness score for that image. As such, the score applies to the image, not the animal. To get a score for an animal, users could average the distinctiveness scores across images for that animal.\n\n::: {#a2a91d0b .cell execution_count=8}\n``` {.python .cell-code}\nfrom pyseter.grade import rate_distinctiveness\ndistinctiveness = rate_distinctiveness(feature_array, match_threshold=0.5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/philtpatton/miniforge3/envs/pymc_env/lib/python3.13/site-packages/pyseter/grade.py:35: UserWarning: Distinctiveness grades are experimental and should be verified.\n  warn(UserWarning('Distinctiveness grades are experimental and should be verified.'))\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nUnrecognizable identity cluster consists of 196 images.\n```\n:::\n:::\n\n\nWe can evaluate the effectiveness of these scores with AUC (i.e., the area under the receiver operating curve). AUC is a metric for evaluating a classifier's ability to balance true positive and false positive rates. In this example, a classifier built from the distinctiveness grades achieves an AUC of 0.925 [@fig-roc].\n\n::: {#cell-fig-roc .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, roc_auc_score\nimport pandas as pd\n\n# merge the ers scores with the true values to compare for each image\nmapping = pd.read_csv('/Users/philtpatton/datasets/pyseter-data/file_mapping.csv').iloc[:, 1:]\ners_df = pd.DataFrame({'image_new': filenames, 'ers': 1 - distinctiveness})\ners_df = ers_df.merge(mapping)\n\n# compute the curve first, which will get displayed\ny_score = ers_df['ers']\ny_test, _ = ers_df.distinctiveness.factorize()\nfpr, tpr, _ = roc_curve(y_test, y_score)\n\nfig, ax = plt.subplots(figsize=(4, 3))\n\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nroc_auc = roc_auc_score(y_test, y_score)\nax.text(0.95, 0.6, f'AUC={roc_auc:0.3f}', ha='right', va='top')\n\nax.set_title('ROC Curve for \\nDistinctiveness Classifier')\n\nfig.savefig('images/fig-auc.png', transparent=False, bbox_inches='tight', dpi=300)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/philtpatton/miniforge3/envs/pymc_env/lib/python3.13/site-packages/sklearn/metrics/_plot/roc_curve.py:189: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  self.ax_.legend(loc=\"lower right\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Receiving operator characteristic (ROC) curve for a classifier based on the ERS.  The area under the curve (AUC), a measure of overall performance, is listed in the middle.](tutorial_files/figure-html/fig-roc-output-2.png){#fig-roc width=303 height=320}\n:::\n:::\n\n\nThese distinctiveness scores are experimental in that have not been robustly tested across a variety of scenarios. Nevertheless, users might use them as a guide, potentially making distinctiveness grading somewhat easier.\n\n### Clustering images into proposed IDs\n\nTo use `NetworkCluster`, users must first compute the similarity scores between each pair of images. After computing the scores, we cluster the images, where each cluster represents a proposed identity.\n\n::: {#8b9eb9ce .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pyseter.sort import NetworkCluster, report_cluster_results\n\nsimilarity_scores = cosine_similarity(feature_array)\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFollowing clusters may contain false positives:\n['ID_0002', 'ID_0011', 'ID_0045', 'ID_0048', 'ID_0150']\n```\n:::\n:::\n\n\nAs mentioned above, network clustering has one major hyperparameter, `match_threshold`, which indicates whether two images should be grouped within a cluster. High thresholds mean that few images will be clustered together, creating many clusters. Very low thresholds mean that many images will be clustered together, creating few clusters. The `report_cluster_results()` function produces a quick and dirty summary of the number of clusters created, and the size of the largest cluster (i.e., the number of images associated to the most photographed individual). This is a quick sanity check. The `results` object has several useful attributes and methods (see below). For example, `results.cluster_idx` contains the proposed ID for each image.\n\n::: {#b6a09d28 .cell execution_count=11}\n``` {.python .cell-code}\nnetwork_idx = results.cluster_idx\nreport_cluster_results(network_idx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 208 clusters.\nLargest cluster has 128 images.\n```\n:::\n:::\n\n\nIn this example, the `nc.cluster_images()` function warned that some clusters may contain “false positives.” False positive matches occur when two separate individuals fall under the same proposed ID. We can diagnose possible false positives by evaluating the network. Recall that, in the network, a blob of connected nodes (i.e., connected components) represents a proposed ID. These connected components, however, can sometimes look less like a blob and more like a barbell, where two sets of images have many connections amongst each other, yet the two blobs are only connected by a single link. We suspect that such clusters represent false positives, i.e., two sets of images for two individuals connected by one spurious link. We can plot the networks of the suspicious clusters with the `results.plot_suspicious()` function.\n\n::: {#cell-fig-false .cell execution_count=12}\n``` {.python .cell-code}\nresults.plot_suspicious()\n```\n\n::: {.cell-output .cell-output-display}\n![Proposed IDs that may contain false positive errors. Each image (colored)  circle is linked to another image (gray line) if similarity between them exceeds  a threshold. The colors of each circle represent potential IDs within the proposed  ID.](tutorial_files/figure-html/fig-false-output-1.png){#fig-false width=710 height=146}\n:::\n:::\n\n\nBoth `ID_0011` and `ID_0150` appear to have spurious links combining two separate IDs. To deal with this, we could increase manually separate these clusters, or increase the match threshold. Cluster `ID_0002` represents the \"unrecognizable individual\" cluster (see Section 3.3).\n\nAs the number of images being clustered grows, the overall false positive rate also grows (this is analogous the multiple comparison problem in statistics). At some point, network matching becomes untenable; all but the highest match thresholds would produce too many false positives to be useful. For these cases, there is `HierarchicalCluster`, which relies on the Hierarchical Agglomerative Clustering algorithm provided by the popular machine learning package, scikit-learn [@scikit-learn]. Note that `HierarchicalCluster` will run noticeably slower than `NetworkCluster`. See the supplement for an example with `HierarchicalCluster`.\n\n### Sorting images by proposed ID then encounter\n\nWith these cluster results, we can sort images by proposed ID then encounter. To do so, we need to create a Pandas `DataFrame` that indicates the proposed ID and encounter for each filename [@pandas]. Recall that we created the `encounter_info.csv` with the `prep_images()` function above.\n\n::: {#461f733d .cell execution_count=13}\n``` {.python .cell-code}\nimport pandas as pd\n\nid_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})\n\n# join with the encounter information using \"encounter\" as a key \nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nid_df = id_df.merge(encounter_info)\nid_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>proposed_id</th>\n      <th>encounter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ae16cb2c0815b31c.jpg</td>\n      <td>ID-0000</td>\n      <td>enc3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f86cdadd6ecb59aa.jpg</td>\n      <td>ID-0001</td>\n      <td>enc0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ee348a1b39cc38b3.jpg</td>\n      <td>ID-0002</td>\n      <td>enc12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dc11f24c81a8e339.jpg</td>\n      <td>ID-0003</td>\n      <td>enc10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ae466530552590e3.jpg</td>\n      <td>ID-0004</td>\n      <td>enc4</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFinally, to sort the images, we need to specify an output directory, then run the `sort_images()` function. Note that the ID `DataFrame` must have columns named `image`, `proposed_id`, and `encounter`. Otherwise `sort_images()` will not work.\n\n::: {#c6ececc7 .cell execution_count=14}\n``` {.python .cell-code}\nfrom pyseter.sort import sort_images\n\n# make an output directory \nsorted_dir = working_dir + '/sorted_images'\nos.makedirs(sorted_dir, exist_ok=True)\n\nsort_images(id_df, all_image_dir=image_dir, output_dir=sorted_dir)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSorted 1251 images into 311 folders.\n```\n:::\n:::\n\n\nNow the flat directory, `all_images` has been copied to a new folder, `sorted_images`, that is organized by proposed ID, then encounter.\n\n```\nsorted_images\n├── ID-0000\n│   ├── enc0\n│   │   ├── 0a49385ef8f1e74a.jpg\n│   │   ├── 4d69031e07ef3393.jpg\n│   │   ├── b4f1ca6229180f18.jpg\n│   │   └── e0016bed0be5bf9f.jpg\n│   ├── enc10\n│   │   ├── 0cdbe7c151420b0c.jpg\n│   │   ├── 19dd8e9db3a26066.jpg\n            ...\n│   └── enc3\n│       ├── 1ced4b1e3bd63781.jpg\n            ...\n│       └── bc17d3aa572320cc.jpg\n├── ID-0001\n│   ├── enc0\n│   │   ├── e7dbbe01ac71d6e8.jpg\n│   │   └── f86cdadd6ecb59aa.jpg\n│   ├── enc2\n│   │   ├── 2b66eacdced6c165.jpg\n│   │   ├── 7ce2b670757443de.jpg\n            ... \n├── ID-0206\n│   └── enc4\n│       └── 767bdedefe51aabb.jpg\n└── ID-0207\n    └── enc4\n        └── e345d4ddaae7db02.jpg\n```\n\n## Conclusion\n\nWe have demonstrated one possible workflow for using Pyseter. A potential user would manually grade images for quality and crop images of sufficient quality to the identifying mark. Then, they would use Pyseter to grade these images for distinctiveness and to sort the images above a distinctiveness threshold into folders, selecting the highest quality image of each proposed individual. Finally, the user would compare these images against the reference set, either manually or with an interface such as Happywhale.\n\nNevertheless, Pyseter offers users with cetacean datasets flexibility to conduct several kinds of analyses because it allows them to extract feature vectors from images with AnyDorsal. For example, users could compute the false negative rate for their dataset, which is useful for population assessments [@patton-2025-optimizing]. Similarly, one could use Pyseter to evaluate the predictive performance of AnyDorsal on a species that was outside the training dataset. Further, after the initial sort, users could clean up the results by doing a second round of clustering and sorting. As Pyseter matures, we plan to add documentation on how to conduct such analyses with Pyseter. Finally, as photo-ID technology improves, e.g., automated quality grading, we plan on adding these algorithms to Pyseter. \n\n## Supplements\n\n### Hierarchical Agglomerative Clustering\n\nBelow is a demonstration of how to use the HAC algorithm to cluster images, then sort them into folders.\n\n::: {#c0c5cc4a .cell execution_count=15}\n``` {.python .cell-code}\nfrom pyseter.sort import HierarchicalCluster, format_ids\n\nhc = HierarchicalCluster(match_threshold=0.5)\nhac_idx = hc.cluster_images(feature_array)\n\n# format_ids converts the integer labels to something like 'ID-0001'\nhac_labels = format_ids(hac_idx)\nreport_cluster_results(hac_labels)\n\nhac_df = pd.DataFrame({'image': filenames, 'proposed_id': hac_labels})\nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nhac_df = hac_df.merge(encounter_info)\n\n# separate directory for the hac images\nhac_dir = working_dir + '/sorted_images_hac'\nos.makedirs(hac_dir, exist_ok=True)\nsort_images(hac_df, all_image_dir=image_dir, output_dir=hac_dir)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 298 clusters.\nLargest cluster has 27 images.\nSorted 1251 images into 403 folders.\n```\n:::\n:::\n\n\n`HierarchicalCluster` is useful for large datasets, yet will be more prone to false negative errors. In this example, it found 60 more clusters (proposed IDs) than the network matching, which may be dubious. Users will have to decide how to balance false positive versus false negative matches. For example, we recommend that users preprocess their images with Pyseter, then identify animals in the pre-processed images manually or a program such as Happywhale. This second round of identification should help clean up false negative matches. As such, users following this approach might be more averse to false positive errors in the first stage. \n\n### Resizing images with Pillow\n\nHere is how is one example on how to do so with the Pillow library. \n\n::: {#44f86c7b .cell execution_count=16}\n``` {.python .cell-code}\nfrom PIL import Image\n\nnew_size = 512, 512\n\n# files for resizing and where they'll be sent\nall_images = [i for i in os.listdir(image_dir) if i.endswith('jpg')]\nnew_dir = os.path.join(working_dir, 'thumbnails')\nos.makedirs(new_dir, exist_ok=True)\n\n# resize and save the files \nfor file in all_images:\n    old_path = os.path.join(image_dir, file)\n    with Image.open(old_path) as im:\n        im.thumbnail(new_size)\n        new_path = os.path.join(new_dir, file)\n        im.save(new_path)\n```\n:::\n\n\n## Reference {.unnumbered}\n\n",
    "supporting": [
      "tutorial_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}