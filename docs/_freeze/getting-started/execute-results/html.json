{
  "hash": "6f87f3de42bdd950fd8c29ba693abfe0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Getting started with Pyseter\njupyter:\n  kernelspec:\n    display_name: Python (Pyseter)\n    language: python\n    name: pyseter_env\n---\n\n\nHere we present a quick overview of the main functions of Pyseter with a demo dataset.  \n\n:::{.callout-tip}\n## R user tip\n\nThroughout we'll provide tips for R users who are new to Python. If this is you, check out [Installation - New to Python](install-r.qmd) instructions.\n:::\n\n\n## Installation\n\nIf you haven't already, install some form of conda. [Anaconda](https://www.anaconda.com) is probably the easiest to install, but we are personally partial to [miniforge](https://github.com/conda-forge/miniforge?tab=readme-ov-file). Of course, you are also welcome to use any other package management tool, such as [uv](https://docs.astral.sh/uv/), [pixi](https://pixi.sh/latest/), or good old fashioned [venv](https://docs.python.org/3/library/venv.html).\n\nOnce you've installed conda, open the command line interface (e.g., the miniforge prompt or the terminal). Then you'll need to do several things:\n\n- Create the conda environment you'll be using\n- Active the environment and install `pip`\n- Install packages\n  - Pytorch, which will depend on your operating system and GPU availability\n  - gdown (for downloading the  the dataset)\n  - ipykernel (optional, useful if using Jupyter Lab or Jupyter Notebook)\n  - pyseter\n\nBelow is an example of the commands necessary to install on a Windows machine equipped with an NVIDIA GPU. \n\n```bash\nconda create -n pyseter_env -y  # create new environment \nsource activate pyseter_env     # activate it\nconda install pip -y            # pip is necessary to install torch and pyseter\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu128 \npip install pyseter gdown ipykernel==6.30.1 ipywidgets\npython -m ipykernel install --user --name pyseter_env --display-name \"Python (Pyseter)\"\n```\n\n## Dataset\n\nThe images in this example were collected during a multi-year photo-ID survey of spinner dolphins in Hawaiʻi. Every image collected during the study was graded for quality and distinctiveness. This example dataset only includes images of sufficient quality that have been cropped to the identifying mark—the dorsal fin. This example includes 208 images of animals without distinctive markings, and 1043 images of animals with distinctive markings.\n\nOpen a Jupyter notebook and download the data with `gdown`, which pulls data from Google Drive.\n\n::: {#4ed72bc7 .cell execution_count=1}\n``` {.python .cell-code}\nimport gdown\nimport zipfile\n\n# make the file url with an f string\nfile_id = '1puM7YBTVFbIAT3xNBQV1g09K0bMGLk1y' \nfile_url = f'https://drive.google.com/uc?id={file_id}'\n\n# download the demo data\ngdown.download(file_url, quiet=False, use_cookies=False)\n\n# extract the files to the working directory\nwith zipfile.ZipFile('original_images.zip', 'r') as zip_ref:\n    zip_ref.extractall('working_dir')\n```\n:::\n\n\n:::{.callout-tip}\n## R user tip\n\nIn R, the above code block would look something like\n\n```\nlibrary(gdown)\ndownload()\n```\n\nImports work a little differently in Python. First, we need tell Python that this package is available for imports, `import pyseter`, then we need to explicitly call the function from the library `pyseter.verify_pytorch()`. To an R user, this can feel overly wordy. Nevertheless, this wordiness helps keep the global environment clean. Whereas R sessions frequently have to deal with [masking names](https://adv-r.hadley.nz/functions.html?q=masking#lexical-scoping), this rarely happens in Python. \n:::\n\n::: {.callout-tip}\n## R user tip\n\nThis block uses two fancy Python concepts, `with` (or the context manager), and the `f''` string. The `f''` string allows us to substitute the `file_id` into the `file_url`. The `with` statement is a way to safely open and close a connection to a file.\n:::\n\n## Working with Pyseter\n\nNow we're ready to work in Pyseter. First, verify that your PyTorch installation. This will also let you know how fast (or slow) you can expect Pyseter to be.\n\n::: {#aac3241c .cell execution_count=2}\n``` {.python .cell-code}\nimport pyseter\npyseter.verify_pytorch()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n:) PyTorch 2.10.0 detected\n:) Apple Silicon (MPS) GPU available\n```\n:::\n:::\n\n\nThe demo dataset is organized into subfolders by encounter. \n\n```\nworking_dir\n└── original_images\n    ├── enc0\n    │   ├── 0a49385ef8f1e74a.jpg\n        ├── 1e105f9659c12a66.jpg\n            ...\n    │   └── f5093b3089b44e67.jpg\n    └── enc12\n        ├── 0b5c44f167d89d6c.jpg\n        ├── 1e0c186da31a53c4.jpg\n            ...\n        └── f9bb41e7ce0d672d.jpg\n```\n\nOur lives will be a little easier if we do two things: move all these images to a flat folder, i.e., with no subfolders, and create a .csv that indicates which image belongs to which encounter (i.e., a map from image to encounter). The `prep_images()` function does just that.\n\n::: {#62fa8c69 .cell execution_count=3}\n``` {.python .cell-code}\nfrom pyseter.sort import prep_images\n\nworking_dir = 'working_dir'\noriginal_image_dir = working_dir + '/original_images'\n\n# new, flattened directory containing every image\nimage_dir = working_dir + '/all_images'\nprep_images(original_image_dir, all_image_dir=image_dir)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCopied 1251 images to: working_dir/all_images\nSaved encounter information to: /Users/PattonP/source/repos/pyseter/docs/working_dir/encounter_info.csv\n```\n:::\n:::\n\n\nWe can look at a few random images with `matplotlib`.\n\n::: {#e6c25587 .cell execution_count=4}\n``` {.python .cell-code}\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# select a random id to plot\nrng = np.random.default_rng(seed=17)\nimages = os.listdir(image_dir)\n\n# create a figure where each subplot is an image \ncolumn_count = 3\nrow_count = 7\nfig, axes = plt.subplots(row_count, column_count, tight_layout=True, \n                         figsize=(9, row_count * 3))\n\n# plot each image\nfor i in range(column_count * row_count):\n    ax = axes.flat[i]\n    file_name = images[i]\n    path = f'working_dir/all_images/{file_name}'\n    image = Image.open(path)\n    ax.imshow(image)\n    ax.axis('off')\n```\n\n::: {.cell-output .cell-output-display}\n![](getting-started_files/figure-html/cell-5-output-1.png){width=854 height=1930}\n:::\n:::\n\n\n### Feature extraction\n\nIdentifying animals in images requires extracting feature vectors. The distance between two feature vectors tells us similarity between two images. Similar images likely contain the same individual, if the model is working. \n\nTo do so, we'll initialize the `FeatureExtractor`. The only argument is the batch size, which we recommend setting to something low, like 4. \n\n::: {#78c8eb08 .cell execution_count=5}\n``` {.python .cell-code}\nimport os\nfrom pyseter.extract import FeatureExtractor\n\n# we'll save the results in the feature_dir\nfeature_dir = working_dir + '/features'\nos.makedirs(feature_dir, exist_ok=True)\n\n# initialize the extractor \nfe = FeatureExtractor(batch_size=4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: mps (Apple Silicon GPU)\n```\n:::\n:::\n\n\nNow we can extract features. This will take a while, depending on hardware. On a machine with an NVIDIA GPU, this will take a minute or two. On a Mac, this will take around 10 minutes. \n\n:::{.callout-warning}\nFeature extraction will be extremely slow for users who do not have access to an NVIDIA GPU or Apple Silicon. Users may want to check if their university or agency has access to GPUs via a high performance computing cluster (HPC). Also, Google Colab and other similar services (e.g., Kaggle) allow users to rent GPU resources. \n:::\n\nThe first time you extract features with Pyseter, it will download [AnyDorsal](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.14167) to your machine. AnyDorsal is huge (4.5GB), so be prepared! \n\n::: {#dd351c11 .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\n\nfeatures = fe.extract(image_dir=image_dir)\n\n# this saves the dictionary as an numpy file\nout_path = feature_dir + '/features.npy'\nnp.save(out_path, features)\n\n# convert keys and values to numpy arrays\nfilenames = np.array(list(features.keys()))\nfeature_array = np.array(list(features.values()))\n```\n:::\n\n\n:::{.callout-tip}\n## R user tip\nIn this case, `FeatureExtractor` is a class and `extract()` is a method of that class. Classes and methods also exist in R, but operate more behind the scenes. For example, `x <- data.frame()` initializes an object of class data.frame, and `summary(x)` calls the summary method for data.frames. Python makes this relationship more explicit. For example, the equivalent Python code would be `x = pandas.DataFrame()` and `x.summary()`. The `pandas` library provides data frames in Python.\n:::\n\nYou can also load in features from a previously saved session. \n\n::: {#515aa5cc .cell execution_count=7}\n``` {.python .cell-code}\n# alternatively, load in the feature dictionary from file \nimport numpy as np \nfrom pyseter.sort import load_features\nout_path = feature_dir + '/features.npy'\nfilenames, feature_array = load_features(out_path)\n```\n:::\n\n\n## Clustering individuals\n\nNext, we can cluster individuals into proposed IDs. One simple method for doing so is to say that any two images with similarity scores above a `match_threshold` belong to the same individual (note that the similarity score is one minus the distance between two feature vectors). This creates a network where each node is an image and the connected nodes represent one proposed ID. Hence, we call this `NetworkCluster`\n\n::: {#ea301ef3 .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom pyseter.sort import NetworkCluster, report_cluster_results\n\nsimilarity_scores = cosine_similarity(feature_array)\nnc = NetworkCluster(match_threshold=0.55)\nresults = nc.cluster_images(similarity_scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFollowing clusters may contain false positives:\n['ID_0001', 'ID_0006', 'ID_0008', 'ID_0021', 'ID_0110']\n```\n:::\n:::\n\n\nYou can look at the results with `report_cluster_results`.\n\n::: {#2964dc51 .cell execution_count=9}\n``` {.python .cell-code}\nnetwork_idx = results.cluster_idx\nreport_cluster_results(network_idx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 208 clusters.\nLargest cluster has 128 images.\n```\n:::\n:::\n\n\nIn an ideal world, every image of the same individual would be connected in the network, and not connected to images of any other individuals. As such, the ideal network would look like a bunch of blobs. As such, we might be suspicious of clusters that look like barbells, i.e., two blobs connected by one link. That one link might be a *false positive*, linking two distinct identities. The `plot_suspicious` function plots things that might look like barbells.\n\n::: {#ba79c9f3 .cell execution_count=10}\n``` {.python .cell-code}\nresults.plot_suspicious()\n```\n\n::: {.cell-output .cell-output-display}\n![](getting-started_files/figure-html/cell-11-output-1.png){width=710 height=146}\n:::\n:::\n\n\n## Sorting individuals into folders \n\nNow, we might want to inspect these proposed ID. One method is to chuck the IDs into a table, i.e., a `DataFrame`.\n\n::: {#a14bd2ba .cell execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\n\nid_df = pd.DataFrame({'image': filenames, 'proposed_id': network_idx})\n\n# join with the encounter information using \"encounter\" as a key \nencounter_info = pd.read_csv(working_dir + '/encounter_info.csv')\nid_df = id_df.merge(encounter_info)\nid_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>proposed_id</th>\n      <th>encounter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2c8750b066372ab5.jpg</td>\n      <td>ID-0000</td>\n      <td>enc8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>568fc1d376b616a6.jpg</td>\n      <td>ID-0001</td>\n      <td>enc8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ddeb347716d7861c.jpg</td>\n      <td>ID-0002</td>\n      <td>enc6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b65f9334b05f48f4.jpg</td>\n      <td>ID-0003</td>\n      <td>enc0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d84aefa4d99d6f9a.jpg</td>\n      <td>ID-0004</td>\n      <td>enc4</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBut this isn't particularly satisfying, since we can't actually see the IDs. Probably the easiest way to to sort the images into folders then explore them with the file explore (e.g., Finder on Mac), or with a program like ACDSee. We can do so with `sort_images`.\n\n::: {#8a26ccc2 .cell execution_count=12}\n``` {.python .cell-code}\nfrom pyseter.sort import sort_images\n\n# make an output directory \nsorted_dir = working_dir + '/sorted_images'\nos.makedirs(sorted_dir, exist_ok=True)\n\nsort_images(id_df, all_image_dir=image_dir, output_dir=sorted_dir)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSorted 1251 images into 311 folders.\n```\n:::\n:::\n\n\nWe can also plot some images with `matplotlib`, if that's your thing.\n\n::: {#6a170276 .cell execution_count=13}\n``` {.python .cell-code}\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# select a random id to plot\nrng = np.random.default_rng(seed=17)\nrandom_id = rng.choice(id_df.proposed_id)\n\n# collect all the images for that id\nid_info = id_df.loc[id_df.proposed_id == random_id].sort_values('encounter').reset_index(drop=True)\nimage_count = len(id_info)\n\n# create a figure where each subplot is an image \ncolumn_count = 3\nrow_count = (image_count + column_count - 1) // column_count\nfig, axes = plt.subplots(row_count, column_count, tight_layout=True, \n                         figsize=(9, row_count * 3))\n\n# plot each image\nfor i, row in id_info.iterrows():\n    ax = axes.flat[i]\n    path = f'working_dir/all_images/{row.image}'\n    image = Image.open(path)\n    ax.imshow(image)\n    ax.set_title(row.encounter)\n    ax.axis('off')\n\nfig.text(0.5, 1.0, f'Proposed ID: {random_id}', ha='center', va='bottom',\n         fontsize=16)\n\n# delete any unnecessary subplots\nfor i in range(image_count, row_count * column_count):\n    axes.flat[i].remove() \n```\n\n::: {.cell-output .cell-output-display}\n![](getting-started_files/figure-html/cell-14-output-1.png){width=854 height=3157}\n:::\n:::\n\n\n",
    "supporting": [
      "getting-started_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}